{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from my_utilities import read_dict, save_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Metadata Harvester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "## Grab records from the requested timespan, from all or from one selected category\n",
    "\n",
    "## Write to tab-delimited local csv:\n",
    "## columns: *id, categories, title, abstract*\n",
    "(Dealing with funny characters in the names of authors was beyond me. One could also get a date associated with each record but it's supposed not to necessarily correspond to the date of posting by the authors.)\n",
    "### There are two functions (the code is in *my_utilities.py*).\n",
    "Both will talk to You using prints.\n",
    "* *harvest_slice* needs You to explicitly choose the category (possibly 'all') and the filename as arguments\n",
    "    * just appends lines to the file, it's up to You not to make a mess\n",
    "\n",
    "\n",
    "* *harvest_data* divides the timespan into slices of given length and harvests those using *harvest_slice*:\n",
    "    * can make up the name of the file on its own\n",
    "    * adds the header to the csv\n",
    "    * default behavior when the file already exists is to quit\n",
    "    * default category is 'all'\n",
    "\n",
    "### It is slow.    \n",
    "### Examples:\n",
    "*  ~ 1 min,  2 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")\n",
    "*  ~ 5 min, 11 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")\n",
    "* ~ 10 min, 16 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)\n",
    "* ~ 1 h, 68 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\")\n",
    "\n",
    "### Example of a basic query used in the code:\n",
    "* http://export.arxiv.org/oai2?verb=ListRecords&from=2012-01-01&until=2018-02-01&set=physics:hep-th&metadataPrefix=arXiv\n",
    "* \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "\n",
    "See https://arxiv.org/help/bulk_data for more info.\n",
    "\n",
    "### Side effect:\n",
    "Create small text files that - loosely speaking - list the extracted labels: \n",
    "   * categories.txt\n",
    "   * top_cats.txt\n",
    "   * physics_genres.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside from having authors, a title and an abstract (a summary), articles on *ArXiv* are typically assigned to a category, e.g. Computer Science, Economics, etc. Those informations form the meta-data of an article that is easily obtainable with an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One can talk with *ArXiv* using two different interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first one serves to answer typical complicated search queries.\n",
    "For example looking for articles by Stephen Hawking about black holes we could start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"ti:black%20hole+AND+au:Hawking\"\n",
    "\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entries = soup.find_all('entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a typical data we get is the following. Notice that there is both the *primary category* and a general *category* list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/hep-th/0507171v2\n",
      "S. W. Hawking\n",
      "Information Loss in Black Holes\n",
      "primary category: hep-th\n",
      "all categories: ['hep-th']\n",
      "abstract:   The question of whether information is lost in black holes is investigated\n",
      "using Euclidean path integrals. The formation and evaporation of black holes is\n",
      "regarded as a scattering problem with all m ...\n"
     ]
    }
   ],
   "source": [
    "entry = entries[0]\n",
    "\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:200]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another example we see that there can be more categories: e.g. one from Economics (*econ.EM*) and an another one from Statistics (*stat.AP*), and that the first one in the list is the primary category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/1803.11233v1\n",
      "Kamil Jodź\n",
      "Mortality in a heterogeneous population - Lee-Carter's methodology\n",
      "primary category: econ.EM\n",
      "all categories: ['econ.EM', 'stat.AP']\n",
      "abstract:   The EU Solvency II directive recommends insurance companie ...\n"
     ]
    }
   ],
   "source": [
    "search_query = \"1803.11233\"\n",
    "# search_query = \"0707.3787\"\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entry = soup.find('entry')\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:60]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But this first API is not suited for bulk data downloads. Instead, we want to use the interface specified by Open Archives Initiative (OAI) that ArXiv complies with.\n",
    "This time we build the query by specifying the time slice from which we want the articles. We can also filter for one category, if we want. Take the following query for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from = \"2018-04-02\"\n",
    "date_until = \"2018-04-02\"\n",
    "category = \"econ\" # Economics\n",
    "\n",
    "search_query = f\"&from={date_from}&until={date_until}&set={category}\"\n",
    "query = \"http://export.arxiv.org/oai2?verb=ListRecords\" + search_query + \"&metadataPrefix=arXiv\"\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "records = soup.find_all('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<record>\n",
       "<header>\n",
       "<identifier>oai:arXiv.org:1803.11233</identifier>\n",
       "<datestamp>2018-04-02</datestamp>\n",
       "<setspec>econ</setspec>\n",
       "</header>\n",
       "<metadata>\n",
       "<arxiv xmlns=\"http://arxiv.org/OAI/arXiv/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemalocation=\"http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd\">\n",
       "<id>1803.11233</id><created>2018-03-29</created><authors><author><keyname>Jodź</keyname><forenames>Kamil</forenames></author></authors><title>Mortality in a heterogeneous population - Lee-Carter's methodology</title><categories>econ.EM stat.AP</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EU Solvency II directive recommends insurance companies to pay more\n",
       "attention to the risk management methods. The sense of risk management is the\n",
       "ability to quantify risk and apply methods that reduce uncertainty. In life\n",
       "insurance, the risk is a consequence of the random variable describing the life\n",
       "expectancy. The article will present a proposal for stochastic mortality\n",
       "modeling based on the Lee and Carter methodology. The maximum likelihood method\n",
       "is often used to estimate parameters in mortality models. This method assumes\n",
       "that the population is homogeneous and the number of deaths has the Poisson\n",
       "distribution. The aim of this article is to change assumptions about the\n",
       "distribution of the number of deaths. The results indicate that the model can\n",
       "get a better match to historical data, when the number of deaths has a negative\n",
       "binomial distribution.\n",
       "</abstract></arxiv>\n",
       "</metadata>\n",
       "</record>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time there is only the single ***categories*** tag. **We will be assuming that there is a convention that the first item on that list is the primary category of an article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803.11233\n",
      "econ.EM stat.AP\n"
     ]
    }
   ],
   "source": [
    "print(records[0].id.string)\n",
    "print(records[0].categories.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the *set* field in the last query. We can retrieve the list of all possible *sets* using another fixed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'econ': 'Economics',\n",
       " 'eess': 'Electrical Engineering and Systems Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'physics:astro-ph': 'Astrophysics',\n",
       " 'physics:cond-mat': 'Condensed Matter',\n",
       " 'physics:gr-qc': 'General Relativity and Quantum Cosmology',\n",
       " 'physics:hep-ex': 'High Energy Physics - Experiment',\n",
       " 'physics:hep-lat': 'High Energy Physics - Lattice',\n",
       " 'physics:hep-ph': 'High Energy Physics - Phenomenology',\n",
       " 'physics:hep-th': 'High Energy Physics - Theory',\n",
       " 'physics:math-ph': 'Mathematical Physics',\n",
       " 'physics:nlin': 'Nonlinear Sciences',\n",
       " 'physics:nucl-ex': 'Nuclear Experiment',\n",
       " 'physics:nucl-th': 'Nuclear Theory',\n",
       " 'physics:physics': 'Physics (Other)',\n",
       " 'physics:quant-ph': 'Quantum Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query retrieves xml about the accesible 'sets', e.g.\n",
    "# <set>\n",
    "# <setspec>cs</setspec>\n",
    "# <setname>Computer Science</setname>\n",
    "# </set>\n",
    "\n",
    "if not Path(\"categories.txt\").is_file() :\n",
    "    \n",
    "    xml_query = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "    sauce = urllib.request.urlopen(xml_query).read()\n",
    "    soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "    sets = soup.find_all(\"set\")\n",
    "\n",
    "    categories = {}\n",
    "\n",
    "    for set_ in sets:\n",
    "        categories[set_.setspec.string] = set_.setname.string\n",
    "\n",
    "    save_dict(categories, \"categories.txt\")\n",
    "            \n",
    "\n",
    "categories = read_dict(\"categories.txt\")\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently physics enthusiasts get more options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The matter of actual article categories is more messy, see https://arxiv.org/ and https://arxiv.org/help/prep#subj\n",
    "Physics gets an additional level of gradation: e.g. *physics:astro-ph* is a subset of *physics*. And the categorization chosen by an author her- or himself is finer and may be multiple, e.g. *cs.ai* (Computer Science: Artificial Intelligence) instead of just *cs*, together with *physics:astro-ph.GA* (Physics: Astrophysics: Astrophysics of Galaxies) instead of just *physics:astro-ph* (assuming that the article was both about Artificial Intelligence and Galaxies). But, again, first of the categories is the primary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dictionaries. One with the top-level categories, and the second with physics genres.\n",
    "\n",
    "pattern = re.compile('physics:(.+)')\n",
    "\n",
    "physics_genres = {}\n",
    "top_categories = {}\n",
    "\n",
    "for category, description in categories.items():\n",
    "    match = pattern.match(category)\n",
    "    if match:\n",
    "        physics_genres[match.group(1)] = description\n",
    "    else:\n",
    "        top_categories[category] = description\n",
    "        \n",
    "save_dict(physics_genres, \"physics_genres.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, *Economics* and *Electrical Engineering* start only in 2017 (I've checked), so we will exclude *econ* and *eess* from *top_categories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats = {cat: cat_name for (cat, cat_name) in top_categories.items() if cat not in  ['econ' ,'eess']}\n",
    "save_dict(top_cats, \"top_cats.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cs': 'Computer Science',\n",
       "  'math': 'Mathematics',\n",
       "  'physics': 'Physics',\n",
       "  'q-bio': 'Quantitative Biology',\n",
       "  'q-fin': 'Quantitative Finance',\n",
       "  'stat': 'Statistics'},\n",
       " {'astro-ph': 'Astrophysics',\n",
       "  'cond-mat': 'Condensed Matter',\n",
       "  'gr-qc': 'General Relativity and Quantum Cosmology',\n",
       "  'hep-ex': 'High Energy Physics - Experiment',\n",
       "  'hep-lat': 'High Energy Physics - Lattice',\n",
       "  'hep-ph': 'High Energy Physics - Phenomenology',\n",
       "  'hep-th': 'High Energy Physics - Theory',\n",
       "  'math-ph': 'Mathematical Physics',\n",
       "  'nlin': 'Nonlinear Sciences',\n",
       "  'nucl-ex': 'Nuclear Experiment',\n",
       "  'nucl-th': 'Nuclear Theory',\n",
       "  'physics': 'Physics (Other)',\n",
       "  'quant-ph': 'Quantum Physics'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cats, physics_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The API will serve us 1000 records each 10 seconds (plus a considerable overhead for communication)\n",
    "The imported *harvest_slice* function, given a time-slice, category and a file-path, works in a loop and\n",
    "    * sends the query\n",
    "    * saves the received records into a file\n",
    "    * using the last *resumption token* (appended to the xml) and the given dates forms a next query\n",
    "    * finally returns the number of retrieved records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_slice(date_from, date_until, category, file) -> int:\n",
    "    # returns number of downloaded records if succesful\n",
    "    \n",
    "    base_query = \"http://export.arxiv.org/oai2?verb=ListRecords\"\n",
    "    \n",
    "    if category == \"all\":\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&metadataPrefix=arXiv\"\n",
    "    else:\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&set={category}&metadataPrefix=arXiv\"\n",
    "    \n",
    "    retrieved = 0\n",
    "    \n",
    "    while query:\n",
    "        \n",
    "        time_0 = time.time()\n",
    "        \n",
    "        # try to download\n",
    "        try:            \n",
    "            sauce = urllib.request.urlopen(query).read()\n",
    "\n",
    "        except:\n",
    "            print(f\":( Failed requesting {query}\\nMoving on\")\n",
    "            break\n",
    "        \n",
    "        # parse the xml looking for <record>'s\n",
    "        soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "        records = soup.find_all('record')\n",
    "\n",
    "        retrieved = retrieved + len(records)\n",
    "\n",
    "        with open(file, \"a\", encoding='utf-8') as dump:\n",
    "\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            for record in records:                \n",
    "                record_string = [(record.id.string if record.id else 'nan'),\n",
    "                                 [(author.forenames.string+\" \" if author.forenames else \"\") + (author.keyname.string if author.keyname else 'nan') for author in record.find_all('author')],\n",
    "                                 (record.title.string if record.title else 'nan'),\n",
    "                                 (record.abstract.string if record.abstract else 'nan'),\n",
    "                                 (record.categories.string if record.categories else 'nan')\n",
    "                                ]\n",
    "                writer.writerow(record_string)\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"\".join([category,\" from \", f\"{date_from}\",\" until \", f\"{date_until}\",\" empty\"]))\n",
    "            break\n",
    "        \n",
    "        # info at the end of 'soup' about where to resume if the data stream was cut at 1000 records\n",
    "        # None if the stream wasn't cut\n",
    "        res_token = soup.find(\"resumptiontoken\")\n",
    "        \n",
    "        if res_token:\n",
    "\n",
    "            # data in the current loop started at this record in the 'query'\n",
    "            started_at = int(res_token['cursor']) + 1\n",
    "            \n",
    "            # total number of records in the 'query', should be the same in each loop\n",
    "            all_to_retrieve = int(res_token['completelistsize'])\n",
    "            \n",
    "            if res_token.string:\n",
    "                # the identifier that allows to resume the query\n",
    "                # None if the slice was completed\n",
    "\n",
    "                query = base_query + f\"&resumptionToken={res_token.string}\"\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                query = None\n",
    "            \n",
    "        else:\n",
    "            started_at = 1 \n",
    "            all_to_retrieve = len(records)\n",
    "            query = None\n",
    "        \n",
    "        time_1 = time.time()\n",
    "        \n",
    "        print(\"\".join([category,\n",
    "                       \" from \", f\"{date_from}\", \" until \", f\"{date_until}\",\n",
    "                       f\" ({started_at:>5}-{started_at+len(records)-1:>5})/{all_to_retrieve:>5}\",\n",
    "                      \" in \", f\"{(time_1 - time_0):3.2f}\", \"s\"]) )\n",
    "    \n",
    "    # end of while loop\n",
    "     \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are receiving the data in batches with 1000 records each. Each batch from the time period defined by the arguments ends up in the same file. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math from 2018-10-01 until 2018-10-10 (    1- 1000)/ 2328 in 24.69s\n",
      "math from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 2328 in 25.26s\n",
      "math from 2018-10-01 until 2018-10-10 ( 2001- 2328)/ 2328 in 6.21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to download different categories separately like that, the records that belong to more than one category would be repeated in each file. But we can be downloading all categories at the same time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2018-10-01 until 2018-10-10 (    1- 1000)/ 7482 in 26.81s\n",
      "all from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 7482 in 27.57s\n",
      "all from 2018-10-01 until 2018-10-10 ( 2001- 3000)/ 7482 in 26.85s\n",
      "all from 2018-10-01 until 2018-10-10 ( 3001- 4000)/ 7482 in 27.77s\n",
      "all from 2018-10-01 until 2018-10-10 ( 4001- 5000)/ 7482 in 28.97s\n",
      "all from 2018-10-01 until 2018-10-10 ( 5001- 6000)/ 7482 in 25.77s\n",
      "all from 2018-10-01 until 2018-10-10 ( 6001- 7000)/ 7482 in 32.03s\n",
      "all from 2018-10-01 until 2018-10-10 ( 7001- 7482)/ 7482 in 8.67s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just as a precaution, let's split longer time-slices into multiple shorter ones in case there is an upper limit for the total number of records we can retrieve with one query.\n",
    "We divide the time-slice into 92-days long (by default) periods, and write to an automatically named file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around harvest_slice\n",
    "# * handles file-names\n",
    "# * slices the time period of papers into intervals of given number of days (days_in_slice)\n",
    "\n",
    "def harvest_data(isoday_0, isoday_1, category='all', days_in_slice = 92, file_name=None, overwrite=False) -> int:\n",
    "\n",
    "    date_0 = dateutil.parser.parse(isoday_0).date()\n",
    "    date_1 = dateutil.parser.parse(isoday_1).date()\n",
    "\n",
    "    if not file_name:\n",
    "        # create a file with an overly descriptive name\n",
    "        file = f\"arXivMeta_{category.replace(':','--')}_from_{date_0}_to_{date_1}.csv\"\n",
    "    else:\n",
    "        file = file_name\n",
    "    \n",
    "    # check if file already exists\n",
    "    if Path(file).is_file():\n",
    "        if overwrite :\n",
    "\n",
    "            # try to backup the old file\n",
    "            file_info = re.match(r\"(\\w.+)\\.(\\w\\w+)\", file)\n",
    "            if file_info:\n",
    "                new_file = \"\".join([ file_info.group(1), \"_bak.\", file_info.group(2) ])\n",
    "                if not Path(new_file).is_file():\n",
    "                    os.rename(file, new_file)\n",
    "                    print(f\"Old file backed up as {new_file}\")\n",
    "\n",
    "            # clear the file\n",
    "            print(f\"Overwriting {file}\")\n",
    "            with open(file, \"w\") as dump:\n",
    "                dump.truncate(0)\n",
    "            \n",
    "        else:\n",
    "            print(f\"The file {file} already exists\")\n",
    "            return -1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Writing to {file}\")\n",
    "    \n",
    "    with open(file, \"a\") as dump:\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            header = ['id', 'authors', 'title', 'abstract', 'categories']\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    # Start the clock\n",
    "    time_0 = time.time()\n",
    "    \n",
    "    # Let's count all downloaded records\n",
    "    retrieved = 0\n",
    "    \n",
    "    # We'll go from 'date_0' until 'date_1' in slices of 'days_in_slice' days\n",
    "    # The server's response presumably maxes out at some number of records,\n",
    "    # so we hope to have slices with less records than that.\n",
    "\n",
    "    date_from = date_0\n",
    "\n",
    "    while date_from <= date_1:\n",
    "        \n",
    "        date_until = min(date_1, date_from + datetime.timedelta(days_in_slice-1))\n",
    "\n",
    "        # try to download the slice\n",
    "        newly_retrieved = harvest_slice(date_from, date_until, category, file)\n",
    "        retrieved = retrieved + newly_retrieved\n",
    "\n",
    "        # move on to the next slice\n",
    "        date_from = date_until + datetime.timedelta(days=1)\n",
    "        \n",
    "        # time-out\n",
    "        time.sleep(10)\n",
    "\n",
    "    time_1 = time.time()\n",
    "    \n",
    "    print(\"\".join([category,\n",
    "                   \" from \", str(date_0), \" until \", str(date_1),\n",
    "                   \" retrieved \", str(retrieved), \" records\"\n",
    "                   ,\" in \", f\"{(time_1 - time_0)/60:.0f}\", \" min\\n\"])\n",
    "         )\n",
    "    \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can do for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old file backed up as test_bak.csv\n",
      "Overwriting test.csv\n",
      "math from 2018-08-01 until 2018-10-31 (    1- 1000)/18206 in 43.39s\n",
      "math from 2018-08-01 until 2018-10-31 ( 1001- 2000)/18206 in 62.31s\n",
      "math from 2018-08-01 until 2018-10-31 ( 2001- 3000)/18206 in 103.59s\n",
      "math from 2018-08-01 until 2018-10-31 ( 3001- 4000)/18206 in 57.29s\n",
      "math from 2018-08-01 until 2018-10-31 ( 4001- 5000)/18206 in 90.82s\n",
      "math from 2018-08-01 until 2018-10-31 ( 5001- 6000)/18206 in 54.15s\n",
      "math from 2018-08-01 until 2018-10-31 ( 6001- 7000)/18206 in 46.50s\n",
      "math from 2018-08-01 until 2018-10-31 ( 7001- 8000)/18206 in 31.82s\n",
      "math from 2018-08-01 until 2018-10-31 ( 8001- 9000)/18206 in 45.30s\n",
      "math from 2018-08-01 until 2018-10-31 ( 9001-10000)/18206 in 25.56s\n",
      "math from 2018-08-01 until 2018-10-31 (10001-11000)/18206 in 26.49s\n",
      "math from 2018-08-01 until 2018-10-31 (11001-12000)/18206 in 27.74s\n",
      "math from 2018-08-01 until 2018-10-31 (12001-13000)/18206 in 25.49s\n",
      "math from 2018-08-01 until 2018-10-31 (13001-14000)/18206 in 24.23s\n",
      "math from 2018-08-01 until 2018-10-31 (14001-15000)/18206 in 27.26s\n",
      "math from 2018-08-01 until 2018-10-31 (15001-16000)/18206 in 25.01s\n",
      "math from 2018-08-01 until 2018-10-31 (16001-17000)/18206 in 25.85s\n",
      "math from 2018-08-01 until 2018-10-31 (17001-18000)/18206 in 26.64s\n",
      "math from 2018-08-01 until 2018-10-31 (18001-18206)/18206 in 6.48s\n",
      "math from 2018-11-01 until 2018-11-01 (    1-  262)/  262 in 6.96s\n",
      "math from 2018-08-01 until 2018-11-01 retrieved 18468 records in 13 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18468"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name=\"test.csv\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there were two time-slices, one with 18206 records, second with 262 records. All saved in \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_all_from_2010-01-01_to_2010-07-01.csv\n",
      "all from 2010-01-01 until 2010-04-02 (    1- 1000)/13727 in 34.98s\n",
      "all from 2010-01-01 until 2010-04-02 ( 1001- 2000)/13727 in 34.47s\n",
      "all from 2010-01-01 until 2010-04-02 ( 2001- 3000)/13727 in 39.02s\n",
      "all from 2010-01-01 until 2010-04-02 ( 3001- 4000)/13727 in 37.09s\n",
      "all from 2010-01-01 until 2010-04-02 ( 4001- 5000)/13727 in 32.30s\n",
      "all from 2010-01-01 until 2010-04-02 ( 5001- 6000)/13727 in 32.11s\n",
      "all from 2010-01-01 until 2010-04-02 ( 6001- 7000)/13727 in 96.90s\n",
      "all from 2010-01-01 until 2010-04-02 ( 7001- 8000)/13727 in 65.82s\n",
      "all from 2010-01-01 until 2010-04-02 ( 8001- 9000)/13727 in 76.95s\n",
      "all from 2010-01-01 until 2010-04-02 ( 9001-10000)/13727 in 90.02s\n",
      "all from 2010-01-01 until 2010-04-02 (10001-11000)/13727 in 83.90s\n",
      "all from 2010-01-01 until 2010-04-02 (11001-12000)/13727 in 90.99s\n",
      "all from 2010-01-01 until 2010-04-02 (12001-13000)/13727 in 71.82s\n",
      "all from 2010-01-01 until 2010-04-02 (13001-13727)/13727 in 36.72s\n",
      "all from 2010-04-03 until 2010-07-01 (    1- 1000)/15366 in 51.50s\n",
      "all from 2010-04-03 until 2010-07-01 ( 1001- 2000)/15366 in 80.02s\n",
      "all from 2010-04-03 until 2010-07-01 ( 2001- 3000)/15366 in 35.78s\n",
      "all from 2010-04-03 until 2010-07-01 ( 3001- 4000)/15366 in 31.35s\n",
      "all from 2010-04-03 until 2010-07-01 ( 4001- 5000)/15366 in 33.55s\n",
      "all from 2010-04-03 until 2010-07-01 ( 5001- 6000)/15366 in 31.94s\n",
      "all from 2010-04-03 until 2010-07-01 ( 6001- 7000)/15366 in 33.26s\n",
      "all from 2010-04-03 until 2010-07-01 ( 7001- 8000)/15366 in 30.22s\n",
      "all from 2010-04-03 until 2010-07-01 ( 8001- 9000)/15366 in 29.24s\n",
      "all from 2010-04-03 until 2010-07-01 ( 9001-10000)/15366 in 28.46s\n",
      "all from 2010-04-03 until 2010-07-01 (10001-11000)/15366 in 29.48s\n",
      "all from 2010-04-03 until 2010-07-01 (11001-12000)/15366 in 31.97s\n",
      "all from 2010-04-03 until 2010-07-01 (12001-13000)/15366 in 28.27s\n",
      "all from 2010-04-03 until 2010-07-01 (13001-14000)/15366 in 30.02s\n",
      "all from 2010-04-03 until 2010-07-01 (14001-15000)/15366 in 28.78s\n",
      "all from 2010-04-03 until 2010-07-01 (15001-15366)/15366 in 9.74s\n",
      "all from 2010-01-01 until 2010-07-01 retrieved 29093 records in 23 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2010-07-02_to_2010-12-31.csv\n",
      "all from 2010-07-02 until 2010-10-01 (    1- 1000)/ 9903 in 30.93s\n",
      "all from 2010-07-02 until 2010-10-01 ( 1001- 2000)/ 9903 in 31.10s\n",
      "all from 2010-07-02 until 2010-10-01 ( 2001- 3000)/ 9903 in 33.49s\n",
      "all from 2010-07-02 until 2010-10-01 ( 3001- 4000)/ 9903 in 30.86s\n",
      "all from 2010-07-02 until 2010-10-01 ( 4001- 5000)/ 9903 in 28.82s\n",
      "all from 2010-07-02 until 2010-10-01 ( 5001- 6000)/ 9903 in 30.53s\n",
      "all from 2010-07-02 until 2010-10-01 ( 6001- 7000)/ 9903 in 30.32s\n",
      "all from 2010-07-02 until 2010-10-01 ( 7001- 8000)/ 9903 in 31.95s\n",
      "all from 2010-07-02 until 2010-10-01 ( 8001- 9000)/ 9903 in 32.76s\n",
      "all from 2010-07-02 until 2010-10-01 ( 9001- 9903)/ 9903 in 16.92s\n",
      "all from 2010-10-02 until 2010-12-31 (    1- 1000)/18258 in 36.20s\n",
      "all from 2010-10-02 until 2010-12-31 ( 1001- 2000)/18258 in 30.86s\n",
      "all from 2010-10-02 until 2010-12-31 ( 2001- 3000)/18258 in 34.01s\n",
      "all from 2010-10-02 until 2010-12-31 ( 3001- 4000)/18258 in 34.71s\n",
      "all from 2010-10-02 until 2010-12-31 ( 4001- 5000)/18258 in 36.64s\n",
      "all from 2010-10-02 until 2010-12-31 ( 5001- 6000)/18258 in 32.36s\n",
      "all from 2010-10-02 until 2010-12-31 ( 6001- 7000)/18258 in 31.23s\n",
      "all from 2010-10-02 until 2010-12-31 ( 7001- 8000)/18258 in 28.89s\n",
      "all from 2010-10-02 until 2010-12-31 ( 8001- 9000)/18258 in 30.15s\n",
      "all from 2010-10-02 until 2010-12-31 ( 9001-10000)/18258 in 29.61s\n",
      "all from 2010-10-02 until 2010-12-31 (10001-11000)/18258 in 28.80s\n",
      "all from 2010-10-02 until 2010-12-31 (11001-12000)/18258 in 32.09s\n",
      "all from 2010-10-02 until 2010-12-31 (12001-13000)/18258 in 27.22s\n",
      "all from 2010-10-02 until 2010-12-31 (13001-14000)/18258 in 30.75s\n",
      "all from 2010-10-02 until 2010-12-31 (14001-15000)/18258 in 27.10s\n",
      "all from 2010-10-02 until 2010-12-31 (15001-16000)/18258 in 27.80s\n",
      "all from 2010-10-02 until 2010-12-31 (16001-17000)/18258 in 26.36s\n",
      "all from 2010-10-02 until 2010-12-31 (17001-18000)/18258 in 27.54s\n",
      "all from 2010-10-02 until 2010-12-31 (18001-18258)/18258 in 7.29s\n",
      "all from 2010-07-02 until 2010-12-31 retrieved 28161 records in 15 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28161"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single year split in two files\n",
    "year = 2010\n",
    "\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-07-01\")\n",
    "harvest_data(f\"{year}-07-02\", f\"{year}-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_all_from_2011-01-01_to_2011-07-01.csv\n",
      "all from 2011-01-01 until 2011-04-02 (    1- 1000)/16544 in 28.84s\n",
      "all from 2011-01-01 until 2011-04-02 ( 1001- 2000)/16544 in 28.68s\n",
      "all from 2011-01-01 until 2011-04-02 ( 2001- 3000)/16544 in 26.71s\n",
      "all from 2011-01-01 until 2011-04-02 ( 3001- 4000)/16544 in 26.42s\n",
      "all from 2011-01-01 until 2011-04-02 ( 4001- 5000)/16544 in 27.60s\n",
      "all from 2011-01-01 until 2011-04-02 ( 5001- 6000)/16544 in 26.40s\n",
      "all from 2011-01-01 until 2011-04-02 ( 6001- 7000)/16544 in 28.16s\n",
      "all from 2011-01-01 until 2011-04-02 ( 7001- 8000)/16544 in 29.50s\n",
      "all from 2011-01-01 until 2011-04-02 ( 8001- 9000)/16544 in 25.17s\n",
      "all from 2011-01-01 until 2011-04-02 ( 9001-10000)/16544 in 24.91s\n",
      "all from 2011-01-01 until 2011-04-02 (10001-11000)/16544 in 25.32s\n",
      "all from 2011-01-01 until 2011-04-02 (11001-12000)/16544 in 24.55s\n",
      "all from 2011-01-01 until 2011-04-02 (12001-13000)/16544 in 25.32s\n",
      "all from 2011-01-01 until 2011-04-02 (13001-14000)/16544 in 27.80s\n",
      "all from 2011-01-01 until 2011-04-02 (14001-15000)/16544 in 26.62s\n",
      "all from 2011-01-01 until 2011-04-02 (15001-16000)/16544 in 25.85s\n",
      "all from 2011-01-01 until 2011-04-02 (16001-16544)/16544 in 9.13s\n",
      "all from 2011-04-03 until 2011-07-01 (    1- 1000)/14922 in 29.98s\n",
      "all from 2011-04-03 until 2011-07-01 ( 1001- 2000)/14922 in 34.91s\n",
      "all from 2011-04-03 until 2011-07-01 ( 2001- 3000)/14922 in 26.10s\n",
      "all from 2011-04-03 until 2011-07-01 ( 3001- 4000)/14922 in 83.38s\n",
      "all from 2011-04-03 until 2011-07-01 ( 4001- 5000)/14922 in 41.11s\n",
      "all from 2011-04-03 until 2011-07-01 ( 5001- 6000)/14922 in 76.31s\n",
      "all from 2011-04-03 until 2011-07-01 ( 6001- 7000)/14922 in 109.28s\n",
      "all from 2011-04-03 until 2011-07-01 ( 7001- 8000)/14922 in 63.13s\n",
      "all from 2011-04-03 until 2011-07-01 ( 8001- 9000)/14922 in 25.02s\n",
      "all from 2011-04-03 until 2011-07-01 ( 9001-10000)/14922 in 76.29s\n",
      "all from 2011-04-03 until 2011-07-01 (10001-11000)/14922 in 67.03s\n",
      "all from 2011-04-03 until 2011-07-01 (11001-12000)/14922 in 67.09s\n",
      "all from 2011-04-03 until 2011-07-01 (12001-13000)/14922 in 92.17s\n",
      "all from 2011-04-03 until 2011-07-01 (13001-14000)/14922 in 101.21s\n",
      "all from 2011-04-03 until 2011-07-01 (14001-14922)/14922 in 72.85s\n",
      "all from 2011-01-01 until 2011-07-01 retrieved 31466 records in 24 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2011-07-02_to_2011-12-31.csv\n",
      "all from 2011-07-02 until 2011-10-01 (    1- 1000)/16923 in 57.42s\n",
      "all from 2011-07-02 until 2011-10-01 ( 1001- 2000)/16923 in 78.80s\n",
      "all from 2011-07-02 until 2011-10-01 ( 2001- 3000)/16923 in 25.97s\n",
      "all from 2011-07-02 until 2011-10-01 ( 3001- 4000)/16923 in 25.53s\n",
      "all from 2011-07-02 until 2011-10-01 ( 4001- 5000)/16923 in 26.69s\n",
      "all from 2011-07-02 until 2011-10-01 ( 5001- 6000)/16923 in 27.17s\n",
      "all from 2011-07-02 until 2011-10-01 ( 6001- 7000)/16923 in 27.66s\n",
      "all from 2011-07-02 until 2011-10-01 ( 7001- 8000)/16923 in 24.64s\n",
      "all from 2011-07-02 until 2011-10-01 ( 8001- 9000)/16923 in 26.01s\n",
      "all from 2011-07-02 until 2011-10-01 ( 9001-10000)/16923 in 25.29s\n",
      "all from 2011-07-02 until 2011-10-01 (10001-11000)/16923 in 26.23s\n",
      "all from 2011-07-02 until 2011-10-01 (11001-12000)/16923 in 25.19s\n",
      "all from 2011-07-02 until 2011-10-01 (12001-13000)/16923 in 24.96s\n",
      "all from 2011-07-02 until 2011-10-01 (13001-14000)/16923 in 31.55s\n",
      "all from 2011-07-02 until 2011-10-01 (14001-15000)/16923 in 31.94s\n",
      "all from 2011-07-02 until 2011-10-01 (15001-16000)/16923 in 32.06s\n",
      "all from 2011-07-02 until 2011-10-01 (16001-16923)/16923 in 21.04s\n",
      "all from 2011-10-02 until 2011-12-31 (    1- 1000)/14713 in 31.35s\n",
      "all from 2011-10-02 until 2011-12-31 ( 1001- 2000)/14713 in 27.84s\n",
      "all from 2011-10-02 until 2011-12-31 ( 2001- 3000)/14713 in 26.12s\n",
      "all from 2011-10-02 until 2011-12-31 ( 3001- 4000)/14713 in 26.59s\n",
      "all from 2011-10-02 until 2011-12-31 ( 4001- 5000)/14713 in 26.42s\n",
      "all from 2011-10-02 until 2011-12-31 ( 5001- 6000)/14713 in 26.93s\n",
      "all from 2011-10-02 until 2011-12-31 ( 6001- 7000)/14713 in 24.75s\n",
      "all from 2011-10-02 until 2011-12-31 ( 7001- 8000)/14713 in 25.06s\n",
      "all from 2011-10-02 until 2011-12-31 ( 8001- 9000)/14713 in 44.10s\n",
      "all from 2011-10-02 until 2011-12-31 ( 9001-10000)/14713 in 55.48s\n",
      "all from 2011-10-02 until 2011-12-31 (10001-11000)/14713 in 49.23s\n",
      "all from 2011-10-02 until 2011-12-31 (11001-12000)/14713 in 129.08s\n",
      "all from 2011-10-02 until 2011-12-31 (12001-13000)/14713 in 187.26s\n",
      "all from 2011-10-02 until 2011-12-31 (13001-14000)/14713 in 193.35s\n",
      "all from 2011-10-02 until 2011-12-31 (14001-14713)/14713 in 68.84s\n",
      "all from 2011-07-02 until 2011-12-31 retrieved 31636 records in 25 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2012-01-01_to_2012-07-01.csv\n",
      "all from 2012-01-01 until 2012-04-01 (    1- 1000)/13381 in 59.30s\n",
      "all from 2012-01-01 until 2012-04-01 ( 1001- 2000)/13381 in 27.16s\n",
      "all from 2012-01-01 until 2012-04-01 ( 2001- 3000)/13381 in 26.12s\n",
      "all from 2012-01-01 until 2012-04-01 ( 3001- 4000)/13381 in 28.70s\n",
      "all from 2012-01-01 until 2012-04-01 ( 4001- 5000)/13381 in 27.56s\n",
      "all from 2012-01-01 until 2012-04-01 ( 5001- 6000)/13381 in 28.16s\n",
      "all from 2012-01-01 until 2012-04-01 ( 6001- 7000)/13381 in 25.44s\n",
      "all from 2012-01-01 until 2012-04-01 ( 7001- 8000)/13381 in 24.87s\n",
      "all from 2012-01-01 until 2012-04-01 ( 8001- 9000)/13381 in 26.43s\n",
      "all from 2012-01-01 until 2012-04-01 ( 9001-10000)/13381 in 29.33s\n",
      "all from 2012-01-01 until 2012-04-01 (10001-11000)/13381 in 56.99s\n",
      "all from 2012-01-01 until 2012-04-01 (11001-12000)/13381 in 50.82s\n",
      "all from 2012-01-01 until 2012-04-01 (12001-13000)/13381 in 84.24s\n",
      "all from 2012-01-01 until 2012-04-01 (13001-13381)/13381 in 98.54s\n",
      "all from 2012-04-02 until 2012-07-01 (    1- 1000)/13751 in 113.64s\n",
      "all from 2012-04-02 until 2012-07-01 ( 1001- 2000)/13751 in 55.86s\n",
      "all from 2012-04-02 until 2012-07-01 ( 2001- 3000)/13751 in 131.11s\n",
      "all from 2012-04-02 until 2012-07-01 ( 3001- 4000)/13751 in 38.80s\n",
      "all from 2012-04-02 until 2012-07-01 ( 4001- 5000)/13751 in 44.43s\n",
      "all from 2012-04-02 until 2012-07-01 ( 5001- 6000)/13751 in 68.76s\n",
      "all from 2012-04-02 until 2012-07-01 ( 6001- 7000)/13751 in 86.12s\n",
      "all from 2012-04-02 until 2012-07-01 ( 7001- 8000)/13751 in 70.61s\n",
      "all from 2012-04-02 until 2012-07-01 ( 8001- 9000)/13751 in 42.07s\n",
      "all from 2012-04-02 until 2012-07-01 ( 9001-10000)/13751 in 113.85s\n",
      "all from 2012-04-02 until 2012-07-01 (10001-11000)/13751 in 78.66s\n",
      "all from 2012-04-02 until 2012-07-01 (11001-12000)/13751 in 30.21s\n",
      "all from 2012-04-02 until 2012-07-01 (12001-13000)/13751 in 25.81s\n",
      "all from 2012-04-02 until 2012-07-01 (13001-13751)/13751 in 14.66s\n",
      "all from 2012-01-01 until 2012-07-01 retrieved 27132 records in 25 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2012-07-02_to_2012-12-31.csv\n",
      "all from 2012-07-02 until 2012-10-01 (    1- 1000)/16562 in 31.20s\n",
      "all from 2012-07-02 until 2012-10-01 ( 1001- 2000)/16562 in 31.69s\n",
      "all from 2012-07-02 until 2012-10-01 ( 2001- 3000)/16562 in 29.31s\n",
      "all from 2012-07-02 until 2012-10-01 ( 3001- 4000)/16562 in 30.63s\n",
      "all from 2012-07-02 until 2012-10-01 ( 4001- 5000)/16562 in 29.48s\n",
      "all from 2012-07-02 until 2012-10-01 ( 5001- 6000)/16562 in 31.84s\n",
      "all from 2012-07-02 until 2012-10-01 ( 6001- 7000)/16562 in 28.97s\n",
      "all from 2012-07-02 until 2012-10-01 ( 7001- 8000)/16562 in 27.07s\n",
      "all from 2012-07-02 until 2012-10-01 ( 8001- 9000)/16562 in 25.91s\n",
      "all from 2012-07-02 until 2012-10-01 ( 9001-10000)/16562 in 26.35s\n",
      "all from 2012-07-02 until 2012-10-01 (10001-11000)/16562 in 34.59s\n",
      "all from 2012-07-02 until 2012-10-01 (11001-12000)/16562 in 25.75s\n",
      "all from 2012-07-02 until 2012-10-01 (12001-13000)/16562 in 40.86s\n",
      "all from 2012-07-02 until 2012-10-01 (13001-14000)/16562 in 24.77s\n",
      "all from 2012-07-02 until 2012-10-01 (14001-15000)/16562 in 28.05s\n",
      "all from 2012-07-02 until 2012-10-01 (15001-16000)/16562 in 33.35s\n",
      "all from 2012-07-02 until 2012-10-01 (16001-16562)/16562 in 14.29s\n",
      "all from 2012-10-02 until 2012-12-31 (    1- 1000)/14867 in 28.64s\n",
      "all from 2012-10-02 until 2012-12-31 ( 1001- 2000)/14867 in 26.98s\n",
      "all from 2012-10-02 until 2012-12-31 ( 2001- 3000)/14867 in 27.00s\n",
      "all from 2012-10-02 until 2012-12-31 ( 3001- 4000)/14867 in 28.92s\n",
      "all from 2012-10-02 until 2012-12-31 ( 4001- 5000)/14867 in 27.70s\n",
      "all from 2012-10-02 until 2012-12-31 ( 5001- 6000)/14867 in 29.08s\n",
      "all from 2012-10-02 until 2012-12-31 ( 6001- 7000)/14867 in 26.46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2012-10-02 until 2012-12-31 ( 7001- 8000)/14867 in 25.13s\n",
      "all from 2012-10-02 until 2012-12-31 ( 8001- 9000)/14867 in 26.69s\n",
      "all from 2012-10-02 until 2012-12-31 ( 9001-10000)/14867 in 26.10s\n",
      "all from 2012-10-02 until 2012-12-31 (10001-11000)/14867 in 25.93s\n",
      "all from 2012-10-02 until 2012-12-31 (11001-12000)/14867 in 26.02s\n",
      "all from 2012-10-02 until 2012-12-31 (12001-13000)/14867 in 29.35s\n",
      "all from 2012-10-02 until 2012-12-31 (13001-14000)/14867 in 25.38s\n",
      "all from 2012-10-02 until 2012-12-31 (14001-14867)/14867 in 23.47s\n",
      "all from 2012-07-02 until 2012-12-31 retrieved 31429 records in 15 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2013-01-01_to_2013-07-01.csv\n",
      "all from 2013-01-01 until 2013-04-02 (    1- 1000)/17385 in 28.06s\n",
      "all from 2013-01-01 until 2013-04-02 ( 1001- 2000)/17385 in 25.14s\n",
      "all from 2013-01-01 until 2013-04-02 ( 2001- 3000)/17385 in 27.22s\n",
      "all from 2013-01-01 until 2013-04-02 ( 3001- 4000)/17385 in 29.03s\n",
      "all from 2013-01-01 until 2013-04-02 ( 4001- 5000)/17385 in 28.02s\n",
      "all from 2013-01-01 until 2013-04-02 ( 5001- 6000)/17385 in 28.13s\n",
      "all from 2013-01-01 until 2013-04-02 ( 6001- 7000)/17385 in 29.38s\n",
      "all from 2013-01-01 until 2013-04-02 ( 7001- 8000)/17385 in 27.75s\n",
      "all from 2013-01-01 until 2013-04-02 ( 8001- 9000)/17385 in 36.30s\n",
      "all from 2013-01-01 until 2013-04-02 ( 9001-10000)/17385 in 25.90s\n",
      "all from 2013-01-01 until 2013-04-02 (10001-11000)/17385 in 30.83s\n",
      "all from 2013-01-01 until 2013-04-02 (11001-12000)/17385 in 25.66s\n",
      "all from 2013-01-01 until 2013-04-02 (12001-13000)/17385 in 24.89s\n",
      "all from 2013-01-01 until 2013-04-02 (13001-14000)/17385 in 25.82s\n",
      "all from 2013-01-01 until 2013-04-02 (14001-15000)/17385 in 27.78s\n",
      "all from 2013-01-01 until 2013-04-02 (15001-16000)/17385 in 26.07s\n",
      "all from 2013-01-01 until 2013-04-02 (16001-17000)/17385 in 26.93s\n",
      "all from 2013-01-01 until 2013-04-02 (17001-17385)/17385 in 8.86s\n",
      "all from 2013-04-03 until 2013-07-01 (    1- 1000)/21323 in 50.04s\n",
      "all from 2013-04-03 until 2013-07-01 ( 1001- 2000)/21323 in 54.08s\n",
      "all from 2013-04-03 until 2013-07-01 ( 2001- 3000)/21323 in 77.45s\n",
      "all from 2013-04-03 until 2013-07-01 ( 3001- 4000)/21323 in 65.76s\n",
      "all from 2013-04-03 until 2013-07-01 ( 4001- 5000)/21323 in 47.71s\n",
      "all from 2013-04-03 until 2013-07-01 ( 5001- 6000)/21323 in 73.81s\n",
      "all from 2013-04-03 until 2013-07-01 ( 6001- 7000)/21323 in 71.83s\n",
      "all from 2013-04-03 until 2013-07-01 ( 7001- 8000)/21323 in 49.85s\n",
      "all from 2013-04-03 until 2013-07-01 ( 8001- 9000)/21323 in 51.70s\n",
      "all from 2013-04-03 until 2013-07-01 ( 9001-10000)/21323 in 74.19s\n",
      "all from 2013-04-03 until 2013-07-01 (10001-11000)/21323 in 40.15s\n",
      "all from 2013-04-03 until 2013-07-01 (11001-12000)/21323 in 29.03s\n",
      "all from 2013-04-03 until 2013-07-01 (12001-13000)/21323 in 26.22s\n",
      "all from 2013-04-03 until 2013-07-01 (13001-14000)/21323 in 25.69s\n",
      "all from 2013-04-03 until 2013-07-01 (14001-15000)/21323 in 25.21s\n",
      "all from 2013-04-03 until 2013-07-01 (15001-16000)/21323 in 25.94s\n",
      "all from 2013-04-03 until 2013-07-01 (16001-17000)/21323 in 25.24s\n",
      "all from 2013-04-03 until 2013-07-01 (17001-18000)/21323 in 25.74s\n",
      "all from 2013-04-03 until 2013-07-01 (18001-19000)/21323 in 27.51s\n",
      "all from 2013-04-03 until 2013-07-01 (19001-20000)/21323 in 27.20s\n",
      "all from 2013-04-03 until 2013-07-01 (20001-21000)/21323 in 25.80s\n",
      "all from 2013-04-03 until 2013-07-01 (21001-21323)/21323 in 7.04s\n",
      "all from 2013-01-01 until 2013-07-01 retrieved 38708 records in 24 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2013-07-02_to_2013-12-31.csv\n",
      "all from 2013-07-02 until 2013-10-01 (    1- 1000)/17178 in 29.08s\n",
      "all from 2013-07-02 until 2013-10-01 ( 1001- 2000)/17178 in 27.92s\n",
      "all from 2013-07-02 until 2013-10-01 ( 2001- 3000)/17178 in 27.68s\n",
      "all from 2013-07-02 until 2013-10-01 ( 3001- 4000)/17178 in 27.28s\n",
      "all from 2013-07-02 until 2013-10-01 ( 4001- 5000)/17178 in 27.59s\n",
      "all from 2013-07-02 until 2013-10-01 ( 5001- 6000)/17178 in 28.19s\n",
      "all from 2013-07-02 until 2013-10-01 ( 6001- 7000)/17178 in 26.75s\n",
      "all from 2013-07-02 until 2013-10-01 ( 7001- 8000)/17178 in 28.82s\n",
      "all from 2013-07-02 until 2013-10-01 ( 8001- 9000)/17178 in 27.03s\n",
      "all from 2013-07-02 until 2013-10-01 ( 9001-10000)/17178 in 27.78s\n",
      "all from 2013-07-02 until 2013-10-01 (10001-11000)/17178 in 31.58s\n",
      "all from 2013-07-02 until 2013-10-01 (11001-12000)/17178 in 27.73s\n",
      "all from 2013-07-02 until 2013-10-01 (12001-13000)/17178 in 26.01s\n",
      "all from 2013-07-02 until 2013-10-01 (13001-14000)/17178 in 25.96s\n",
      "all from 2013-07-02 until 2013-10-01 (14001-15000)/17178 in 27.01s\n",
      "all from 2013-07-02 until 2013-10-01 (15001-16000)/17178 in 25.63s\n",
      "all from 2013-07-02 until 2013-10-01 (16001-17000)/17178 in 27.36s\n",
      "all from 2013-07-02 until 2013-10-01 (17001-17178)/17178 in 5.83s\n",
      "all from 2013-10-02 until 2013-12-31 (    1- 1000)/17887 in 28.30s\n",
      "all from 2013-10-02 until 2013-12-31 ( 1001- 2000)/17887 in 27.20s\n",
      "all from 2013-10-02 until 2013-12-31 ( 2001- 3000)/17887 in 27.17s\n",
      "all from 2013-10-02 until 2013-12-31 ( 3001- 4000)/17887 in 28.15s\n",
      "all from 2013-10-02 until 2013-12-31 ( 4001- 5000)/17887 in 27.77s\n",
      "all from 2013-10-02 until 2013-12-31 ( 5001- 6000)/17887 in 27.34s\n",
      "all from 2013-10-02 until 2013-12-31 ( 6001- 7000)/17887 in 29.32s\n",
      "all from 2013-10-02 until 2013-12-31 ( 7001- 8000)/17887 in 29.25s\n",
      "all from 2013-10-02 until 2013-12-31 ( 8001- 9000)/17887 in 30.92s\n",
      "all from 2013-10-02 until 2013-12-31 ( 9001-10000)/17887 in 31.38s\n",
      "all from 2013-10-02 until 2013-12-31 (10001-11000)/17887 in 25.22s\n",
      "all from 2013-10-02 until 2013-12-31 (11001-12000)/17887 in 24.90s\n",
      "all from 2013-10-02 until 2013-12-31 (12001-13000)/17887 in 26.54s\n",
      "all from 2013-10-02 until 2013-12-31 (13001-14000)/17887 in 25.94s\n",
      "all from 2013-10-02 until 2013-12-31 (14001-15000)/17887 in 24.81s\n",
      "all from 2013-10-02 until 2013-12-31 (15001-16000)/17887 in 25.07s\n",
      "all from 2013-10-02 until 2013-12-31 (16001-17000)/17887 in 25.35s\n",
      "all from 2013-10-02 until 2013-12-31 (17001-17887)/17887 in 14.29s\n",
      "all from 2013-07-02 until 2013-12-31 retrieved 35065 records in 16 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2014-01-01_to_2014-07-01.csv\n",
      "all from 2014-01-01 until 2014-04-02 (    1- 1000)/20163 in 26.22s\n",
      "all from 2014-01-01 until 2014-04-02 ( 1001- 2000)/20163 in 26.26s\n",
      "all from 2014-01-01 until 2014-04-02 ( 2001- 3000)/20163 in 25.16s\n",
      "all from 2014-01-01 until 2014-04-02 ( 3001- 4000)/20163 in 26.31s\n",
      "all from 2014-01-01 until 2014-04-02 ( 4001- 5000)/20163 in 25.69s\n",
      "all from 2014-01-01 until 2014-04-02 ( 5001- 6000)/20163 in 32.17s\n",
      "all from 2014-01-01 until 2014-04-02 ( 6001- 7000)/20163 in 31.17s\n",
      "all from 2014-01-01 until 2014-04-02 ( 7001- 8000)/20163 in 31.02s\n",
      "all from 2014-01-01 until 2014-04-02 ( 8001- 9000)/20163 in 30.74s\n",
      "all from 2014-01-01 until 2014-04-02 ( 9001-10000)/20163 in 29.05s\n",
      "all from 2014-01-01 until 2014-04-02 (10001-11000)/20163 in 28.34s\n",
      "all from 2014-01-01 until 2014-04-02 (11001-12000)/20163 in 30.57s\n",
      "all from 2014-01-01 until 2014-04-02 (12001-13000)/20163 in 33.38s\n",
      "all from 2014-01-01 until 2014-04-02 (13001-14000)/20163 in 28.42s\n",
      "all from 2014-01-01 until 2014-04-02 (14001-15000)/20163 in 29.69s\n",
      "all from 2014-01-01 until 2014-04-02 (15001-16000)/20163 in 29.50s\n",
      "all from 2014-01-01 until 2014-04-02 (16001-17000)/20163 in 30.30s\n",
      "all from 2014-01-01 until 2014-04-02 (17001-18000)/20163 in 25.60s\n",
      "all from 2014-01-01 until 2014-04-02 (18001-19000)/20163 in 28.51s\n",
      "all from 2014-01-01 until 2014-04-02 (19001-20000)/20163 in 28.39s\n",
      "all from 2014-01-01 until 2014-04-02 (20001-20163)/20163 in 5.55s\n",
      "all from 2014-04-03 until 2014-07-01 (    1- 1000)/17697 in 28.74s\n",
      "all from 2014-04-03 until 2014-07-01 ( 1001- 2000)/17697 in 28.88s\n",
      "all from 2014-04-03 until 2014-07-01 ( 2001- 3000)/17697 in 29.53s\n",
      "all from 2014-04-03 until 2014-07-01 ( 3001- 4000)/17697 in 31.61s\n",
      "all from 2014-04-03 until 2014-07-01 ( 4001- 5000)/17697 in 28.61s\n",
      "all from 2014-04-03 until 2014-07-01 ( 5001- 6000)/17697 in 29.55s\n",
      "all from 2014-04-03 until 2014-07-01 ( 6001- 7000)/17697 in 29.75s\n",
      "all from 2014-04-03 until 2014-07-01 ( 7001- 8000)/17697 in 35.15s\n",
      "all from 2014-04-03 until 2014-07-01 ( 8001- 9000)/17697 in 30.37s\n",
      "all from 2014-04-03 until 2014-07-01 ( 9001-10000)/17697 in 27.76s\n",
      "all from 2014-04-03 until 2014-07-01 (10001-11000)/17697 in 26.60s\n",
      "all from 2014-04-03 until 2014-07-01 (11001-12000)/17697 in 27.47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2014-04-03 until 2014-07-01 (12001-13000)/17697 in 29.61s\n",
      "all from 2014-04-03 until 2014-07-01 (13001-14000)/17697 in 28.44s\n",
      "all from 2014-04-03 until 2014-07-01 (14001-15000)/17697 in 25.37s\n",
      "all from 2014-04-03 until 2014-07-01 (15001-16000)/17697 in 27.12s\n",
      "all from 2014-04-03 until 2014-07-01 (16001-17000)/17697 in 27.32s\n",
      "all from 2014-04-03 until 2014-07-01 (17001-17697)/17697 in 14.97s\n",
      "all from 2014-01-01 until 2014-07-01 retrieved 37860 records in 18 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2014-07-02_to_2014-12-31.csv\n",
      "all from 2014-07-02 until 2014-10-01 (    1- 1000)/19557 in 34.67s\n",
      "all from 2014-07-02 until 2014-10-01 ( 1001- 2000)/19557 in 27.18s\n",
      "all from 2014-07-02 until 2014-10-01 ( 2001- 3000)/19557 in 27.03s\n",
      "all from 2014-07-02 until 2014-10-01 ( 3001- 4000)/19557 in 28.25s\n",
      "all from 2014-07-02 until 2014-10-01 ( 4001- 5000)/19557 in 27.63s\n",
      "all from 2014-07-02 until 2014-10-01 ( 5001- 6000)/19557 in 27.80s\n",
      "all from 2014-07-02 until 2014-10-01 ( 6001- 7000)/19557 in 27.60s\n",
      "all from 2014-07-02 until 2014-10-01 ( 7001- 8000)/19557 in 28.62s\n",
      "all from 2014-07-02 until 2014-10-01 ( 8001- 9000)/19557 in 33.27s\n",
      "all from 2014-07-02 until 2014-10-01 ( 9001-10000)/19557 in 28.98s\n",
      "all from 2014-07-02 until 2014-10-01 (10001-11000)/19557 in 29.15s\n",
      "all from 2014-07-02 until 2014-10-01 (11001-12000)/19557 in 26.28s\n",
      "all from 2014-07-02 until 2014-10-01 (12001-13000)/19557 in 25.53s\n",
      "all from 2014-07-02 until 2014-10-01 (13001-14000)/19557 in 25.63s\n",
      "all from 2014-07-02 until 2014-10-01 (14001-15000)/19557 in 25.67s\n",
      "all from 2014-07-02 until 2014-10-01 (15001-16000)/19557 in 33.96s\n",
      "all from 2014-07-02 until 2014-10-01 (16001-17000)/19557 in 26.17s\n",
      "all from 2014-07-02 until 2014-10-01 (17001-18000)/19557 in 25.60s\n",
      "all from 2014-07-02 until 2014-10-01 (18001-19000)/19557 in 24.73s\n",
      "all from 2014-07-02 until 2014-10-01 (19001-19557)/19557 in 10.34s\n",
      "all from 2014-10-02 until 2014-12-31 (    1- 1000)/33387 in 29.18s\n",
      "all from 2014-10-02 until 2014-12-31 ( 1001- 2000)/33387 in 30.17s\n",
      "all from 2014-10-02 until 2014-12-31 ( 2001- 3000)/33387 in 29.33s\n",
      "all from 2014-10-02 until 2014-12-31 ( 3001- 4000)/33387 in 29.99s\n",
      "all from 2014-10-02 until 2014-12-31 ( 4001- 5000)/33387 in 30.12s\n",
      "all from 2014-10-02 until 2014-12-31 ( 5001- 6000)/33387 in 29.93s\n",
      "all from 2014-10-02 until 2014-12-31 ( 6001- 7000)/33387 in 27.21s\n",
      "all from 2014-10-02 until 2014-12-31 ( 7001- 8000)/33387 in 26.63s\n",
      "all from 2014-10-02 until 2014-12-31 ( 8001- 9000)/33387 in 31.74s\n",
      "all from 2014-10-02 until 2014-12-31 ( 9001-10000)/33387 in 27.97s\n",
      "all from 2014-10-02 until 2014-12-31 (10001-11000)/33387 in 27.79s\n",
      "all from 2014-10-02 until 2014-12-31 (11001-12000)/33387 in 27.61s\n",
      "all from 2014-10-02 until 2014-12-31 (12001-13000)/33387 in 29.43s\n",
      "all from 2014-10-02 until 2014-12-31 (13001-14000)/33387 in 32.46s\n",
      "all from 2014-10-02 until 2014-12-31 (14001-15000)/33387 in 29.53s\n",
      "all from 2014-10-02 until 2014-12-31 (15001-16000)/33387 in 29.83s\n",
      "all from 2014-10-02 until 2014-12-31 (16001-17000)/33387 in 26.17s\n",
      "all from 2014-10-02 until 2014-12-31 (17001-18000)/33387 in 26.04s\n",
      "all from 2014-10-02 until 2014-12-31 (18001-19000)/33387 in 29.24s\n",
      "all from 2014-10-02 until 2014-12-31 (19001-20000)/33387 in 154.75s\n",
      "all from 2014-10-02 until 2014-12-31 (20001-21000)/33387 in 71.81s\n",
      "all from 2014-10-02 until 2014-12-31 (21001-22000)/33387 in 78.00s\n",
      "all from 2014-10-02 until 2014-12-31 (22001-23000)/33387 in 61.51s\n",
      "all from 2014-10-02 until 2014-12-31 (23001-24000)/33387 in 122.05s\n",
      "all from 2014-10-02 until 2014-12-31 (24001-25000)/33387 in 149.40s\n",
      "all from 2014-10-02 until 2014-12-31 (25001-26000)/33387 in 58.49s\n",
      "all from 2014-10-02 until 2014-12-31 (26001-27000)/33387 in 31.81s\n",
      "all from 2014-10-02 until 2014-12-31 (27001-28000)/33387 in 29.40s\n",
      "all from 2014-10-02 until 2014-12-31 (28001-29000)/33387 in 28.19s\n",
      "all from 2014-10-02 until 2014-12-31 (29001-30000)/33387 in 28.53s\n",
      "all from 2014-10-02 until 2014-12-31 (30001-31000)/33387 in 27.59s\n",
      "all from 2014-10-02 until 2014-12-31 (31001-32000)/33387 in 29.02s\n",
      "all from 2014-10-02 until 2014-12-31 (32001-33000)/33387 in 29.12s\n",
      "all from 2014-10-02 until 2014-12-31 (33001-33387)/33387 in 9.41s\n",
      "all from 2014-07-02 until 2014-12-31 retrieved 52944 records in 34 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2015-01-01_to_2015-07-01.csv\n",
      "all from 2015-01-01 until 2015-04-02 (    1- 1000)/29949 in 31.93s\n",
      "all from 2015-01-01 until 2015-04-02 ( 1001- 2000)/29949 in 31.42s\n",
      "all from 2015-01-01 until 2015-04-02 ( 2001- 3000)/29949 in 36.18s\n",
      "all from 2015-01-01 until 2015-04-02 ( 3001- 4000)/29949 in 27.79s\n",
      "all from 2015-01-01 until 2015-04-02 ( 4001- 5000)/29949 in 30.35s\n",
      "all from 2015-01-01 until 2015-04-02 ( 5001- 6000)/29949 in 30.00s\n",
      "all from 2015-01-01 until 2015-04-02 ( 6001- 7000)/29949 in 54.35s\n",
      "all from 2015-01-01 until 2015-04-02 ( 7001- 8000)/29949 in 27.61s\n",
      "all from 2015-01-01 until 2015-04-02 ( 8001- 9000)/29949 in 26.21s\n",
      "all from 2015-01-01 until 2015-04-02 ( 9001-10000)/29949 in 29.86s\n",
      "all from 2015-01-01 until 2015-04-02 (10001-11000)/29949 in 29.59s\n",
      "all from 2015-01-01 until 2015-04-02 (11001-12000)/29949 in 28.65s\n",
      "all from 2015-01-01 until 2015-04-02 (12001-13000)/29949 in 28.28s\n",
      "all from 2015-01-01 until 2015-04-02 (13001-14000)/29949 in 25.98s\n",
      "all from 2015-01-01 until 2015-04-02 (14001-15000)/29949 in 28.33s\n",
      "all from 2015-01-01 until 2015-04-02 (15001-16000)/29949 in 28.46s\n",
      "all from 2015-01-01 until 2015-04-02 (16001-17000)/29949 in 29.05s\n",
      "all from 2015-01-01 until 2015-04-02 (17001-18000)/29949 in 32.61s\n",
      "all from 2015-01-01 until 2015-04-02 (18001-19000)/29949 in 29.56s\n",
      "all from 2015-01-01 until 2015-04-02 (19001-20000)/29949 in 25.73s\n",
      "all from 2015-01-01 until 2015-04-02 (20001-21000)/29949 in 47.12s\n",
      "all from 2015-01-01 until 2015-04-02 (21001-22000)/29949 in 30.06s\n",
      "all from 2015-01-01 until 2015-04-02 (22001-23000)/29949 in 29.15s\n",
      "all from 2015-01-01 until 2015-04-02 (23001-24000)/29949 in 26.55s\n",
      "all from 2015-01-01 until 2015-04-02 (24001-25000)/29949 in 26.79s\n",
      "all from 2015-01-01 until 2015-04-02 (25001-26000)/29949 in 25.08s\n",
      "all from 2015-01-01 until 2015-04-02 (26001-27000)/29949 in 25.55s\n",
      "all from 2015-01-01 until 2015-04-02 (27001-28000)/29949 in 26.04s\n",
      "all from 2015-01-01 until 2015-04-02 (28001-29000)/29949 in 26.91s\n",
      "all from 2015-01-01 until 2015-04-02 (29001-29949)/29949 in 16.85s\n",
      "all from 2015-04-03 until 2015-07-01 (    1- 1000)/139925 in 32.19s\n",
      "all from 2015-04-03 until 2015-07-01 ( 1001- 2000)/139925 in 30.07s\n",
      "all from 2015-04-03 until 2015-07-01 ( 2001- 3000)/139925 in 36.83s\n",
      "all from 2015-04-03 until 2015-07-01 ( 3001- 4000)/139925 in 28.70s\n",
      "all from 2015-04-03 until 2015-07-01 ( 4001- 5000)/139925 in 27.28s\n",
      "all from 2015-04-03 until 2015-07-01 ( 5001- 6000)/139925 in 29.92s\n",
      "all from 2015-04-03 until 2015-07-01 ( 6001- 7000)/139925 in 28.19s\n",
      "all from 2015-04-03 until 2015-07-01 ( 7001- 8000)/139925 in 27.53s\n",
      "all from 2015-04-03 until 2015-07-01 ( 8001- 9000)/139925 in 28.82s\n",
      "all from 2015-04-03 until 2015-07-01 ( 9001-10000)/139925 in 29.20s\n",
      "all from 2015-04-03 until 2015-07-01 (10001-11000)/139925 in 28.41s\n",
      "all from 2015-04-03 until 2015-07-01 (11001-12000)/139925 in 28.45s\n",
      "all from 2015-04-03 until 2015-07-01 (12001-13000)/139925 in 29.36s\n",
      "all from 2015-04-03 until 2015-07-01 (13001-14000)/139925 in 31.65s\n",
      "all from 2015-04-03 until 2015-07-01 (14001-15000)/139925 in 29.66s\n",
      "all from 2015-04-03 until 2015-07-01 (15001-16000)/139925 in 27.59s\n",
      "all from 2015-04-03 until 2015-07-01 (16001-17000)/139925 in 28.38s\n",
      "all from 2015-04-03 until 2015-07-01 (17001-18000)/139925 in 27.95s\n",
      "all from 2015-04-03 until 2015-07-01 (18001-19000)/139925 in 30.09s\n",
      "all from 2015-04-03 until 2015-07-01 (19001-20000)/139925 in 33.51s\n",
      "all from 2015-04-03 until 2015-07-01 (20001-21000)/139925 in 31.36s\n",
      "all from 2015-04-03 until 2015-07-01 (21001-22000)/139925 in 29.18s\n",
      "all from 2015-04-03 until 2015-07-01 (22001-23000)/139925 in 29.34s\n",
      "all from 2015-04-03 until 2015-07-01 (23001-24000)/139925 in 28.67s\n",
      "all from 2015-04-03 until 2015-07-01 (24001-25000)/139925 in 31.78s\n",
      "all from 2015-04-03 until 2015-07-01 (25001-26000)/139925 in 30.92s\n",
      "all from 2015-04-03 until 2015-07-01 (26001-27000)/139925 in 30.16s\n",
      "all from 2015-04-03 until 2015-07-01 (27001-28000)/139925 in 32.81s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2015-04-03 until 2015-07-01 (28001-29000)/139925 in 30.37s\n",
      "all from 2015-04-03 until 2015-07-01 (29001-30000)/139925 in 28.34s\n",
      "all from 2015-04-03 until 2015-07-01 (30001-31000)/139925 in 31.13s\n",
      "all from 2015-04-03 until 2015-07-01 (31001-32000)/139925 in 28.32s\n",
      "all from 2015-04-03 until 2015-07-01 (32001-33000)/139925 in 30.69s\n",
      "all from 2015-04-03 until 2015-07-01 (33001-34000)/139925 in 28.48s\n",
      "all from 2015-04-03 until 2015-07-01 (34001-35000)/139925 in 27.84s\n",
      "all from 2015-04-03 until 2015-07-01 (35001-36000)/139925 in 30.17s\n",
      "all from 2015-04-03 until 2015-07-01 (36001-37000)/139925 in 28.18s\n",
      "all from 2015-04-03 until 2015-07-01 (37001-38000)/139925 in 28.00s\n",
      "all from 2015-04-03 until 2015-07-01 (38001-39000)/139925 in 29.41s\n",
      "all from 2015-04-03 until 2015-07-01 (39001-40000)/139925 in 29.76s\n",
      ":( Failed requesting http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3148006|40001\n",
      "Moving on\n",
      "all from 2015-01-01 until 2015-07-01 retrieved 69949 records in 35 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2015-07-02_to_2015-12-31.csv\n",
      ":( Failed requesting http://export.arxiv.org/oai2?verb=ListRecords&from=2015-07-02&until=2015-10-01&metadataPrefix=arXiv\n",
      "Moving on\n",
      "all from 2015-10-02 until 2015-12-31 (    1- 1000)/25832 in 27.76s\n",
      "all from 2015-10-02 until 2015-12-31 ( 1001- 2000)/25832 in 26.77s\n",
      "all from 2015-10-02 until 2015-12-31 ( 2001- 3000)/25832 in 29.61s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42c54d834423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2011'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2012'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2013'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2014'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2015'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2016'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2017'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mharvest_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{year}-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{year}-07-01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mharvest_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{year}-07-02\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{year}-12-31\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-653ffaa93c2f>\u001b[0m in \u001b[0;36mharvest_data\u001b[0;34m(isoday_0, isoday_1, category, days_in_slice, file_name, overwrite)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# try to download the slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mnewly_retrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mharvest_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_until\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieved\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewly_retrieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-dd01ed2573c3>\u001b[0m in \u001b[0;36mharvest_slice\u001b[0;34m(date_from, date_until, category, file)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 record_string = [(record.id.string if record.id else 'nan'),\n\u001b[1;32m     36\u001b[0m                                  \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforenames\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                  \u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                  \u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                  \u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# We special case contents to avoid recursion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"contents\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         raise AttributeError(\n\u001b[1;32m   1118\u001b[0m             \"'%s' object has no attribute '%s'\" % (self.__class__, tag))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, name, attrs, recursive, text, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m         criteria.\"\"\"\n\u001b[1;32m   1354\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind_all\u001b[0;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m     \u001b[0mfindAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all\u001b[0m       \u001b[0;31m# BS3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all\u001b[0m  \u001b[0;31m# BS2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m_find_all\u001b[0;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# If it's text, make sure the text matches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36msearch_tag\u001b[0;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1729\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0mmarkup_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# multiple years (also split in two files each)\n",
    "\n",
    "for year in ['2011','2012','2013','2014','2015','2016','2017']:\n",
    "    harvest_data(f\"{year}-01-01\", f\"{year}-07-01\")\n",
    "    harvest_data(f\"{year}-07-02\", f\"{year}-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
