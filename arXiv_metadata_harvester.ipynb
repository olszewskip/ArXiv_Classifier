{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from my_utilities import read_dict, save_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Metadata Harvester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "## Grab records from the requested timespan, from all or from one selected category\n",
    "\n",
    "## Write to tab-delimited local csv:\n",
    "## columns: *id, authors, title, abstract, categories*\n",
    "in utf-8 encoding\n",
    "### There are two functions.\n",
    "Both will talk to You using prints.\n",
    "* ***harvest_slice*** needs You to explicitly choose the category (possibly 'all') and the filename as arguments\n",
    "    * just appends lines to the file, it's up to You not to make a mess\n",
    "\n",
    "\n",
    "* ***harvest_data*** divides the timespan into slices of given length and harvests those using *harvest_slice*:\n",
    "    * can make up the name of the file on its own\n",
    "    * adds the header to the csv\n",
    "    * default behavior when the file already exists is to quit\n",
    "    * default category is 'all'\n",
    "\n",
    "### It is slow.    \n",
    "### Examples:\n",
    "*  ~ 1 min,  2 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")\n",
    "*  ~ 5 min, 11 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")\n",
    "* ~ 10 min, 16 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)\n",
    "* ~ 1 h, 68 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\")\n",
    "\n",
    "### Example of a basic query used in the code:\n",
    "* http://export.arxiv.org/oai2?verb=ListRecords&from=2012-01-01&until=2018-02-01&set=physics:hep-th&metadataPrefix=arXiv\n",
    "* \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "\n",
    "See https://arxiv.org/help/bulk_data for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside from having authors, a title and an abstract (a summary), articles on *ArXiv* are typically assigned to a category, e.g. Computer Science, Economics, etc. Those informations form the meta-data of an article that is easily obtainable with an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One can talk with *ArXiv* using two different interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first one serves to answer typical complicated search queries.\n",
    "For example looking for articles by Stephen Hawking about black holes we could start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"ti:black%20hole+AND+au:Hawking\"\n",
    "\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entries = soup.find_all('entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a typical data we get is the following. Notice that there is both the *primary category* and a general *category* list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/hep-th/0507171v2\n",
      "S. W. Hawking\n",
      "Information Loss in Black Holes\n",
      "primary category: hep-th\n",
      "all categories: ['hep-th']\n",
      "abstract:   The question of whether information is lost in black holes is investigated\n",
      "using Euclidean path integrals. The formation and evaporation of black holes is\n",
      "regarded as a scattering problem with all m ...\n"
     ]
    }
   ],
   "source": [
    "entry = entries[0]\n",
    "\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:200]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another example we see that there can be more categories: e.g. one from Economics (*econ.EM*) and an another one from Statistics (*stat.AP*), and that the first one in the list is the primary category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/1803.11233v1\n",
      "Kamil Jodź\n",
      "Mortality in a heterogeneous population - Lee-Carter's methodology\n",
      "primary category: econ.EM\n",
      "all categories: ['econ.EM', 'stat.AP']\n",
      "abstract:   The EU Solvency II directive recommends insurance companie ...\n"
     ]
    }
   ],
   "source": [
    "search_query = \"1803.11233\"\n",
    "# search_query = \"0707.3787\"\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entry = soup.find('entry')\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:60]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But this first API is not suited for bulk data downloads. Instead, we want to use the interface specified by Open Archives Initiative (OAI) that ArXiv complies with.\n",
    "This time we build the query by specifying the time slice from which we want the articles. We can also filter for one category, if we want. Take the following query for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from = \"2018-04-02\"\n",
    "date_until = \"2018-04-02\"\n",
    "category = \"econ\" # Economics\n",
    "\n",
    "search_query = f\"&from={date_from}&until={date_until}&set={category}\"\n",
    "query = \"http://export.arxiv.org/oai2?verb=ListRecords\" + search_query + \"&metadataPrefix=arXiv\"\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "records = soup.find_all('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<record>\n",
       "<header>\n",
       "<identifier>oai:arXiv.org:1803.11233</identifier>\n",
       "<datestamp>2018-04-02</datestamp>\n",
       "<setspec>econ</setspec>\n",
       "</header>\n",
       "<metadata>\n",
       "<arxiv xmlns=\"http://arxiv.org/OAI/arXiv/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemalocation=\"http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd\">\n",
       "<id>1803.11233</id><created>2018-03-29</created><authors><author><keyname>Jodź</keyname><forenames>Kamil</forenames></author></authors><title>Mortality in a heterogeneous population - Lee-Carter's methodology</title><categories>econ.EM stat.AP</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EU Solvency II directive recommends insurance companies to pay more\n",
       "attention to the risk management methods. The sense of risk management is the\n",
       "ability to quantify risk and apply methods that reduce uncertainty. In life\n",
       "insurance, the risk is a consequence of the random variable describing the life\n",
       "expectancy. The article will present a proposal for stochastic mortality\n",
       "modeling based on the Lee and Carter methodology. The maximum likelihood method\n",
       "is often used to estimate parameters in mortality models. This method assumes\n",
       "that the population is homogeneous and the number of deaths has the Poisson\n",
       "distribution. The aim of this article is to change assumptions about the\n",
       "distribution of the number of deaths. The results indicate that the model can\n",
       "get a better match to historical data, when the number of deaths has a negative\n",
       "binomial distribution.\n",
       "</abstract></arxiv>\n",
       "</metadata>\n",
       "</record>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time there is only the single ***categories*** tag. **We will be assuming that there is a convention that the first item on that list is the primary category of an article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803.11233\n",
      "econ.EM stat.AP\n"
     ]
    }
   ],
   "source": [
    "print(records[0].id.string)\n",
    "print(records[0].categories.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the *set* field in the last query. We can retrieve the list of all possible *sets* using another fixed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'econ': 'Economics',\n",
       " 'eess': 'Electrical Engineering and Systems Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'physics:astro-ph': 'Astrophysics',\n",
       " 'physics:cond-mat': 'Condensed Matter',\n",
       " 'physics:gr-qc': 'General Relativity and Quantum Cosmology',\n",
       " 'physics:hep-ex': 'High Energy Physics - Experiment',\n",
       " 'physics:hep-lat': 'High Energy Physics - Lattice',\n",
       " 'physics:hep-ph': 'High Energy Physics - Phenomenology',\n",
       " 'physics:hep-th': 'High Energy Physics - Theory',\n",
       " 'physics:math-ph': 'Mathematical Physics',\n",
       " 'physics:nlin': 'Nonlinear Sciences',\n",
       " 'physics:nucl-ex': 'Nuclear Experiment',\n",
       " 'physics:nucl-th': 'Nuclear Theory',\n",
       " 'physics:physics': 'Physics (Other)',\n",
       " 'physics:quant-ph': 'Quantum Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query retrieves xml about the accesible 'sets', e.g.\n",
    "# <set>\n",
    "# <setspec>cs</setspec>\n",
    "# <setname>Computer Science</setname>\n",
    "# </set>\n",
    "\n",
    "if not Path(\"categories.txt\").is_file() :\n",
    "    \n",
    "    xml_query = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "    sauce = urllib.request.urlopen(xml_query).read()\n",
    "    soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "    sets = soup.find_all(\"set\")\n",
    "\n",
    "    categories = {}\n",
    "\n",
    "    for set_ in sets:\n",
    "        categories[set_.setspec.string] = set_.setname.string\n",
    "\n",
    "    save_dict(categories, \"categories.txt\")\n",
    "            \n",
    "\n",
    "categories = {}\n",
    "categories = read_dict(\"categories.txt\")\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently physics enthusiasts get more options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The matter of actual article categories is more messy, see https://arxiv.org/ and https://arxiv.org/help/prep#subj\n",
    "Physics gets an additional level of gradation: e.g. *physics:astro-ph* is a subset of *physics*. And the categorization chosen by an author her- or himself is finer and may be multiple, e.g. *cs.ai* (Computer Science: Artificial Intelligence) instead of just *cs*, together with *physics:astro-ph.GA* (Physics: Astrophysics: Astrophysics of Galaxies) instead of just *physics:astro-ph* (assuming that the article was both about Artificial Intelligence and Galaxies). But, again, first of the categories is the primary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dictionaries. One with the top-level categories, and the second with physics genres.\n",
    "\n",
    "pattern = re.compile('physics:(.+)')\n",
    "\n",
    "physics_genres = {}\n",
    "top_categories = {}\n",
    "\n",
    "for category, description in categories.items():\n",
    "    match = pattern.match(category)\n",
    "    if match:\n",
    "        physics_genres[match.group(1)] = description\n",
    "    else:\n",
    "        top_categories[category] = description\n",
    "\n",
    "# save physics_genres and top_categories for later use\n",
    "save_dict(physics_genres, \"physics_genres.txt\")\n",
    "save_dict(top_categories, \"top_cats.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cs': 'Computer Science',\n",
       "  'econ': 'Economics',\n",
       "  'eess': 'Electrical Engineering and Systems Science',\n",
       "  'math': 'Mathematics',\n",
       "  'physics': 'Physics',\n",
       "  'q-bio': 'Quantitative Biology',\n",
       "  'q-fin': 'Quantitative Finance',\n",
       "  'stat': 'Statistics'},\n",
       " {'astro-ph': 'Astrophysics',\n",
       "  'cond-mat': 'Condensed Matter',\n",
       "  'gr-qc': 'General Relativity and Quantum Cosmology',\n",
       "  'hep-ex': 'High Energy Physics - Experiment',\n",
       "  'hep-lat': 'High Energy Physics - Lattice',\n",
       "  'hep-ph': 'High Energy Physics - Phenomenology',\n",
       "  'hep-th': 'High Energy Physics - Theory',\n",
       "  'math-ph': 'Mathematical Physics',\n",
       "  'nlin': 'Nonlinear Sciences',\n",
       "  'nucl-ex': 'Nuclear Experiment',\n",
       "  'nucl-th': 'Nuclear Theory',\n",
       "  'physics': 'Physics (Other)',\n",
       "  'quant-ph': 'Quantum Physics'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_categories, physics_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The API will serve us 1000 records each 10 seconds (plus a considerable overhead for communication)\n",
    "The imported *harvest_slice* function, given a time-slice, category and a file-path, works in a loop and\n",
    "    * sends the query\n",
    "    * saves the received records into a file\n",
    "    * using the last *resumption token* (appended to the xml) and the given dates forms a next query\n",
    "    * finally returns the number of retrieved records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_slice(date_from, date_until, category, file) -> int:\n",
    "    # returns number of downloaded records if succesful\n",
    "    \n",
    "    base_query = \"http://export.arxiv.org/oai2?verb=ListRecords\"\n",
    "    \n",
    "    if category == \"all\":\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&metadataPrefix=arXiv\"\n",
    "    else:\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&set={category}&metadataPrefix=arXiv\"\n",
    "    \n",
    "    retrieved = 0\n",
    "    \n",
    "    while query:\n",
    "        \n",
    "        time_0 = time.time()\n",
    "        \n",
    "        # try to download\n",
    "        try:            \n",
    "            sauce = urllib.request.urlopen(query).read()\n",
    "\n",
    "        except:\n",
    "            print(f\":( Failed requesting {query}\\nMoving on\")\n",
    "            break\n",
    "        \n",
    "        # parse the xml looking for <record>'s\n",
    "        soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "        records = soup.find_all('record')\n",
    "\n",
    "        retrieved = retrieved + len(records)\n",
    "\n",
    "        with open(file, \"a\", encoding='utf-8') as dump:\n",
    "\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            for record in records:                \n",
    "                record_string = [(record.id.string if record.id else 'nan'),\n",
    "                                 [(author.forenames.string+\" \" if author.forenames else \"\") + (author.keyname.string if author.keyname else 'nan') for author in record.find_all('author')],\n",
    "                                 (record.title.string if record.title else 'nan'),\n",
    "                                 (record.abstract.string if record.abstract else 'nan'),\n",
    "                                 (record.categories.string if record.categories else 'nan')\n",
    "                                ]\n",
    "                writer.writerow(record_string)\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"\".join([category,\" from \", f\"{date_from}\",\" until \", f\"{date_until}\",\" empty\"]))\n",
    "            break\n",
    "        \n",
    "        # info at the end of 'soup' about where to resume if the data stream was cut at 1000 records\n",
    "        # None if the stream wasn't cut\n",
    "        res_token = soup.find(\"resumptiontoken\")\n",
    "        \n",
    "        if res_token:\n",
    "\n",
    "            # data in the current loop started at this record in the 'query'\n",
    "            started_at = int(res_token['cursor']) + 1\n",
    "            \n",
    "            # total number of records in the 'query', should be the same in each loop\n",
    "            all_to_retrieve = int(res_token['completelistsize'])\n",
    "            \n",
    "            if res_token.string:\n",
    "                # the identifier that allows to resume the query\n",
    "                # None if the slice was completed\n",
    "\n",
    "                query = base_query + f\"&resumptionToken={res_token.string}\"\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                query = None\n",
    "            \n",
    "        else:\n",
    "            started_at = 1 \n",
    "            all_to_retrieve = len(records)\n",
    "            query = None\n",
    "        \n",
    "        time_1 = time.time()\n",
    "        \n",
    "        print(\"\".join([category,\n",
    "                       \" from \", f\"{date_from}\", \" until \", f\"{date_until}\",\n",
    "                       f\" ({started_at:>5}-{started_at+len(records)-1:>5})/{all_to_retrieve:>5}\",\n",
    "                      \" in \", f\"{(time_1 - time_0):3.2f}\", \"s\"]) )\n",
    "    \n",
    "    # end of while loop\n",
    "     \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are receiving the data in batches with 1000 records each. Each batch from the time period defined by the arguments ends up in the same file. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math from 2018-10-01 until 2018-10-10 (    1- 1000)/ 2328 in 23.12s\n",
      "math from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 2328 in 24.31s\n",
      "math from 2018-10-01 until 2018-10-10 ( 2001- 2328)/ 2328 in 5.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2328"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to download different categories separately like that, the records that belong to more than one category would be repeated in each file. But we can be downloading all categories at the same time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2018-10-01 until 2018-10-10 (    1- 1000)/ 7482 in 26.81s\n",
      "all from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 7482 in 27.57s\n",
      "all from 2018-10-01 until 2018-10-10 ( 2001- 3000)/ 7482 in 26.85s\n",
      "all from 2018-10-01 until 2018-10-10 ( 3001- 4000)/ 7482 in 27.77s\n",
      "all from 2018-10-01 until 2018-10-10 ( 4001- 5000)/ 7482 in 28.97s\n",
      "all from 2018-10-01 until 2018-10-10 ( 5001- 6000)/ 7482 in 25.77s\n",
      "all from 2018-10-01 until 2018-10-10 ( 6001- 7000)/ 7482 in 32.03s\n",
      "all from 2018-10-01 until 2018-10-10 ( 7001- 7482)/ 7482 in 8.67s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just as a precaution, let's split longer time-slices into multiple shorter ones in case there is an upper limit for the total number of records we can retrieve with one query.\n",
    "We divide the time-slice into 92-days long (by default) periods, and write to an automatically named file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around harvest_slice\n",
    "# * handles file-names\n",
    "# * slices the time period of papers into intervals of given number of days (days_in_slice)\n",
    "\n",
    "def harvest_data(isoday_0, isoday_1, category='all', days_in_slice = 92, file_name=None, overwrite=False) -> int:\n",
    "\n",
    "    date_0 = dateutil.parser.parse(isoday_0).date()\n",
    "    date_1 = dateutil.parser.parse(isoday_1).date()\n",
    "\n",
    "    if not file_name:\n",
    "        # create a file with an overly descriptive name\n",
    "        file = f\"arXivMeta_{category.replace(':','--')}_from_{date_0}_to_{date_1}.csv\"\n",
    "    else:\n",
    "        file = file_name\n",
    "    \n",
    "    # check if file already exists\n",
    "    if Path(file).is_file():\n",
    "        if overwrite :\n",
    "\n",
    "            # try to backup the old file\n",
    "            file_info = re.match(r\"(\\w.+)\\.(\\w\\w+)\", file)\n",
    "            if file_info:\n",
    "                new_file = \"\".join([ file_info.group(1), \"_bak.\", file_info.group(2) ])\n",
    "                if not Path(new_file).is_file():\n",
    "                    os.rename(file, new_file)\n",
    "                    print(f\"Old file backed up as {new_file}\")\n",
    "\n",
    "            # clear the file\n",
    "            print(f\"Overwriting {file}\")\n",
    "            with open(file, \"w\") as dump:\n",
    "                dump.truncate(0)\n",
    "            \n",
    "        else:\n",
    "            print(f\"The file {file} already exists\\n\")\n",
    "            return -1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Writing to {file}\")\n",
    "    \n",
    "    with open(file, \"a\") as dump:\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            header = ['id', 'authors', 'title', 'abstract', 'categories']\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    # Start the clock\n",
    "    time_0 = time.time()\n",
    "    \n",
    "    # Let's count all downloaded records\n",
    "    retrieved = 0\n",
    "    \n",
    "    # We'll go from 'date_0' until 'date_1' in slices of 'days_in_slice' days\n",
    "    # The server's response presumably maxes out at some number of records,\n",
    "    # so we hope to have slices with less records than that.\n",
    "\n",
    "    date_from = date_0\n",
    "\n",
    "    while date_from <= date_1:\n",
    "        \n",
    "        date_until = min(date_1, date_from + datetime.timedelta(days_in_slice-1))\n",
    "\n",
    "        # try to download the slice\n",
    "        newly_retrieved = harvest_slice(date_from, date_until, category, file)\n",
    "        retrieved = retrieved + newly_retrieved\n",
    "\n",
    "        # move on to the next slice\n",
    "        date_from = date_until + datetime.timedelta(days=1)\n",
    "        \n",
    "        # time-out\n",
    "        time.sleep(10)\n",
    "\n",
    "    time_1 = time.time()\n",
    "    \n",
    "    print(\"\".join([category,\n",
    "                   \" from \", str(date_0), \" until \", str(date_1),\n",
    "                   \" retrieved \", str(retrieved), \" records\"\n",
    "                   ,\" in \", f\"{(time_1 - time_0)/60:.0f}\", \" min\\n\"])\n",
    "         )\n",
    "    \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can do for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old file backed up as test_bak.csv\n",
      "Overwriting test.csv\n",
      "math from 2018-08-01 until 2018-10-31 (    1- 1000)/18206 in 43.39s\n",
      "math from 2018-08-01 until 2018-10-31 ( 1001- 2000)/18206 in 62.31s\n",
      "math from 2018-08-01 until 2018-10-31 ( 2001- 3000)/18206 in 103.59s\n",
      "math from 2018-08-01 until 2018-10-31 ( 3001- 4000)/18206 in 57.29s\n",
      "math from 2018-08-01 until 2018-10-31 ( 4001- 5000)/18206 in 90.82s\n",
      "math from 2018-08-01 until 2018-10-31 ( 5001- 6000)/18206 in 54.15s\n",
      "math from 2018-08-01 until 2018-10-31 ( 6001- 7000)/18206 in 46.50s\n",
      "math from 2018-08-01 until 2018-10-31 ( 7001- 8000)/18206 in 31.82s\n",
      "math from 2018-08-01 until 2018-10-31 ( 8001- 9000)/18206 in 45.30s\n",
      "math from 2018-08-01 until 2018-10-31 ( 9001-10000)/18206 in 25.56s\n",
      "math from 2018-08-01 until 2018-10-31 (10001-11000)/18206 in 26.49s\n",
      "math from 2018-08-01 until 2018-10-31 (11001-12000)/18206 in 27.74s\n",
      "math from 2018-08-01 until 2018-10-31 (12001-13000)/18206 in 25.49s\n",
      "math from 2018-08-01 until 2018-10-31 (13001-14000)/18206 in 24.23s\n",
      "math from 2018-08-01 until 2018-10-31 (14001-15000)/18206 in 27.26s\n",
      "math from 2018-08-01 until 2018-10-31 (15001-16000)/18206 in 25.01s\n",
      "math from 2018-08-01 until 2018-10-31 (16001-17000)/18206 in 25.85s\n",
      "math from 2018-08-01 until 2018-10-31 (17001-18000)/18206 in 26.64s\n",
      "math from 2018-08-01 until 2018-10-31 (18001-18206)/18206 in 6.48s\n",
      "math from 2018-11-01 until 2018-11-01 (    1-  262)/  262 in 6.96s\n",
      "math from 2018-08-01 until 2018-11-01 retrieved 18468 records in 13 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18468"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name=\"test.csv\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there were two time-slices, one with 18206 records, second with 262 records. All saved in \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_all_from_2010-01-01_to_2010-07-01.csv\n",
      "all from 2010-01-01 until 2010-04-02 (    1- 1000)/13727 in 34.98s\n",
      "all from 2010-01-01 until 2010-04-02 ( 1001- 2000)/13727 in 34.47s\n",
      "all from 2010-01-01 until 2010-04-02 ( 2001- 3000)/13727 in 39.02s\n",
      "all from 2010-01-01 until 2010-04-02 ( 3001- 4000)/13727 in 37.09s\n",
      "all from 2010-01-01 until 2010-04-02 ( 4001- 5000)/13727 in 32.30s\n",
      "all from 2010-01-01 until 2010-04-02 ( 5001- 6000)/13727 in 32.11s\n",
      "all from 2010-01-01 until 2010-04-02 ( 6001- 7000)/13727 in 96.90s\n",
      "all from 2010-01-01 until 2010-04-02 ( 7001- 8000)/13727 in 65.82s\n",
      "all from 2010-01-01 until 2010-04-02 ( 8001- 9000)/13727 in 76.95s\n",
      "all from 2010-01-01 until 2010-04-02 ( 9001-10000)/13727 in 90.02s\n",
      "all from 2010-01-01 until 2010-04-02 (10001-11000)/13727 in 83.90s\n",
      "all from 2010-01-01 until 2010-04-02 (11001-12000)/13727 in 90.99s\n",
      "all from 2010-01-01 until 2010-04-02 (12001-13000)/13727 in 71.82s\n",
      "all from 2010-01-01 until 2010-04-02 (13001-13727)/13727 in 36.72s\n",
      "all from 2010-04-03 until 2010-07-01 (    1- 1000)/15366 in 51.50s\n",
      "all from 2010-04-03 until 2010-07-01 ( 1001- 2000)/15366 in 80.02s\n",
      "all from 2010-04-03 until 2010-07-01 ( 2001- 3000)/15366 in 35.78s\n",
      "all from 2010-04-03 until 2010-07-01 ( 3001- 4000)/15366 in 31.35s\n",
      "all from 2010-04-03 until 2010-07-01 ( 4001- 5000)/15366 in 33.55s\n",
      "all from 2010-04-03 until 2010-07-01 ( 5001- 6000)/15366 in 31.94s\n",
      "all from 2010-04-03 until 2010-07-01 ( 6001- 7000)/15366 in 33.26s\n",
      "all from 2010-04-03 until 2010-07-01 ( 7001- 8000)/15366 in 30.22s\n",
      "all from 2010-04-03 until 2010-07-01 ( 8001- 9000)/15366 in 29.24s\n",
      "all from 2010-04-03 until 2010-07-01 ( 9001-10000)/15366 in 28.46s\n",
      "all from 2010-04-03 until 2010-07-01 (10001-11000)/15366 in 29.48s\n",
      "all from 2010-04-03 until 2010-07-01 (11001-12000)/15366 in 31.97s\n",
      "all from 2010-04-03 until 2010-07-01 (12001-13000)/15366 in 28.27s\n",
      "all from 2010-04-03 until 2010-07-01 (13001-14000)/15366 in 30.02s\n",
      "all from 2010-04-03 until 2010-07-01 (14001-15000)/15366 in 28.78s\n",
      "all from 2010-04-03 until 2010-07-01 (15001-15366)/15366 in 9.74s\n",
      "all from 2010-01-01 until 2010-07-01 retrieved 29093 records in 23 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2010-07-02_to_2010-12-31.csv\n",
      "all from 2010-07-02 until 2010-10-01 (    1- 1000)/ 9903 in 30.93s\n",
      "all from 2010-07-02 until 2010-10-01 ( 1001- 2000)/ 9903 in 31.10s\n",
      "all from 2010-07-02 until 2010-10-01 ( 2001- 3000)/ 9903 in 33.49s\n",
      "all from 2010-07-02 until 2010-10-01 ( 3001- 4000)/ 9903 in 30.86s\n",
      "all from 2010-07-02 until 2010-10-01 ( 4001- 5000)/ 9903 in 28.82s\n",
      "all from 2010-07-02 until 2010-10-01 ( 5001- 6000)/ 9903 in 30.53s\n",
      "all from 2010-07-02 until 2010-10-01 ( 6001- 7000)/ 9903 in 30.32s\n",
      "all from 2010-07-02 until 2010-10-01 ( 7001- 8000)/ 9903 in 31.95s\n",
      "all from 2010-07-02 until 2010-10-01 ( 8001- 9000)/ 9903 in 32.76s\n",
      "all from 2010-07-02 until 2010-10-01 ( 9001- 9903)/ 9903 in 16.92s\n",
      "all from 2010-10-02 until 2010-12-31 (    1- 1000)/18258 in 36.20s\n",
      "all from 2010-10-02 until 2010-12-31 ( 1001- 2000)/18258 in 30.86s\n",
      "all from 2010-10-02 until 2010-12-31 ( 2001- 3000)/18258 in 34.01s\n",
      "all from 2010-10-02 until 2010-12-31 ( 3001- 4000)/18258 in 34.71s\n",
      "all from 2010-10-02 until 2010-12-31 ( 4001- 5000)/18258 in 36.64s\n",
      "all from 2010-10-02 until 2010-12-31 ( 5001- 6000)/18258 in 32.36s\n",
      "all from 2010-10-02 until 2010-12-31 ( 6001- 7000)/18258 in 31.23s\n",
      "all from 2010-10-02 until 2010-12-31 ( 7001- 8000)/18258 in 28.89s\n",
      "all from 2010-10-02 until 2010-12-31 ( 8001- 9000)/18258 in 30.15s\n",
      "all from 2010-10-02 until 2010-12-31 ( 9001-10000)/18258 in 29.61s\n",
      "all from 2010-10-02 until 2010-12-31 (10001-11000)/18258 in 28.80s\n",
      "all from 2010-10-02 until 2010-12-31 (11001-12000)/18258 in 32.09s\n",
      "all from 2010-10-02 until 2010-12-31 (12001-13000)/18258 in 27.22s\n",
      "all from 2010-10-02 until 2010-12-31 (13001-14000)/18258 in 30.75s\n",
      "all from 2010-10-02 until 2010-12-31 (14001-15000)/18258 in 27.10s\n",
      "all from 2010-10-02 until 2010-12-31 (15001-16000)/18258 in 27.80s\n",
      "all from 2010-10-02 until 2010-12-31 (16001-17000)/18258 in 26.36s\n",
      "all from 2010-10-02 until 2010-12-31 (17001-18000)/18258 in 27.54s\n",
      "all from 2010-10-02 until 2010-12-31 (18001-18258)/18258 in 7.29s\n",
      "all from 2010-07-02 until 2010-12-31 retrieved 28161 records in 15 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28161"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single year split in two files\n",
    "year = 2010\n",
    "\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-07-01\")\n",
    "harvest_data(f\"{year}-07-02\", f\"{year}-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file arXivMeta_all_from_2011-01-01_to_2011-07-01.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2011-07-02_to_2011-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2012-01-01_to_2012-07-01.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2012-07-02_to_2012-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2013-01-01_to_2013-07-01.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2013-07-02_to_2013-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2014-01-01_to_2014-07-01.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2014-07-02_to_2014-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2015-01-01_to_2015-07-01.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2015-07-02_to_2015-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2016-01-01_to_2016-07-01.csv already exists\n",
      "\n",
      "Writing to arXivMeta_all_from_2016-07-02_to_2016-12-31.csv\n",
      "all from 2016-07-02 until 2016-10-01 (    1- 1000)/44640 in 30.30s\n",
      "all from 2016-07-02 until 2016-10-01 ( 1001- 2000)/44640 in 46.58s\n",
      "all from 2016-07-02 until 2016-10-01 ( 2001- 3000)/44640 in 35.09s\n",
      "all from 2016-07-02 until 2016-10-01 ( 3001- 4000)/44640 in 38.02s\n",
      "all from 2016-07-02 until 2016-10-01 ( 4001- 5000)/44640 in 36.88s\n",
      "all from 2016-07-02 until 2016-10-01 ( 5001- 6000)/44640 in 33.98s\n",
      "all from 2016-07-02 until 2016-10-01 ( 6001- 7000)/44640 in 30.09s\n",
      "all from 2016-07-02 until 2016-10-01 ( 7001- 8000)/44640 in 29.27s\n",
      "all from 2016-07-02 until 2016-10-01 ( 8001- 9000)/44640 in 28.06s\n",
      "all from 2016-07-02 until 2016-10-01 ( 9001-10000)/44640 in 27.18s\n",
      "all from 2016-07-02 until 2016-10-01 (10001-11000)/44640 in 28.82s\n",
      "all from 2016-07-02 until 2016-10-01 (11001-12000)/44640 in 28.85s\n",
      "all from 2016-07-02 until 2016-10-01 (12001-13000)/44640 in 27.49s\n",
      "all from 2016-07-02 until 2016-10-01 (13001-14000)/44640 in 27.97s\n",
      "all from 2016-07-02 until 2016-10-01 (14001-15000)/44640 in 26.84s\n",
      "all from 2016-07-02 until 2016-10-01 (15001-16000)/44640 in 26.99s\n",
      "all from 2016-07-02 until 2016-10-01 (16001-17000)/44640 in 28.01s\n",
      "all from 2016-07-02 until 2016-10-01 (17001-18000)/44640 in 25.50s\n",
      "all from 2016-07-02 until 2016-10-01 (18001-19000)/44640 in 27.50s\n",
      "all from 2016-07-02 until 2016-10-01 (19001-20000)/44640 in 26.39s\n",
      "all from 2016-07-02 until 2016-10-01 (20001-21000)/44640 in 27.95s\n",
      "all from 2016-07-02 until 2016-10-01 (21001-22000)/44640 in 25.51s\n",
      "all from 2016-07-02 until 2016-10-01 (22001-23000)/44640 in 25.15s\n",
      "all from 2016-07-02 until 2016-10-01 (23001-24000)/44640 in 28.32s\n",
      "all from 2016-07-02 until 2016-10-01 (24001-25000)/44640 in 25.82s\n",
      "all from 2016-07-02 until 2016-10-01 (25001-26000)/44640 in 25.13s\n",
      "all from 2016-07-02 until 2016-10-01 (26001-27000)/44640 in 24.64s\n",
      "all from 2016-07-02 until 2016-10-01 (27001-28000)/44640 in 26.78s\n",
      "all from 2016-07-02 until 2016-10-01 (28001-29000)/44640 in 26.62s\n",
      "all from 2016-07-02 until 2016-10-01 (29001-30000)/44640 in 26.92s\n",
      "all from 2016-07-02 until 2016-10-01 (30001-31000)/44640 in 27.02s\n",
      "all from 2016-07-02 until 2016-10-01 (31001-32000)/44640 in 28.47s\n",
      "all from 2016-07-02 until 2016-10-01 (32001-33000)/44640 in 24.13s\n",
      "all from 2016-07-02 until 2016-10-01 (33001-34000)/44640 in 26.13s\n",
      "all from 2016-07-02 until 2016-10-01 (34001-35000)/44640 in 24.12s\n",
      "all from 2016-07-02 until 2016-10-01 (35001-36000)/44640 in 24.68s\n",
      "all from 2016-07-02 until 2016-10-01 (36001-37000)/44640 in 23.88s\n",
      "all from 2016-07-02 until 2016-10-01 (37001-38000)/44640 in 23.46s\n",
      "all from 2016-07-02 until 2016-10-01 (38001-39000)/44640 in 23.72s\n",
      "all from 2016-07-02 until 2016-10-01 (39001-40000)/44640 in 24.18s\n",
      "all from 2016-07-02 until 2016-10-01 (40001-41000)/44640 in 26.88s\n",
      "all from 2016-07-02 until 2016-10-01 (41001-42000)/44640 in 24.00s\n",
      "all from 2016-07-02 until 2016-10-01 (42001-43000)/44640 in 28.59s\n",
      "all from 2016-07-02 until 2016-10-01 (43001-44000)/44640 in 25.40s\n",
      "all from 2016-07-02 until 2016-10-01 (44001-44640)/44640 in 10.60s\n",
      "all from 2016-10-02 until 2016-12-31 (    1- 1000)/36613 in 25.38s\n",
      "all from 2016-10-02 until 2016-12-31 ( 1001- 2000)/36613 in 27.53s\n",
      "all from 2016-10-02 until 2016-12-31 ( 2001- 3000)/36613 in 26.80s\n",
      "all from 2016-10-02 until 2016-12-31 ( 3001- 4000)/36613 in 27.59s\n",
      "all from 2016-10-02 until 2016-12-31 ( 4001- 5000)/36613 in 29.76s\n",
      "all from 2016-10-02 until 2016-12-31 ( 5001- 6000)/36613 in 34.78s\n",
      "all from 2016-10-02 until 2016-12-31 ( 6001- 7000)/36613 in 29.74s\n",
      "all from 2016-10-02 until 2016-12-31 ( 7001- 8000)/36613 in 31.84s\n",
      "all from 2016-10-02 until 2016-12-31 ( 8001- 9000)/36613 in 28.61s\n",
      "all from 2016-10-02 until 2016-12-31 ( 9001-10000)/36613 in 30.13s\n",
      "all from 2016-10-02 until 2016-12-31 (10001-11000)/36613 in 31.11s\n",
      "all from 2016-10-02 until 2016-12-31 (11001-12000)/36613 in 28.68s\n",
      "all from 2016-10-02 until 2016-12-31 (12001-13000)/36613 in 29.38s\n",
      "all from 2016-10-02 until 2016-12-31 (13001-14000)/36613 in 28.76s\n",
      "all from 2016-10-02 until 2016-12-31 (14001-15000)/36613 in 29.04s\n",
      "all from 2016-10-02 until 2016-12-31 (15001-16000)/36613 in 39.03s\n",
      "all from 2016-10-02 until 2016-12-31 (16001-17000)/36613 in 27.48s\n",
      "all from 2016-10-02 until 2016-12-31 (17001-18000)/36613 in 29.69s\n",
      "all from 2016-10-02 until 2016-12-31 (18001-19000)/36613 in 27.43s\n",
      "all from 2016-10-02 until 2016-12-31 (19001-20000)/36613 in 33.94s\n",
      "all from 2016-10-02 until 2016-12-31 (20001-21000)/36613 in 27.65s\n",
      "all from 2016-10-02 until 2016-12-31 (21001-22000)/36613 in 26.72s\n",
      "all from 2016-10-02 until 2016-12-31 (22001-23000)/36613 in 25.35s\n",
      "all from 2016-10-02 until 2016-12-31 (23001-24000)/36613 in 26.37s\n",
      "all from 2016-10-02 until 2016-12-31 (24001-25000)/36613 in 25.88s\n",
      "all from 2016-10-02 until 2016-12-31 (25001-26000)/36613 in 25.67s\n",
      "all from 2016-10-02 until 2016-12-31 (26001-27000)/36613 in 25.37s\n",
      "all from 2016-10-02 until 2016-12-31 (27001-28000)/36613 in 25.44s\n",
      "all from 2016-10-02 until 2016-12-31 (28001-29000)/36613 in 27.11s\n",
      "all from 2016-10-02 until 2016-12-31 (29001-30000)/36613 in 25.10s\n",
      "all from 2016-10-02 until 2016-12-31 (30001-31000)/36613 in 28.08s\n",
      "all from 2016-10-02 until 2016-12-31 (31001-32000)/36613 in 26.46s\n",
      "all from 2016-10-02 until 2016-12-31 (32001-33000)/36613 in 27.03s\n",
      "all from 2016-10-02 until 2016-12-31 (33001-34000)/36613 in 27.17s\n",
      "all from 2016-10-02 until 2016-12-31 (34001-35000)/36613 in 26.00s\n",
      "all from 2016-10-02 until 2016-12-31 (35001-36000)/36613 in 27.58s\n",
      "all from 2016-10-02 until 2016-12-31 (36001-36613)/36613 in 12.47s\n",
      "all from 2016-07-02 until 2016-12-31 retrieved 81253 records in 38 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2017-01-01_to_2017-07-01.csv\n",
      "all from 2017-01-01 until 2017-04-02 (    1- 1000)/31369 in 26.43s\n",
      "all from 2017-01-01 until 2017-04-02 ( 1001- 2000)/31369 in 28.64s\n",
      "all from 2017-01-01 until 2017-04-02 ( 2001- 3000)/31369 in 27.14s\n",
      "all from 2017-01-01 until 2017-04-02 ( 3001- 4000)/31369 in 29.25s\n",
      "all from 2017-01-01 until 2017-04-02 ( 4001- 5000)/31369 in 27.68s\n",
      "all from 2017-01-01 until 2017-04-02 ( 5001- 6000)/31369 in 26.90s\n",
      "all from 2017-01-01 until 2017-04-02 ( 6001- 7000)/31369 in 28.38s\n",
      "all from 2017-01-01 until 2017-04-02 ( 7001- 8000)/31369 in 29.68s\n",
      "all from 2017-01-01 until 2017-04-02 ( 8001- 9000)/31369 in 30.95s\n",
      "all from 2017-01-01 until 2017-04-02 ( 9001-10000)/31369 in 29.77s\n",
      "all from 2017-01-01 until 2017-04-02 (10001-11000)/31369 in 30.51s\n",
      "all from 2017-01-01 until 2017-04-02 (11001-12000)/31369 in 28.94s\n",
      "all from 2017-01-01 until 2017-04-02 (12001-13000)/31369 in 30.77s\n",
      "all from 2017-01-01 until 2017-04-02 (13001-14000)/31369 in 33.55s\n",
      "all from 2017-01-01 until 2017-04-02 (14001-15000)/31369 in 30.07s\n",
      "all from 2017-01-01 until 2017-04-02 (15001-16000)/31369 in 28.49s\n",
      "all from 2017-01-01 until 2017-04-02 (16001-17000)/31369 in 27.82s\n",
      "all from 2017-01-01 until 2017-04-02 (17001-18000)/31369 in 27.56s\n",
      "all from 2017-01-01 until 2017-04-02 (18001-19000)/31369 in 29.03s\n",
      "all from 2017-01-01 until 2017-04-02 (19001-20000)/31369 in 30.80s\n",
      "all from 2017-01-01 until 2017-04-02 (20001-21000)/31369 in 29.06s\n",
      "all from 2017-01-01 until 2017-04-02 (21001-22000)/31369 in 29.31s\n",
      "all from 2017-01-01 until 2017-04-02 (22001-23000)/31369 in 27.71s\n",
      "all from 2017-01-01 until 2017-04-02 (23001-24000)/31369 in 27.13s\n",
      "all from 2017-01-01 until 2017-04-02 (24001-25000)/31369 in 25.44s\n",
      "all from 2017-01-01 until 2017-04-02 (25001-26000)/31369 in 26.31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2017-01-01 until 2017-04-02 (26001-27000)/31369 in 26.97s\n",
      "all from 2017-01-01 until 2017-04-02 (27001-28000)/31369 in 26.83s\n",
      "all from 2017-01-01 until 2017-04-02 (28001-29000)/31369 in 24.59s\n",
      "all from 2017-01-01 until 2017-04-02 (29001-30000)/31369 in 25.03s\n",
      "all from 2017-01-01 until 2017-04-02 (30001-31000)/31369 in 24.56s\n",
      "all from 2017-01-01 until 2017-04-02 (31001-31369)/31369 in 7.20s\n",
      "all from 2017-04-03 until 2017-07-01 (    1- 1000)/32560 in 23.98s\n",
      "all from 2017-04-03 until 2017-07-01 ( 1001- 2000)/32560 in 28.71s\n",
      "all from 2017-04-03 until 2017-07-01 ( 2001- 3000)/32560 in 30.27s\n",
      "all from 2017-04-03 until 2017-07-01 ( 3001- 4000)/32560 in 30.63s\n",
      "all from 2017-04-03 until 2017-07-01 ( 4001- 5000)/32560 in 29.14s\n",
      "all from 2017-04-03 until 2017-07-01 ( 5001- 6000)/32560 in 28.27s\n",
      "all from 2017-04-03 until 2017-07-01 ( 6001- 7000)/32560 in 29.63s\n",
      "all from 2017-04-03 until 2017-07-01 ( 7001- 8000)/32560 in 27.46s\n",
      "all from 2017-04-03 until 2017-07-01 ( 8001- 9000)/32560 in 29.85s\n",
      "all from 2017-04-03 until 2017-07-01 ( 9001-10000)/32560 in 30.65s\n",
      "all from 2017-04-03 until 2017-07-01 (10001-11000)/32560 in 28.44s\n",
      "all from 2017-04-03 until 2017-07-01 (11001-12000)/32560 in 27.86s\n",
      "all from 2017-04-03 until 2017-07-01 (12001-13000)/32560 in 28.86s\n",
      "all from 2017-04-03 until 2017-07-01 (13001-14000)/32560 in 29.24s\n",
      "all from 2017-04-03 until 2017-07-01 (14001-15000)/32560 in 31.25s\n",
      "all from 2017-04-03 until 2017-07-01 (15001-16000)/32560 in 28.67s\n",
      "all from 2017-04-03 until 2017-07-01 (16001-17000)/32560 in 27.56s\n",
      "all from 2017-04-03 until 2017-07-01 (17001-18000)/32560 in 27.26s\n",
      "all from 2017-04-03 until 2017-07-01 (18001-19000)/32560 in 27.14s\n",
      "all from 2017-04-03 until 2017-07-01 (19001-20000)/32560 in 32.90s\n",
      "all from 2017-04-03 until 2017-07-01 (20001-21000)/32560 in 27.02s\n",
      "all from 2017-04-03 until 2017-07-01 (21001-22000)/32560 in 30.03s\n",
      "all from 2017-04-03 until 2017-07-01 (22001-23000)/32560 in 27.50s\n",
      "all from 2017-04-03 until 2017-07-01 (23001-24000)/32560 in 27.62s\n",
      "all from 2017-04-03 until 2017-07-01 (24001-25000)/32560 in 27.63s\n",
      "all from 2017-04-03 until 2017-07-01 (25001-26000)/32560 in 27.92s\n",
      "all from 2017-04-03 until 2017-07-01 (26001-27000)/32560 in 25.22s\n",
      "all from 2017-04-03 until 2017-07-01 (27001-28000)/32560 in 27.42s\n",
      "all from 2017-04-03 until 2017-07-01 (28001-29000)/32560 in 26.60s\n",
      "all from 2017-04-03 until 2017-07-01 (29001-30000)/32560 in 24.60s\n",
      "all from 2017-04-03 until 2017-07-01 (30001-31000)/32560 in 25.77s\n",
      "all from 2017-04-03 until 2017-07-01 (31001-32000)/32560 in 25.68s\n",
      "all from 2017-04-03 until 2017-07-01 (32001-32560)/32560 in 10.09s\n",
      "all from 2017-01-01 until 2017-07-01 retrieved 63929 records in 30 min\n",
      "\n",
      "Writing to arXivMeta_all_from_2017-07-02_to_2017-12-31.csv\n",
      "all from 2017-07-02 until 2017-10-01 (    1- 1000)/36773 in 26.70s\n",
      "all from 2017-07-02 until 2017-10-01 ( 1001- 2000)/36773 in 25.36s\n",
      "all from 2017-07-02 until 2017-10-01 ( 2001- 3000)/36773 in 27.27s\n",
      "all from 2017-07-02 until 2017-10-01 ( 3001- 4000)/36773 in 25.54s\n",
      "all from 2017-07-02 until 2017-10-01 ( 4001- 5000)/36773 in 28.66s\n",
      "all from 2017-07-02 until 2017-10-01 ( 5001- 6000)/36773 in 25.93s\n",
      "all from 2017-07-02 until 2017-10-01 ( 6001- 7000)/36773 in 26.32s\n",
      "all from 2017-07-02 until 2017-10-01 ( 7001- 8000)/36773 in 26.04s\n",
      "all from 2017-07-02 until 2017-10-01 ( 8001- 9000)/36773 in 30.29s\n",
      "all from 2017-07-02 until 2017-10-01 ( 9001-10000)/36773 in 56.62s\n",
      "all from 2017-07-02 until 2017-10-01 (10001-11000)/36773 in 28.52s\n",
      "all from 2017-07-02 until 2017-10-01 (11001-12000)/36773 in 45.98s\n",
      "all from 2017-07-02 until 2017-10-01 (12001-13000)/36773 in 32.42s\n",
      "all from 2017-07-02 until 2017-10-01 (13001-14000)/36773 in 47.62s\n",
      "all from 2017-07-02 until 2017-10-01 (14001-15000)/36773 in 31.22s\n",
      "all from 2017-07-02 until 2017-10-01 (15001-16000)/36773 in 27.21s\n",
      "all from 2017-07-02 until 2017-10-01 (16001-17000)/36773 in 26.53s\n",
      "all from 2017-07-02 until 2017-10-01 (17001-18000)/36773 in 28.32s\n",
      "all from 2017-07-02 until 2017-10-01 (18001-19000)/36773 in 26.96s\n",
      "all from 2017-07-02 until 2017-10-01 (19001-20000)/36773 in 27.94s\n",
      "all from 2017-07-02 until 2017-10-01 (20001-21000)/36773 in 29.62s\n",
      "all from 2017-07-02 until 2017-10-01 (21001-22000)/36773 in 29.32s\n",
      "all from 2017-07-02 until 2017-10-01 (22001-23000)/36773 in 30.21s\n",
      "all from 2017-07-02 until 2017-10-01 (23001-24000)/36773 in 25.26s\n",
      "all from 2017-07-02 until 2017-10-01 (24001-25000)/36773 in 26.23s\n",
      "all from 2017-07-02 until 2017-10-01 (25001-26000)/36773 in 25.58s\n",
      "all from 2017-07-02 until 2017-10-01 (26001-27000)/36773 in 26.46s\n",
      "all from 2017-07-02 until 2017-10-01 (27001-28000)/36773 in 26.21s\n",
      "all from 2017-07-02 until 2017-10-01 (28001-29000)/36773 in 25.13s\n",
      "all from 2017-07-02 until 2017-10-01 (29001-30000)/36773 in 25.56s\n",
      "all from 2017-07-02 until 2017-10-01 (30001-31000)/36773 in 32.62s\n",
      "all from 2017-07-02 until 2017-10-01 (31001-32000)/36773 in 26.93s\n",
      "all from 2017-07-02 until 2017-10-01 (32001-33000)/36773 in 27.65s\n",
      "all from 2017-07-02 until 2017-10-01 (33001-34000)/36773 in 29.20s\n",
      "all from 2017-07-02 until 2017-10-01 (34001-35000)/36773 in 27.10s\n",
      "all from 2017-07-02 until 2017-10-01 (35001-36000)/36773 in 27.51s\n",
      "all from 2017-07-02 until 2017-10-01 (36001-36773)/36773 in 14.62s\n",
      "all from 2017-10-02 until 2017-12-31 (    1- 1000)/36351 in 25.97s\n",
      "all from 2017-10-02 until 2017-12-31 ( 1001- 2000)/36351 in 25.73s\n",
      "all from 2017-10-02 until 2017-12-31 ( 2001- 3000)/36351 in 27.82s\n",
      "all from 2017-10-02 until 2017-12-31 ( 3001- 4000)/36351 in 29.22s\n",
      "all from 2017-10-02 until 2017-12-31 ( 4001- 5000)/36351 in 32.71s\n",
      "all from 2017-10-02 until 2017-12-31 ( 5001- 6000)/36351 in 28.15s\n",
      "all from 2017-10-02 until 2017-12-31 ( 6001- 7000)/36351 in 30.66s\n",
      "all from 2017-10-02 until 2017-12-31 ( 7001- 8000)/36351 in 29.57s\n",
      "all from 2017-10-02 until 2017-12-31 ( 8001- 9000)/36351 in 28.49s\n",
      "all from 2017-10-02 until 2017-12-31 ( 9001-10000)/36351 in 27.22s\n",
      "all from 2017-10-02 until 2017-12-31 (10001-11000)/36351 in 27.26s\n",
      "all from 2017-10-02 until 2017-12-31 (11001-12000)/36351 in 28.80s\n",
      "all from 2017-10-02 until 2017-12-31 (12001-13000)/36351 in 28.85s\n",
      "all from 2017-10-02 until 2017-12-31 (13001-14000)/36351 in 34.40s\n",
      "all from 2017-10-02 until 2017-12-31 (14001-15000)/36351 in 29.71s\n",
      "all from 2017-10-02 until 2017-12-31 (15001-16000)/36351 in 28.97s\n",
      "all from 2017-10-02 until 2017-12-31 (16001-17000)/36351 in 31.07s\n",
      "all from 2017-10-02 until 2017-12-31 (17001-18000)/36351 in 30.54s\n",
      "all from 2017-10-02 until 2017-12-31 (18001-19000)/36351 in 31.11s\n",
      "all from 2017-10-02 until 2017-12-31 (19001-20000)/36351 in 28.26s\n",
      "all from 2017-10-02 until 2017-12-31 (20001-21000)/36351 in 26.84s\n",
      "all from 2017-10-02 until 2017-12-31 (21001-22000)/36351 in 30.12s\n",
      "all from 2017-10-02 until 2017-12-31 (22001-23000)/36351 in 27.27s\n",
      "all from 2017-10-02 until 2017-12-31 (23001-24000)/36351 in 27.40s\n",
      "all from 2017-10-02 until 2017-12-31 (24001-25000)/36351 in 26.46s\n",
      "all from 2017-10-02 until 2017-12-31 (25001-26000)/36351 in 25.73s\n",
      "all from 2017-10-02 until 2017-12-31 (26001-27000)/36351 in 26.27s\n",
      "all from 2017-10-02 until 2017-12-31 (27001-28000)/36351 in 25.74s\n",
      "all from 2017-10-02 until 2017-12-31 (28001-29000)/36351 in 28.28s\n",
      "all from 2017-10-02 until 2017-12-31 (29001-30000)/36351 in 26.77s\n",
      "all from 2017-10-02 until 2017-12-31 (30001-31000)/36351 in 25.02s\n",
      "all from 2017-10-02 until 2017-12-31 (31001-32000)/36351 in 26.77s\n",
      "all from 2017-10-02 until 2017-12-31 (32001-33000)/36351 in 27.20s\n",
      "all from 2017-10-02 until 2017-12-31 (33001-34000)/36351 in 27.96s\n",
      "all from 2017-10-02 until 2017-12-31 (34001-35000)/36351 in 25.85s\n",
      "all from 2017-10-02 until 2017-12-31 (35001-36000)/36351 in 26.71s\n",
      "all from 2017-10-02 until 2017-12-31 (36001-36351)/36351 in 7.92s\n",
      "all from 2017-07-02 until 2017-12-31 retrieved 73124 records in 35 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple years (also split in two files each)\n",
    "\n",
    "for year in ['2011','2012','2013','2014','2015','2016','2017']:\n",
    "    harvest_data(f\"{year}-01-01\", f\"{year}-07-01\")\n",
    "    harvest_data(f\"{year}-07-02\", f\"{year}-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
