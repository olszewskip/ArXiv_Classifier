{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv metadata harvester\n",
    "\n",
    "## Grab records from the requested timespan, from all or from one selected category\n",
    "\n",
    "## Write to tab-delimited local csv:\n",
    "## columns: *id, primary_category, sub_categories, title, abstract*\n",
    "(Dealing with funny characters in the names of authors was beyond me. One could also get a date associated with each record but it's supposed not to necessarily correspond to the date of posting by the authors.)\n",
    "### There are two functions; both will talk to You using prints:\n",
    "* *harvest_slice* needs You to explicitly choose the category (possibly 'all') and the filename as arguments\n",
    "    * just appends lines to the file, it's up to You not to make a mess\n",
    "    \n",
    "* *harvest_data* divides the timespan into slices of given length (367 days by default) and harvests those using *harvest_slice*:\n",
    "    * can make up the name of the file on its own\n",
    "    * adds the header to the csv\n",
    "    * default behavior when the file already exists is to quit\n",
    "    * default category is 'all'\n",
    "\n",
    "### It is slow.    \n",
    "### Examples:\n",
    "*  1 min,  2 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")\n",
    "*  4 min, 11 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")\n",
    "* 8 min, 16 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)\n",
    "* 30 min, 68 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\")\n",
    "\n",
    "### Example of a basic query used in the code:\n",
    "* http://export.arxiv.org/oai2?verb=ListRecords&from=2012-01-01&until=2018-02-01&set=physics:hep-th&metadataPrefix=arXiv\n",
    "* \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "\n",
    "See https://arxiv.org/help/bulk_data for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each paper on arXive has assigned to it a single primary category, e.g. *cs* (Computer Science), *econ* (Economics), etc. We can retrieve the list of existing categories from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'econ': 'Economics',\n",
       " 'eess': 'Electrical Engineering and Systems Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'physics:astro-ph': 'Astrophysics',\n",
       " 'physics:cond-mat': 'Condensed Matter',\n",
       " 'physics:gr-qc': 'General Relativity and Quantum Cosmology',\n",
       " 'physics:hep-ex': 'High Energy Physics - Experiment',\n",
       " 'physics:hep-lat': 'High Energy Physics - Lattice',\n",
       " 'physics:hep-ph': 'High Energy Physics - Phenomenology',\n",
       " 'physics:hep-th': 'High Energy Physics - Theory',\n",
       " 'physics:math-ph': 'Mathematical Physics',\n",
       " 'physics:nlin': 'Nonlinear Sciences',\n",
       " 'physics:nucl-ex': 'Nuclear Experiment',\n",
       " 'physics:nucl-th': 'Nuclear Theory',\n",
       " 'physics:physics': 'Physics (Other)',\n",
       " 'physics:quant-ph': 'Quantum Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query retrieves short xml meta-metadata containing info\n",
    "# about the accesible arXiv categories of topics called 'sets', e.g.\n",
    "# <set>\n",
    "# <setspec>cs</setspec>\n",
    "# <setname>Computer Science</setname>\n",
    "# </set>\n",
    "\n",
    "\n",
    "if not Path(\"categories.txt\").is_file() :\n",
    "    \n",
    "    xml_query = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "    sauce = urllib.request.urlopen(xml_query).read()\n",
    "    soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "    sets = soup.find_all(\"set\")\n",
    "\n",
    "    categories = {}\n",
    "\n",
    "    for set_ in sets:\n",
    "        categories[set_.setspec.string] = set_.setname.string\n",
    "\n",
    "    with open(\"categories.txt\", \"w\") as file:\n",
    "        for category, description in categories.items():\n",
    "            file.write(\"\".join([category,\"\\t\" , description, \"\\n\"]))\n",
    "            \n",
    "\n",
    "categories = {}\n",
    "with open(\"categories.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        category, description = line.rstrip('\\n').split('\\t')\n",
    "        categories[category] = description\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The matter of categories is actually more messy. Physics gets an additional level of gradation: e.g. *physics:astro-ph* is a subset of *physics*. And in general categorizations chosen by an author may be finer, e.g. *cs.ai* (Computer Science: Artificial Intelligence) instead of just *cs*, *physics:astro-ph.GA* (Physics: Astrophysics: Astrophysics of Galaxies) instead of just *physics*.\n",
    "\n",
    "### What You see above are just names of *sets* that can be used in the API queries. Apparently physics enthusiasts get more options. Also *Economics* and *Electrical Engineering* start only in 2017 (I've checked).\n",
    "\n",
    "### Lets treat *physics* as a single category for consistency, and let's exclude *econ* and *eess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = {cat: cat_name for (cat, cat_name) in categories.items() if cat not in  ['econ' ,'eess'] and not re.match(r'physics:.+', cat)}\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_slice(date_from, date_until, category, file) -> int:\n",
    "    # returns -1 if unsuccesful\n",
    "    # returns number of downloaded records if succesful\n",
    "    \n",
    "    base_query = \"http://export.arxiv.org/oai2?verb=ListRecords\"\n",
    "    \n",
    "    if category == \"all\":\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&metadataPrefix=arXiv\"\n",
    "    else:\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&set={category}&metadataPrefix=arXiv\"\n",
    "    \n",
    "    retrieved = 0\n",
    "    \n",
    "    while query:\n",
    "        \n",
    "        time_0 = time.time()\n",
    "        \n",
    "        # try to download\n",
    "        try:            \n",
    "            sauce = urllib.request.urlopen(query).read()\n",
    "\n",
    "        except:\n",
    "            print(f\"Failed at: {category} from {date_from} until {date_until} requesting {query}\\n\")\n",
    "            return -1\n",
    "        \n",
    "        # parse the xml looking for <record>'s\n",
    "        soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "        records = soup.find_all('record')\n",
    "\n",
    "        retrieved = retrieved + len(records)\n",
    "\n",
    "        with open(file, \"a\") as dump:\n",
    "\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            for record in records:                \n",
    "                record_string = [record.id.string,\n",
    "                                 record.setspec.string,\n",
    "                                 record.categories.string,\n",
    "                                 record.title.string,\n",
    "                                 record.abstract.string\n",
    "                                ]\n",
    "                writer.writerow(record_string)\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"\".join([category,\" from \", f\"{date_from}\",\" until \", f\"{date_until}\",\" empty\"]))\n",
    "            break\n",
    "        \n",
    "        # info at the end of 'soup' about where to resume if the data stream was cut at 1000 records\n",
    "        # None if the stream wasn't cut\n",
    "        res_token = soup.find(\"resumptiontoken\")\n",
    "        \n",
    "        if res_token:\n",
    "\n",
    "            # data in the current loop started at this record in the 'query'\n",
    "            started_at = int(res_token['cursor']) + 1\n",
    "            \n",
    "            # total number of records in the 'query', should be the same in each loop\n",
    "            all_to_retrieve = int(res_token['completelistsize'])\n",
    "            \n",
    "            if res_token.string:\n",
    "                # the identifier that allows to resume the query\n",
    "                # None if the slice was completed\n",
    "\n",
    "                query = base_query + f\"&resumptionToken={res_token.string}\"\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                query = None\n",
    "            \n",
    "        else:\n",
    "            started_at = 1 \n",
    "            all_to_retrieve = len(records)\n",
    "            query = None\n",
    "        \n",
    "        time_1 = time.time()\n",
    "        \n",
    "        print(\"\".join([category,\n",
    "                       \" from \", f\"{date_from}\", \" until \", f\"{date_until}\",\n",
    "                       f\" ({started_at:>5}-{started_at+len(records)-1:>5})/{all_to_retrieve:>5}\",\n",
    "                      \" in \", f\"{(time_1 - time_0):3.2f}\", \"s\"]) )\n",
    "    \n",
    "    # end of while loop\n",
    "     \n",
    "    return retrieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math from 2018-10-01 until 2018-10-10 (    1- 1000)/ 2358 in 22.93s\n",
      "math from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 2358 in 25.32s\n",
      "math from 2018-10-01 until 2018-10-10 ( 2001- 2358)/ 2358 in 5.24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2358"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2018-10-01 until 2018-10-10 (    1- 1000)/ 7663 in 32.15s\n",
      "all from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 7663 in 25.72s\n",
      "all from 2018-10-01 until 2018-10-10 ( 2001- 3000)/ 7663 in 26.29s\n",
      "all from 2018-10-01 until 2018-10-10 ( 3001- 4000)/ 7663 in 25.64s\n",
      "all from 2018-10-01 until 2018-10-10 ( 4001- 5000)/ 7663 in 24.63s\n",
      "all from 2018-10-01 until 2018-10-10 ( 5001- 6000)/ 7663 in 24.01s\n",
      "all from 2018-10-01 until 2018-10-10 ( 6001- 7000)/ 7663 in 25.05s\n",
      "all from 2018-10-01 until 2018-10-10 ( 7001- 7663)/ 7663 in 10.29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7663"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around harvest_slice\n",
    "# * handles file-names\n",
    "# * slices the time period of papers into intervals of given number of days (367 by default)\n",
    "# (to avoid maxing-out the response from server)\n",
    "\n",
    "def harvest_data(isoday_0, isoday_1, category='all', days_in_slice = 367, file_name=None, overwrite=False) -> int:\n",
    "\n",
    "    date_0 = dateutil.parser.parse(isoday_0).date()\n",
    "    date_1 = dateutil.parser.parse(isoday_1).date()\n",
    "\n",
    "    if not file_name:\n",
    "        # create a file with an overly descriptive name\n",
    "        file = f\"arXivMeta_{category.replace(':','--')}_from_{date_0}_to_{date_1}.csv\"\n",
    "    else:\n",
    "        file = file_name\n",
    "    \n",
    "    # check if file already exists\n",
    "    if Path(file).is_file():\n",
    "        if overwrite :\n",
    "\n",
    "            # try to backup the old file\n",
    "            file_info = re.match(r\"(\\w.+)\\.(\\w\\w+)\", file)\n",
    "            if file_info:\n",
    "                new_file = \"\".join([ file_info.group(1), \"_bak.\", file_info.group(2) ])\n",
    "                if not Path(new_file).is_file():\n",
    "                    os.rename(file, new_file)\n",
    "                    print(f\"Old file backed up as {new_file}\")\n",
    "\n",
    "            # clear the file\n",
    "            print(f\"Overwriting {file}\\n\")\n",
    "            with open(file, \"w\") as dump:\n",
    "                dump.truncate(0)\n",
    "            \n",
    "        else:\n",
    "            print(f\"The file {file} already exists\")\n",
    "            return -1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Writing to {file}\\n\")\n",
    "    \n",
    "    with open(file, \"a\") as dump:\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            header = ['id', 'prim_cat', 'sec_cats', 'title', 'abstract']\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    # Star the clock\n",
    "    time_0 = time.time()\n",
    "    \n",
    "    # Let's count all downloaded records\n",
    "    retrieved = 0\n",
    "    \n",
    "    # We'll go from 'date_0' until 'date_1' in slices of 'days_in_slice' days\n",
    "    # The server's response presumably maxes out at some number of records,\n",
    "    # so we hope to have slices with less records than that.\n",
    "\n",
    "    date_from = date_0\n",
    "\n",
    "    while date_from <= date_1:\n",
    "        \n",
    "        date_until = min(date_1, date_from + datetime.timedelta(days_in_slice-1))\n",
    "\n",
    "        # try to download the slice\n",
    "        newly_retrieved = harvest_slice(date_from, date_until, category, file)\n",
    "        \n",
    "        if newly_retrieved == -1:\n",
    "            break\n",
    "        \n",
    "        retrieved = retrieved + newly_retrieved\n",
    "\n",
    "        # move on to the next slice\n",
    "        date_from = date_until + datetime.timedelta(days=1)\n",
    "        \n",
    "        # time-out\n",
    "        time.sleep(10)\n",
    "\n",
    "    time_1 = time.time()\n",
    "    \n",
    "    print(\"\".join([category,\n",
    "                   \" from \", str(date_0), \" until \", str(date_1),\n",
    "                   \" retrieved \", str(retrieved), \" records\"\n",
    "                   ,\" in \", f\"{(time_1 - time_0)/60:.0f}\", \" min\\n\"])\n",
    "         )\n",
    "    \n",
    "    return retrieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to test.csv\n",
      "\n",
      "math from 2018-08-01 until 2018-11-01 (    1- 1000)/18739 in 26.22s\n",
      "math from 2018-08-01 until 2018-11-01 ( 1001- 2000)/18739 in 26.94s\n",
      "math from 2018-08-01 until 2018-11-01 ( 2001- 3000)/18739 in 27.15s\n",
      "math from 2018-08-01 until 2018-11-01 ( 3001- 4000)/18739 in 24.76s\n",
      "math from 2018-08-01 until 2018-11-01 ( 4001- 5000)/18739 in 26.61s\n",
      "math from 2018-08-01 until 2018-11-01 ( 5001- 6000)/18739 in 24.70s\n",
      "math from 2018-08-01 until 2018-11-01 ( 6001- 7000)/18739 in 24.08s\n",
      "math from 2018-08-01 until 2018-11-01 ( 7001- 8000)/18739 in 26.17s\n",
      "math from 2018-08-01 until 2018-11-01 ( 8001- 9000)/18739 in 23.18s\n",
      "math from 2018-08-01 until 2018-11-01 ( 9001-10000)/18739 in 24.53s\n",
      "math from 2018-08-01 until 2018-11-01 (10001-11000)/18739 in 24.96s\n",
      "math from 2018-08-01 until 2018-11-01 (11001-12000)/18739 in 23.93s\n",
      "math from 2018-08-01 until 2018-11-01 (12001-13000)/18739 in 23.57s\n",
      "math from 2018-08-01 until 2018-11-01 (13001-14000)/18739 in 22.86s\n",
      "math from 2018-08-01 until 2018-11-01 (14001-15000)/18739 in 23.51s\n",
      "math from 2018-08-01 until 2018-11-01 (15001-16000)/18739 in 22.78s\n",
      "math from 2018-08-01 until 2018-11-01 (16001-17000)/18739 in 23.27s\n",
      "math from 2018-08-01 until 2018-11-01 (17001-18000)/18739 in 22.73s\n",
      "math from 2018-08-01 until 2018-11-01 (18001-18739)/18739 in 10.13s\n",
      "math from 2018-08-01 until 2018-11-01 retrieved 18739 records in   8min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18739"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_all_from_2018-08-01_to_2018-11-01.csv\n",
      "\n",
      "all from 2018-08-01 until 2018-11-01 (    1- 1000)/61276 in 26.90s\n",
      "all from 2018-08-01 until 2018-11-01 ( 1001- 2000)/61276 in 27.15s\n",
      "all from 2018-08-01 until 2018-11-01 ( 2001- 3000)/61276 in 27.79s\n",
      "all from 2018-08-01 until 2018-11-01 ( 3001- 4000)/61276 in 28.60s\n",
      "all from 2018-08-01 until 2018-11-01 ( 4001- 5000)/61276 in 41.76s\n",
      "all from 2018-08-01 until 2018-11-01 ( 5001- 6000)/61276 in 28.32s\n",
      "all from 2018-08-01 until 2018-11-01 ( 6001- 7000)/61276 in 26.41s\n",
      "all from 2018-08-01 until 2018-11-01 ( 7001- 8000)/61276 in 27.18s\n",
      "all from 2018-08-01 until 2018-11-01 ( 8001- 9000)/61276 in 30.19s\n",
      "all from 2018-08-01 until 2018-11-01 ( 9001-10000)/61276 in 29.14s\n",
      "all from 2018-08-01 until 2018-11-01 (10001-11000)/61276 in 30.14s\n",
      "all from 2018-08-01 until 2018-11-01 (11001-12000)/61276 in 27.50s\n",
      "all from 2018-08-01 until 2018-11-01 (12001-13000)/61276 in 26.80s\n",
      "all from 2018-08-01 until 2018-11-01 (13001-14000)/61276 in 72.56s\n",
      "all from 2018-08-01 until 2018-11-01 (14001-15000)/61276 in 27.83s\n",
      "all from 2018-08-01 until 2018-11-01 (15001-16000)/61276 in 28.64s\n",
      "all from 2018-08-01 until 2018-11-01 (16001-17000)/61276 in 27.29s\n",
      "all from 2018-08-01 until 2018-11-01 (17001-18000)/61276 in 28.67s\n",
      "all from 2018-08-01 until 2018-11-01 (18001-19000)/61276 in 30.01s\n",
      "all from 2018-08-01 until 2018-11-01 (19001-20000)/61276 in 27.59s\n",
      "all from 2018-08-01 until 2018-11-01 (20001-21000)/61276 in 30.23s\n",
      "all from 2018-08-01 until 2018-11-01 (21001-22000)/61276 in 31.88s\n",
      "all from 2018-08-01 until 2018-11-01 (22001-23000)/61276 in 26.92s\n",
      "all from 2018-08-01 until 2018-11-01 (23001-24000)/61276 in 29.73s\n",
      "all from 2018-08-01 until 2018-11-01 (24001-25000)/61276 in 28.47s\n",
      "all from 2018-08-01 until 2018-11-01 (25001-26000)/61276 in 28.55s\n",
      "all from 2018-08-01 until 2018-11-01 (26001-27000)/61276 in 28.14s\n",
      "all from 2018-08-01 until 2018-11-01 (27001-28000)/61276 in 26.14s\n",
      "all from 2018-08-01 until 2018-11-01 (28001-29000)/61276 in 27.20s\n",
      "all from 2018-08-01 until 2018-11-01 (29001-30000)/61276 in 25.48s\n",
      "all from 2018-08-01 until 2018-11-01 (30001-31000)/61276 in 25.42s\n",
      "all from 2018-08-01 until 2018-11-01 (31001-32000)/61276 in 25.41s\n",
      "all from 2018-08-01 until 2018-11-01 (32001-33000)/61276 in 25.96s\n",
      "all from 2018-08-01 until 2018-11-01 (33001-34000)/61276 in 25.73s\n",
      "all from 2018-08-01 until 2018-11-01 (34001-35000)/61276 in 25.34s\n",
      "all from 2018-08-01 until 2018-11-01 (35001-36000)/61276 in 25.65s\n",
      "all from 2018-08-01 until 2018-11-01 (36001-37000)/61276 in 33.56s\n",
      "all from 2018-08-01 until 2018-11-01 (37001-38000)/61276 in 25.57s\n",
      "all from 2018-08-01 until 2018-11-01 (38001-39000)/61276 in 24.94s\n",
      "all from 2018-08-01 until 2018-11-01 (39001-40000)/61276 in 23.84s\n",
      "all from 2018-08-01 until 2018-11-01 (40001-41000)/61276 in 24.87s\n",
      "all from 2018-08-01 until 2018-11-01 (41001-42000)/61276 in 24.44s\n",
      "all from 2018-08-01 until 2018-11-01 (42001-43000)/61276 in 24.66s\n",
      "all from 2018-08-01 until 2018-11-01 (43001-44000)/61276 in 25.41s\n",
      "all from 2018-08-01 until 2018-11-01 (44001-45000)/61276 in 24.67s\n",
      "all from 2018-08-01 until 2018-11-01 (45001-46000)/61276 in 23.98s\n",
      "all from 2018-08-01 until 2018-11-01 (46001-47000)/61276 in 25.46s\n",
      "all from 2018-08-01 until 2018-11-01 (47001-48000)/61276 in 25.48s\n",
      "all from 2018-08-01 until 2018-11-01 (48001-49000)/61276 in 24.75s\n",
      "all from 2018-08-01 until 2018-11-01 (49001-50000)/61276 in 25.54s\n",
      "all from 2018-08-01 until 2018-11-01 (50001-51000)/61276 in 25.93s\n",
      "all from 2018-08-01 until 2018-11-01 (51001-52000)/61276 in 24.73s\n",
      "all from 2018-08-01 until 2018-11-01 (52001-53000)/61276 in 24.43s\n",
      "all from 2018-08-01 until 2018-11-01 (53001-54000)/61276 in 23.98s\n",
      "all from 2018-08-01 until 2018-11-01 (54001-55000)/61276 in 23.98s\n",
      "all from 2018-08-01 until 2018-11-01 (55001-56000)/61276 in 24.45s\n",
      "all from 2018-08-01 until 2018-11-01 (56001-57000)/61276 in 23.88s\n",
      "all from 2018-08-01 until 2018-11-01 (57001-58000)/61276 in 23.92s\n",
      "all from 2018-08-01 until 2018-11-01 (58001-59000)/61276 in 24.99s\n",
      "all from 2018-08-01 until 2018-11-01 (59001-60000)/61276 in 34.16s\n",
      "all from 2018-08-01 until 2018-11-01 (60001-61000)/61276 in 23.68s\n",
      "all from 2018-08-01 until 2018-11-01 (61001-61276)/61276 in 15.13s\n",
      "all from 2018-08-01 until 2018-11-01 retrieved 61276 records in 29min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61276"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_data(\"2018-08-01\", \"2018-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_stat_from_2017-01-01_to_2017-12-31.csv\n",
      "\n",
      "stat from 2017-01-01 until 2017-12-31 (    1- 1000)/ 7533 in 61.89s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 1001- 2000)/ 7533 in 168.69s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 2001- 3000)/ 7533 in 52.54s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 3001- 4000)/ 7533 in 75.94s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 4001- 5000)/ 7533 in 96.69s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 5001- 6000)/ 7533 in 119.96s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 6001- 7000)/ 7533 in 253.64s\n",
      "stat from 2017-01-01 until 2017-12-31 ( 7001- 7533)/ 7533 in 8.77s\n",
      "stat from 2017-01-01 until 2017-12-31 retrieved 7533 records in 14 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7533"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'stat'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_q-fin_from_2017-01-01_to_2017-12-31.csv\n",
      "\n",
      "q-fin from 2017-01-01 until 2017-12-31 (    1- 1000)/ 1001 in 23.46s\n",
      "q-fin from 2017-01-01 until 2017-12-31 ( 1001- 1001)/ 1001 in 1.74s\n",
      "q-fin from 2017-01-01 until 2017-12-31 retrieved 1001 records in 1 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'q-fin'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_q-bio_from_2017-01-01_to_2017-12-31.csv\n",
      "\n",
      "q-bio from 2017-01-01 until 2017-12-31 (    1- 1000)/ 2745 in 27.95s\n",
      "q-bio from 2017-01-01 until 2017-12-31 ( 1001- 2000)/ 2745 in 30.29s\n",
      "q-bio from 2017-01-01 until 2017-12-31 ( 2001- 2745)/ 2745 in 45.23s\n",
      "q-bio from 2017-01-01 until 2017-12-31 retrieved 2745 records in 2 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'q-bio'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_cs_from_2017-01-01_to_2017-07-01.csv\n",
      "\n",
      "cs from 2017-01-01 until 2017-07-01 (    1- 1000)/14336 in 65.01s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 1001- 2000)/14336 in 59.40s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 2001- 3000)/14336 in 50.41s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 3001- 4000)/14336 in 64.74s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 4001- 5000)/14336 in 81.46s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 5001- 6000)/14336 in 67.16s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 6001- 7000)/14336 in 52.28s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 7001- 8000)/14336 in 67.22s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 8001- 9000)/14336 in 97.56s\n",
      "cs from 2017-01-01 until 2017-07-01 ( 9001-10000)/14336 in 80.68s\n",
      "cs from 2017-01-01 until 2017-07-01 (10001-11000)/14336 in 139.89s\n",
      "cs from 2017-01-01 until 2017-07-01 (11001-12000)/14336 in 67.61s\n",
      "cs from 2017-01-01 until 2017-07-01 (12001-13000)/14336 in 60.01s\n",
      "cs from 2017-01-01 until 2017-07-01 (13001-14000)/14336 in 25.18s\n",
      "cs from 2017-01-01 until 2017-07-01 (14001-14336)/14336 in 5.84s\n",
      "cs from 2017-01-01 until 2017-07-01 retrieved 14336 records in 17 min\n",
      "\n",
      "Writing to arXivMeta_cs_from_2017-07-02_to_2017-12-31.csv\n",
      "\n",
      "cs from 2017-07-02 until 2017-12-31 (    1- 1000)/16447 in 24.77s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 1001- 2000)/16447 in 24.36s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 2001- 3000)/16447 in 59.00s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 3001- 4000)/16447 in 170.54s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 4001- 5000)/16447 in 107.72s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 5001- 6000)/16447 in 129.56s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 6001- 7000)/16447 in 109.27s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 7001- 8000)/16447 in 191.88s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 8001- 9000)/16447 in 167.95s\n",
      "cs from 2017-07-02 until 2017-12-31 ( 9001-10000)/16447 in 45.45s\n",
      "cs from 2017-07-02 until 2017-12-31 (10001-11000)/16447 in 36.54s\n",
      "cs from 2017-07-02 until 2017-12-31 (11001-12000)/16447 in 50.91s\n",
      "cs from 2017-07-02 until 2017-12-31 (12001-13000)/16447 in 41.83s\n",
      "cs from 2017-07-02 until 2017-12-31 (13001-14000)/16447 in 46.51s\n",
      "cs from 2017-07-02 until 2017-12-31 (14001-15000)/16447 in 91.26s\n",
      "cs from 2017-07-02 until 2017-12-31 (15001-16000)/16447 in 83.95s\n",
      "cs from 2017-07-02 until 2017-12-31 (16001-16447)/16447 in 70.01s\n",
      "cs from 2017-07-02 until 2017-12-31 retrieved 16447 records in 24 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16447"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'cs'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-07-01\", category=cat)\n",
    "harvest_data(f\"{year}-07-02\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_math_from_2017-01-01_to_2017-05-01.csv\n",
      "\n",
      "math from 2017-01-01 until 2017-05-01 (    1- 1000)/14122 in 51.04s\n",
      "math from 2017-01-01 until 2017-05-01 ( 1001- 2000)/14122 in 167.79s\n",
      "math from 2017-01-01 until 2017-05-01 ( 2001- 3000)/14122 in 169.52s\n",
      "math from 2017-01-01 until 2017-05-01 ( 3001- 4000)/14122 in 117.55s\n",
      "math from 2017-01-01 until 2017-05-01 ( 4001- 5000)/14122 in 116.51s\n",
      "math from 2017-01-01 until 2017-05-01 ( 5001- 6000)/14122 in 24.90s\n",
      "math from 2017-01-01 until 2017-05-01 ( 6001- 7000)/14122 in 29.22s\n",
      "math from 2017-01-01 until 2017-05-01 ( 7001- 8000)/14122 in 26.21s\n",
      "math from 2017-01-01 until 2017-05-01 ( 8001- 9000)/14122 in 24.00s\n",
      "math from 2017-01-01 until 2017-05-01 ( 9001-10000)/14122 in 23.72s\n",
      "math from 2017-01-01 until 2017-05-01 (10001-11000)/14122 in 25.60s\n",
      "math from 2017-01-01 until 2017-05-01 (11001-12000)/14122 in 26.83s\n",
      "math from 2017-01-01 until 2017-05-01 (12001-13000)/14122 in 23.24s\n",
      "math from 2017-01-01 until 2017-05-01 (13001-14000)/14122 in 26.16s\n",
      "math from 2017-01-01 until 2017-05-01 (14001-14122)/14122 in 3.78s\n",
      "math from 2017-01-01 until 2017-05-01 retrieved 14122 records in 14 min\n",
      "\n",
      "Writing to arXivMeta_math_from_2017-05-02_to_2017-09-01.csv\n",
      "\n",
      "math from 2017-05-02 until 2017-09-01 (    1- 1000)/15132 in 26.53s\n",
      "math from 2017-05-02 until 2017-09-01 ( 1001- 2000)/15132 in 24.87s\n",
      "math from 2017-05-02 until 2017-09-01 ( 2001- 3000)/15132 in 25.21s\n",
      "math from 2017-05-02 until 2017-09-01 ( 3001- 4000)/15132 in 26.38s\n",
      "math from 2017-05-02 until 2017-09-01 ( 4001- 5000)/15132 in 27.67s\n",
      "math from 2017-05-02 until 2017-09-01 ( 5001- 6000)/15132 in 25.24s\n",
      "math from 2017-05-02 until 2017-09-01 ( 6001- 7000)/15132 in 24.32s\n",
      "math from 2017-05-02 until 2017-09-01 ( 7001- 8000)/15132 in 26.44s\n",
      "math from 2017-05-02 until 2017-09-01 ( 8001- 9000)/15132 in 23.71s\n",
      "math from 2017-05-02 until 2017-09-01 ( 9001-10000)/15132 in 23.91s\n",
      "math from 2017-05-02 until 2017-09-01 (10001-11000)/15132 in 26.68s\n",
      "math from 2017-05-02 until 2017-09-01 (11001-12000)/15132 in 28.11s\n",
      "math from 2017-05-02 until 2017-09-01 (12001-13000)/15132 in 25.55s\n",
      "math from 2017-05-02 until 2017-09-01 (13001-14000)/15132 in 27.47s\n",
      "math from 2017-05-02 until 2017-09-01 (14001-15000)/15132 in 24.06s\n",
      "math from 2017-05-02 until 2017-09-01 (15001-15132)/15132 in 4.51s\n",
      "math from 2017-05-02 until 2017-09-01 retrieved 15132 records in 7 min\n",
      "\n",
      "Writing to arXivMeta_math_from_2017-09-02_to_2017-12-31.csv\n",
      "\n",
      "math from 2017-09-02 until 2017-12-31 (    1- 1000)/15922 in 25.64s\n",
      "math from 2017-09-02 until 2017-12-31 ( 1001- 2000)/15922 in 25.67s\n",
      "math from 2017-09-02 until 2017-12-31 ( 2001- 3000)/15922 in 25.59s\n",
      "math from 2017-09-02 until 2017-12-31 ( 3001- 4000)/15922 in 25.50s\n",
      "math from 2017-09-02 until 2017-12-31 ( 4001- 5000)/15922 in 25.95s\n",
      "math from 2017-09-02 until 2017-12-31 ( 5001- 6000)/15922 in 25.32s\n",
      "math from 2017-09-02 until 2017-12-31 ( 6001- 7000)/15922 in 25.76s\n",
      "math from 2017-09-02 until 2017-12-31 ( 7001- 8000)/15922 in 25.67s\n",
      "math from 2017-09-02 until 2017-12-31 ( 8001- 9000)/15922 in 28.34s\n",
      "math from 2017-09-02 until 2017-12-31 ( 9001-10000)/15922 in 26.03s\n",
      "math from 2017-09-02 until 2017-12-31 (10001-11000)/15922 in 25.67s\n",
      "math from 2017-09-02 until 2017-12-31 (11001-12000)/15922 in 24.82s\n",
      "math from 2017-09-02 until 2017-12-31 (12001-13000)/15922 in 24.89s\n",
      "math from 2017-09-02 until 2017-12-31 (13001-14000)/15922 in 27.59s\n",
      "math from 2017-09-02 until 2017-12-31 (14001-15000)/15922 in 24.86s\n",
      "math from 2017-09-02 until 2017-12-31 (15001-15922)/15922 in 15.13s\n",
      "math from 2017-09-02 until 2017-12-31 retrieved 15922 records in 7 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15922"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'math'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-05-01\", category=cat)\n",
    "harvest_data(f\"{year}-05-02\", f\"{year}-09-01\", category=cat)\n",
    "harvest_data(f\"{year}-09-02\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-01-01_to_2017-05-01.csv\n",
      "\n",
      "physics from 2017-01-01 until 2017-05-01 (    1- 1000)/22316 in 33.18s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 1001- 2000)/22316 in 32.31s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 2001- 3000)/22316 in 28.52s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 3001- 4000)/22316 in 34.51s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 4001- 5000)/22316 in 28.56s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 5001- 6000)/22316 in 30.83s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 6001- 7000)/22316 in 29.97s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 7001- 8000)/22316 in 97.47s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 8001- 9000)/22316 in 105.98s\n",
      "physics from 2017-01-01 until 2017-05-01 ( 9001-10000)/22316 in 90.21s\n",
      "physics from 2017-01-01 until 2017-05-01 (10001-11000)/22316 in 67.14s\n",
      "physics from 2017-01-01 until 2017-05-01 (11001-12000)/22316 in 64.60s\n",
      "physics from 2017-01-01 until 2017-05-01 (12001-13000)/22316 in 51.89s\n",
      "physics from 2017-01-01 until 2017-05-01 (13001-14000)/22316 in 44.62s\n",
      "physics from 2017-01-01 until 2017-05-01 (14001-15000)/22316 in 57.18s\n",
      "physics from 2017-01-01 until 2017-05-01 (15001-16000)/22316 in 31.44s\n",
      "physics from 2017-01-01 until 2017-05-01 (16001-17000)/22316 in 26.28s\n",
      "physics from 2017-01-01 until 2017-05-01 (17001-18000)/22316 in 26.19s\n",
      "physics from 2017-01-01 until 2017-05-01 (18001-19000)/22316 in 25.64s\n",
      "physics from 2017-01-01 until 2017-05-01 (19001-20000)/22316 in 28.00s\n",
      "physics from 2017-01-01 until 2017-05-01 (20001-21000)/22316 in 26.94s\n",
      "physics from 2017-01-01 until 2017-05-01 (21001-22000)/22316 in 30.25s\n",
      "physics from 2017-01-01 until 2017-05-01 (22001-22316)/22316 in 8.74s\n",
      "physics from 2017-01-01 until 2017-05-01 retrieved 22316 records in 17 min\n",
      "\n",
      "Writing to arXivMeta_physics_from_2017-05-02_to_2017-09-01.csv\n",
      "\n",
      "physics from 2017-05-02 until 2017-09-01 (    1- 1000)/24389 in 29.25s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 1001- 2000)/24389 in 30.31s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 2001- 3000)/24389 in 26.61s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 3001- 4000)/24389 in 25.79s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 4001- 5000)/24389 in 27.66s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 5001- 6000)/24389 in 26.73s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 6001- 7000)/24389 in 25.92s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 7001- 8000)/24389 in 27.65s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 8001- 9000)/24389 in 28.82s\n",
      "physics from 2017-05-02 until 2017-09-01 ( 9001-10000)/24389 in 28.39s\n",
      "physics from 2017-05-02 until 2017-09-01 (10001-11000)/24389 in 28.04s\n",
      "physics from 2017-05-02 until 2017-09-01 (11001-12000)/24389 in 27.08s\n",
      "physics from 2017-05-02 until 2017-09-01 (12001-13000)/24389 in 26.00s\n",
      "physics from 2017-05-02 until 2017-09-01 (13001-14000)/24389 in 27.81s\n",
      "physics from 2017-05-02 until 2017-09-01 (14001-15000)/24389 in 32.41s\n",
      "physics from 2017-05-02 until 2017-09-01 (15001-16000)/24389 in 25.38s\n",
      "physics from 2017-05-02 until 2017-09-01 (16001-17000)/24389 in 28.20s\n",
      "physics from 2017-05-02 until 2017-09-01 (17001-18000)/24389 in 33.95s\n",
      "physics from 2017-05-02 until 2017-09-01 (18001-19000)/24389 in 30.50s\n",
      "physics from 2017-05-02 until 2017-09-01 (19001-20000)/24389 in 27.48s\n",
      "physics from 2017-05-02 until 2017-09-01 (20001-21000)/24389 in 25.29s\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 389-390: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bf8e48b2aa78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'physics'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mharvest_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{year}-01-01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{year}-05-01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mharvest_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{year}-05-02\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{year}-09-01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mharvest_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{year}-09-02\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{year}-12-31\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-fe33bfbc4e9e>\u001b[0m in \u001b[0;36mharvest_data\u001b[1;34m(isoday_0, isoday_1, category, days_in_slice, file_name, overwrite)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# try to download the slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mnewly_retrieved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mharvest_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_from\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_until\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewly_retrieved\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a556bae6c174>\u001b[0m in \u001b[0;36mharvest_slice\u001b[1;34m(date_from, date_until, category, file)\u001b[0m\n\u001b[0;32m     40\u001b[0m                                  \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                                 ]\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 389-390: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-01-01\", f\"{year}-05-01\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-05-02_to_2017-07-01.csv\n",
      "\n",
      "physics from 2017-05-02 until 2017-07-01 (    1- 1000)/10406 in 130.66s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 1001- 2000)/10406 in 183.40s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 2001- 3000)/10406 in 63.10s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 3001- 4000)/10406 in 29.57s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 4001- 5000)/10406 in 27.15s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 5001- 6000)/10406 in 28.66s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 6001- 7000)/10406 in 28.66s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 7001- 8000)/10406 in 26.82s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 8001- 9000)/10406 in 26.57s\n",
      "physics from 2017-05-02 until 2017-07-01 ( 9001-10000)/10406 in 25.91s\n",
      "physics from 2017-05-02 until 2017-07-01 (10001-10406)/10406 in 7.98s\n",
      "physics from 2017-05-02 until 2017-07-01 retrieved 10406 records in 10 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-05-02\", f\"{year}-07-01\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-07-02_to_2017-08-01.csv\n",
      "\n",
      "physics from 2017-07-02 until 2017-08-01 (    1- 1000)/ 4688 in 24.29s\n",
      "physics from 2017-07-02 until 2017-08-01 ( 1001- 2000)/ 4688 in 26.95s\n",
      "physics from 2017-07-02 until 2017-08-01 ( 2001- 3000)/ 4688 in 29.75s\n",
      "physics from 2017-07-02 until 2017-08-01 ( 3001- 4000)/ 4688 in 24.30s\n",
      "physics from 2017-07-02 until 2017-08-01 ( 4001- 4688)/ 4688 in 10.85s\n",
      "physics from 2017-07-02 until 2017-08-01 retrieved 4688 records in 2 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4688"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-07-02\", f\"{year}-08-01\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-08-02_to_2017-08-15.csv\n",
      "\n",
      "physics from 2017-08-02 until 2017-08-15 (    1- 1000)/ 2823 in 45.31s\n",
      "physics from 2017-08-02 until 2017-08-15 ( 1001- 2000)/ 2823 in 58.93s\n",
      "physics from 2017-08-02 until 2017-08-15 ( 2001- 2823)/ 2823 in 56.99s\n",
      "physics from 2017-08-02 until 2017-08-15 retrieved 2823 records in 3 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-08-02\", f\"{year}-08-15\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-08-16_to_2017-08-20.csv\n",
      "\n",
      "physics from 2017-08-16 until 2017-08-20 (    1-  662)/  662 in 75.35s\n",
      "physics from 2017-08-16 until 2017-08-20 retrieved 662 records in 1 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-08-16\", f\"{year}-08-20\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-08-21_to_2017-08-21.csv\n",
      "\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 389-390: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7e30e6213949>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2017\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'physics'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mharvest_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{year}-08-21\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{year}-08-21\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-fe33bfbc4e9e>\u001b[0m in \u001b[0;36mharvest_data\u001b[1;34m(isoday_0, isoday_1, category, days_in_slice, file_name, overwrite)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# try to download the slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mnewly_retrieved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mharvest_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_from\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_until\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewly_retrieved\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a556bae6c174>\u001b[0m in \u001b[0;36mharvest_slice\u001b[1;34m(date_from, date_until, category, file)\u001b[0m\n\u001b[0;32m     40\u001b[0m                                  \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                                 ]\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 389-390: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-08-21\", f\"{year}-08-21\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-08-22_to_2017-09-01.csv\n",
      "\n",
      "physics from 2017-08-22 until 2017-09-01 (    1- 1000)/ 5683 in 24.57s\n",
      "physics from 2017-08-22 until 2017-09-01 ( 1001- 2000)/ 5683 in 32.82s\n",
      "physics from 2017-08-22 until 2017-09-01 ( 2001- 3000)/ 5683 in 27.74s\n",
      "physics from 2017-08-22 until 2017-09-01 ( 3001- 4000)/ 5683 in 24.00s\n",
      "physics from 2017-08-22 until 2017-09-01 ( 4001- 5000)/ 5683 in 23.78s\n",
      "physics from 2017-08-22 until 2017-09-01 ( 5001- 5683)/ 5683 in 17.04s\n",
      "physics from 2017-08-22 until 2017-09-01 retrieved 5683 records in 3 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5683"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-08-22\", f\"{year}-09-01\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to arXivMeta_physics_from_2017-09-02_to_2017-12-31.csv\n",
      "\n",
      "physics from 2017-09-02 until 2017-12-31 (    1- 1000)/24150 in 33.95s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 1001- 2000)/24150 in 32.22s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 2001- 3000)/24150 in 27.45s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 3001- 4000)/24150 in 25.59s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 4001- 5000)/24150 in 30.78s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 5001- 6000)/24150 in 26.11s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 6001- 7000)/24150 in 27.98s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 7001- 8000)/24150 in 29.14s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 8001- 9000)/24150 in 28.11s\n",
      "physics from 2017-09-02 until 2017-12-31 ( 9001-10000)/24150 in 29.45s\n",
      "physics from 2017-09-02 until 2017-12-31 (10001-11000)/24150 in 28.80s\n",
      "physics from 2017-09-02 until 2017-12-31 (11001-12000)/24150 in 30.68s\n",
      "physics from 2017-09-02 until 2017-12-31 (12001-13000)/24150 in 26.99s\n",
      "physics from 2017-09-02 until 2017-12-31 (13001-14000)/24150 in 34.22s\n",
      "physics from 2017-09-02 until 2017-12-31 (14001-15000)/24150 in 27.74s\n",
      "physics from 2017-09-02 until 2017-12-31 (15001-16000)/24150 in 25.21s\n",
      "physics from 2017-09-02 until 2017-12-31 (16001-17000)/24150 in 122.42s\n",
      "physics from 2017-09-02 until 2017-12-31 (17001-18000)/24150 in 104.62s\n",
      "physics from 2017-09-02 until 2017-12-31 (18001-19000)/24150 in 96.63s\n",
      "physics from 2017-09-02 until 2017-12-31 (19001-20000)/24150 in 26.20s\n",
      "physics from 2017-09-02 until 2017-12-31 (20001-21000)/24150 in 27.67s\n",
      "physics from 2017-09-02 until 2017-12-31 (21001-22000)/24150 in 26.78s\n",
      "physics from 2017-09-02 until 2017-12-31 (22001-23000)/24150 in 27.86s\n",
      "physics from 2017-09-02 until 2017-12-31 (23001-24000)/24150 in 25.59s\n",
      "physics from 2017-09-02 until 2017-12-31 (24001-24150)/24150 in 4.22s\n",
      "physics from 2017-09-02 until 2017-12-31 retrieved 24150 records in 16 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2017\n",
    "cat = 'physics'\n",
    "harvest_data(f\"{year}-09-02\", f\"{year}-12-31\", category=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
