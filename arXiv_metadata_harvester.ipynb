{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from my_utilities import read_dict, save_dict, harvest_slice, harvest_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Metadata Harvester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "## Grab records from the requested timespan, from all or from one selected category\n",
    "\n",
    "## Write to tab-delimited local csv:\n",
    "## columns: *id, categories, title, abstract*\n",
    "(Dealing with funny characters in the names of authors was beyond me. One could also get a date associated with each record but it's supposed not to necessarily correspond to the date of posting by the authors.)\n",
    "### There are two functions (the code is in *my_utilities.py*).\n",
    "Both will talk to You using prints.\n",
    "* *harvest_slice* needs You to explicitly choose the category (possibly 'all') and the filename as arguments\n",
    "    * just appends lines to the file, it's up to You not to make a mess\n",
    "\n",
    "\n",
    "* *harvest_data* divides the timespan into slices of given length and harvests those using *harvest_slice*:\n",
    "    * can make up the name of the file on its own\n",
    "    * adds the header to the csv\n",
    "    * default behavior when the file already exists is to quit\n",
    "    * default category is 'all'\n",
    "\n",
    "### It is slow.    \n",
    "### Examples:\n",
    "*  ~ 1 min,  2 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")\n",
    "*  ~ 5 min, 11 MB >>> harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")\n",
    "* ~ 10 min, 16 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)\n",
    "* ~ 1 h, 68 MB >>> harvest_data(\"2018-08-01\", \"2018-11-01\")\n",
    "\n",
    "### Example of a basic query used in the code:\n",
    "* http://export.arxiv.org/oai2?verb=ListRecords&from=2012-01-01&until=2018-02-01&set=physics:hep-th&metadataPrefix=arXiv\n",
    "* \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "\n",
    "See https://arxiv.org/help/bulk_data for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside from having authors, a title and an abstract (a summary), articles on *ArXiv* are typically assigned to a category, e.g. Computer Science, Economics, etc. Those informations form the meta-data of an article that is eaily obtainable with an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One can talk with *ArXiv* using two different interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first one serves to answer typical complicated search queries.\n",
    "For example looking for articles by Stephen Hawking about black holes we could start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"ti:black%20hole+AND+au:Hawking\"\n",
    "\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entries = soup.find_all('entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a typical data we get is the following. Notice that there is both the *primary category* and a general *category* list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/hep-th/0507171v2\n",
      "S. W. Hawking\n",
      "Information Loss in Black Holes\n",
      "primary category: hep-th\n",
      "all categories: ['hep-th']\n",
      "abstract:   The question of whether information is lost in black holes is investigated\n",
      "using Euclidean path integrals. The formation and evaporation of black holes is\n",
      "regarded as a scattering problem with all m ...\n"
     ]
    }
   ],
   "source": [
    "entry = entries[0]\n",
    "\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:200]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another example we see that there can be more categories: e.g. one from Economics (*econ.EM*) and an another one from Statistics (*stat.AP*), and that the first one in the list is the primary category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/1803.11233v1\n",
      "Kamil Jodź\n",
      "Mortality in a heterogeneous population - Lee-Carter's methodology\n",
      "primary category: econ.EM\n",
      "all categories: ['econ.EM', 'stat.AP']\n",
      "abstract:   The EU Solvency II directive recommends insurance companie ...\n"
     ]
    }
   ],
   "source": [
    "search_query = \"1803.11233\"\n",
    "# search_query = \"0707.3787\"\n",
    "query = \"http://export.arxiv.org/api/query?search_query=\" + search_query\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "entry = soup.find('entry')\n",
    "print(entry.id.string)\n",
    "print(entry.author.find('name').string)\n",
    "print(entry.title.string)\n",
    "print('primary category:', entry.find(\"arxiv:primary_category\")['term'])\n",
    "print('all categories:', [cat['term'] for cat in entry.find_all(\"category\")])\n",
    "print('abstract:', entry.summary.string[:60]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But this first API is not suited for bulk data downloads. Instead, we want to use the interface specified by Open Archives Initiative (OAI) that ArXiv complies with.\n",
    "This time we build the query by specifying the time slice from which we want the articles. We can also filter for only one category, if we want. Take the following query for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from = \"2018-04-02\"\n",
    "date_until = \"2018-04-02\"\n",
    "category = \"econ\" # Economics\n",
    "\n",
    "search_query = f\"&from={date_from}&until={date_until}&set={category}\"\n",
    "query = \"http://export.arxiv.org/oai2?verb=ListRecords\" + search_query + \"&metadataPrefix=arXiv\"\n",
    "sauce = urllib.request.urlopen(query).read()    \n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "records = soup.find_all('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<record>\n",
       "<header>\n",
       "<identifier>oai:arXiv.org:1803.11233</identifier>\n",
       "<datestamp>2018-04-02</datestamp>\n",
       "<setspec>econ</setspec>\n",
       "</header>\n",
       "<metadata>\n",
       "<arxiv xmlns=\"http://arxiv.org/OAI/arXiv/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemalocation=\"http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd\">\n",
       "<id>1803.11233</id><created>2018-03-29</created><authors><author><keyname>Jodź</keyname><forenames>Kamil</forenames></author></authors><title>Mortality in a heterogeneous population - Lee-Carter's methodology</title><categories>econ.EM stat.AP</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EU Solvency II directive recommends insurance companies to pay more\n",
       "attention to the risk management methods. The sense of risk management is the\n",
       "ability to quantify risk and apply methods that reduce uncertainty. In life\n",
       "insurance, the risk is a consequence of the random variable describing the life\n",
       "expectancy. The article will present a proposal for stochastic mortality\n",
       "modeling based on the Lee and Carter methodology. The maximum likelihood method\n",
       "is often used to estimate parameters in mortality models. This method assumes\n",
       "that the population is homogeneous and the number of deaths has the Poisson\n",
       "distribution. The aim of this article is to change assumptions about the\n",
       "distribution of the number of deaths. The results indicate that the model can\n",
       "get a better match to historical data, when the number of deaths has a negative\n",
       "binomial distribution.\n",
       "</abstract></arxiv>\n",
       "</metadata>\n",
       "</record>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time there is only the single ***categories*** tag. **We will be assuming that there is a convention that the first item on that list is the primary category of an article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803.11233\n",
      "econ.EM stat.AP\n"
     ]
    }
   ],
   "source": [
    "print(records[0].id.string)\n",
    "print(records[0].categories.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the *set* field in the last query. We can retrieve the list of all possible *sets* using another fixed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 'Computer Science',\n",
       " 'econ': 'Economics',\n",
       " 'eess': 'Electrical Engineering and Systems Science',\n",
       " 'math': 'Mathematics',\n",
       " 'physics': 'Physics',\n",
       " 'physics:astro-ph': 'Astrophysics',\n",
       " 'physics:cond-mat': 'Condensed Matter',\n",
       " 'physics:gr-qc': 'General Relativity and Quantum Cosmology',\n",
       " 'physics:hep-ex': 'High Energy Physics - Experiment',\n",
       " 'physics:hep-lat': 'High Energy Physics - Lattice',\n",
       " 'physics:hep-ph': 'High Energy Physics - Phenomenology',\n",
       " 'physics:hep-th': 'High Energy Physics - Theory',\n",
       " 'physics:math-ph': 'Mathematical Physics',\n",
       " 'physics:nlin': 'Nonlinear Sciences',\n",
       " 'physics:nucl-ex': 'Nuclear Experiment',\n",
       " 'physics:nucl-th': 'Nuclear Theory',\n",
       " 'physics:physics': 'Physics (Other)',\n",
       " 'physics:quant-ph': 'Quantum Physics',\n",
       " 'q-bio': 'Quantitative Biology',\n",
       " 'q-fin': 'Quantitative Finance',\n",
       " 'stat': 'Statistics'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query retrieves xml about the accesible 'sets', e.g.\n",
    "# <set>\n",
    "# <setspec>cs</setspec>\n",
    "# <setname>Computer Science</setname>\n",
    "# </set>\n",
    "\n",
    "if not Path(\"categories.txt\").is_file() :\n",
    "    \n",
    "    xml_query = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "    sauce = urllib.request.urlopen(xml_query).read()\n",
    "    soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "    sets = soup.find_all(\"set\")\n",
    "\n",
    "    categories = {}\n",
    "\n",
    "    for set_ in sets:\n",
    "        categories[set_.setspec.string] = set_.setname.string\n",
    "\n",
    "    save_dict(categories, \"categories.txt\")\n",
    "            \n",
    "\n",
    "categories = {}\n",
    "read_dict(categories, \"categories.txt\")\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently physics enthusiasts get more options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The matter of actual article categories is more messy, see https://arxiv.org/ and https://arxiv.org/help/prep#subj\n",
    "Physics gets an additional level of gradation: e.g. *physics:astro-ph* is a subset of *physics*. And the categorization chosen by an author her- or himself is finer and may be multiple, e.g. *cs.ai* (Computer Science: Artificial Intelligence) instead of just *cs*, together with *physics:astro-ph.GA* (Physics: Astrophysics: Astrophysics of Galaxies) instead of just *physics:astro-ph* (assuming that the article was both about Artificial Intelligence and Galaxies). But, again, first of the categories is the primary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dictionaries. One with the top-level categories, and the second with physics genres.\n",
    "\n",
    "pattern = re.compile('physics:(.+)')\n",
    "\n",
    "physics_genres = {}\n",
    "top_categories = {}\n",
    "\n",
    "for category, description in categories.items():\n",
    "    match = pattern.match(category)\n",
    "    if match:\n",
    "        physics_genres[match.group(1)] = description\n",
    "    else:\n",
    "        top_categories[category] = description\n",
    "        \n",
    "save_dict(physics_genres, \"physics_genres.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, *Economics* and *Electrical Engineering* start only in 2017 (I've checked), so we will exclude *econ* and *eess* from *top_categories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats = {cat: cat_name for (cat, cat_name) in top_categories.items() if cat not in  ['econ' ,'eess']}\n",
    "save_dict(top_cats, \"top_cats.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cs': 'Computer Science',\n",
       "  'math': 'Mathematics',\n",
       "  'physics': 'Physics',\n",
       "  'q-bio': 'Quantitative Biology',\n",
       "  'q-fin': 'Quantitative Finance',\n",
       "  'stat': 'Statistics'},\n",
       " {'astro-ph': 'Astrophysics',\n",
       "  'cond-mat': 'Condensed Matter',\n",
       "  'gr-qc': 'General Relativity and Quantum Cosmology',\n",
       "  'hep-ex': 'High Energy Physics - Experiment',\n",
       "  'hep-lat': 'High Energy Physics - Lattice',\n",
       "  'hep-ph': 'High Energy Physics - Phenomenology',\n",
       "  'hep-th': 'High Energy Physics - Theory',\n",
       "  'math-ph': 'Mathematical Physics',\n",
       "  'nlin': 'Nonlinear Sciences',\n",
       "  'nucl-ex': 'Nuclear Experiment',\n",
       "  'nucl-th': 'Nuclear Theory',\n",
       "  'physics': 'Physics (Other)',\n",
       "  'quant-ph': 'Quantum Physics'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cats, physics_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The API will serve us 1000 records each 10 seconds (plus a considerable overhead for communication)\n",
    "The imported *harvest_slice* function, given a time-slice, category and a file-path, works in a loop and\n",
    "    * sends the query\n",
    "    * saves the received records into a file\n",
    "    * using the last *resumption token* (appended to the xml) and the given dates forms a next query\n",
    "    * finally returns the number of retrieved records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_slice(date_from, date_until, category, file) -> int:\n",
    "    # returns number of downloaded records if succesful\n",
    "    \n",
    "    base_query = \"http://export.arxiv.org/oai2?verb=ListRecords\"\n",
    "    \n",
    "    if category == \"all\":\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&metadataPrefix=arXiv\"\n",
    "    else:\n",
    "        query = base_query + f\"&from={date_from}&until={date_until}&set={category}&metadataPrefix=arXiv\"\n",
    "    \n",
    "    retrieved = 0\n",
    "    \n",
    "    while query:\n",
    "        \n",
    "        time_0 = time.time()\n",
    "        \n",
    "        # try to download\n",
    "        try:            \n",
    "            sauce = urllib.request.urlopen(query).read()\n",
    "\n",
    "        except:\n",
    "            print(f\":( Failed requesting {query}\\nMoving on\")\n",
    "            break\n",
    "        \n",
    "        # parse the xml looking for <record>'s\n",
    "        soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "        records = soup.find_all('record')\n",
    "\n",
    "        retrieved = retrieved + len(records)\n",
    "\n",
    "        with open(file, \"a\", encoding='utf-8') as dump:\n",
    "\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            for record in records:                \n",
    "                record_string = [record.id.string,\n",
    "                                 [(author.forenames.string+\" \" if author.forenames else \"\") + author.keyname.string for author in record.find_all('author')],\n",
    "                                 record.title.string,\n",
    "                                 record.abstract.string,\n",
    "                                 record.categories.string\n",
    "                                ]\n",
    "                writer.writerow(record_string)\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"\".join([category,\" from \", f\"{date_from}\",\" until \", f\"{date_until}\",\" empty\"]))\n",
    "            break\n",
    "        \n",
    "        # info at the end of 'soup' about where to resume if the data stream was cut at 1000 records\n",
    "        # None if the stream wasn't cut\n",
    "        res_token = soup.find(\"resumptiontoken\")\n",
    "        \n",
    "        if res_token:\n",
    "\n",
    "            # data in the current loop started at this record in the 'query'\n",
    "            started_at = int(res_token['cursor']) + 1\n",
    "            \n",
    "            # total number of records in the 'query', should be the same in each loop\n",
    "            all_to_retrieve = int(res_token['completelistsize'])\n",
    "            \n",
    "            if res_token.string:\n",
    "                # the identifier that allows to resume the query\n",
    "                # None if the slice was completed\n",
    "\n",
    "                query = base_query + f\"&resumptionToken={res_token.string}\"\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                query = None\n",
    "            \n",
    "        else:\n",
    "            started_at = 1 \n",
    "            all_to_retrieve = len(records)\n",
    "            query = None\n",
    "        \n",
    "        time_1 = time.time()\n",
    "        \n",
    "        print(\"\".join([category,\n",
    "                       \" from \", f\"{date_from}\", \" until \", f\"{date_until}\",\n",
    "                       f\" ({started_at:>5}-{started_at+len(records)-1:>5})/{all_to_retrieve:>5}\",\n",
    "                      \" in \", f\"{(time_1 - time_0):3.2f}\", \"s\"]) )\n",
    "    \n",
    "    # end of while loop\n",
    "     \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math from 2018-10-01 until 2018-10-10 (    1- 1000)/ 2332 in 116.37s\n",
      "math from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 2332 in 46.22s\n",
      "math from 2018-10-01 until 2018-10-10 ( 2001- 2332)/ 2332 in 223.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2332"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"math\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to download different categories separately like that, the records that belong to more than one category would be repeated in each file. But we can be downloading all categories at the same time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all from 2018-10-01 until 2018-10-10 (    1- 1000)/ 7663 in 32.15s\n",
      "all from 2018-10-01 until 2018-10-10 ( 1001- 2000)/ 7663 in 25.72s\n",
      "all from 2018-10-01 until 2018-10-10 ( 2001- 3000)/ 7663 in 26.29s\n",
      "all from 2018-10-01 until 2018-10-10 ( 3001- 4000)/ 7663 in 25.64s\n",
      "all from 2018-10-01 until 2018-10-10 ( 4001- 5000)/ 7663 in 24.63s\n",
      "all from 2018-10-01 until 2018-10-10 ( 5001- 6000)/ 7663 in 24.01s\n",
      "all from 2018-10-01 until 2018-10-10 ( 6001- 7000)/ 7663 in 25.05s\n",
      "all from 2018-10-01 until 2018-10-10 ( 7001- 7663)/ 7663 in 10.29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7663"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_slice(\"2018-10-01\", \"2018-10-10\", \"all\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just as a precaution, lest us split longer time-slices into multiple shorter ones in case there is an upper limit for the total number of records we can retrieve with one query.\n",
    "We divide the time-slice into 92-days long (by default) periods, and write to an automatically named file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrapper around harvest_slice\n",
    "# * handles file-names\n",
    "# * slices the time period of papers into intervals of given number of days (days_in_slice)\n",
    "# (to avoid maxing-out the response from server)\n",
    "\n",
    "def harvest_data(isoday_0, isoday_1, category='all', days_in_slice = 92, file_name=None, overwrite=False) -> int:\n",
    "\n",
    "    date_0 = dateutil.parser.parse(isoday_0).date()\n",
    "    date_1 = dateutil.parser.parse(isoday_1).date()\n",
    "\n",
    "    if not file_name:\n",
    "        # create a file with an overly descriptive name\n",
    "        file = f\"arXivMeta_{category.replace(':','--')}_from_{date_0}_to_{date_1}.csv\"\n",
    "    else:\n",
    "        file = file_name\n",
    "    \n",
    "    # check if file already exists\n",
    "    if Path(file).is_file():\n",
    "        if overwrite :\n",
    "\n",
    "            # try to backup the old file\n",
    "            file_info = re.match(r\"(\\w.+)\\.(\\w\\w+)\", file)\n",
    "            if file_info:\n",
    "                new_file = \"\".join([ file_info.group(1), \"_bak.\", file_info.group(2) ])\n",
    "                if not Path(new_file).is_file():\n",
    "                    os.rename(file, new_file)\n",
    "                    print(f\"Old file backed up as {new_file}\")\n",
    "\n",
    "            # clear the file\n",
    "            print(f\"Overwriting {file}\")\n",
    "            with open(file, \"w\") as dump:\n",
    "                dump.truncate(0)\n",
    "            \n",
    "        else:\n",
    "            print(f\"The file {file} already exists\")\n",
    "            return -1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Writing to {file}\")\n",
    "    \n",
    "    with open(file, \"a\") as dump:\n",
    "            writer = csv.writer(dump, delimiter='\\t')\n",
    "            header = ['id', 'prim_cat', 'sec_cats', 'title', 'abstract']\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    # Start the clock\n",
    "    time_0 = time.time()\n",
    "    \n",
    "    # Let's count all downloaded records\n",
    "    retrieved = 0\n",
    "    \n",
    "    # We'll go from 'date_0' until 'date_1' in slices of 'days_in_slice' days\n",
    "    # The server's response presumably maxes out at some number of records,\n",
    "    # so we hope to have slices with less records than that.\n",
    "\n",
    "    date_from = date_0\n",
    "\n",
    "    while date_from <= date_1:\n",
    "        \n",
    "        date_until = min(date_1, date_from + datetime.timedelta(days_in_slice-1))\n",
    "\n",
    "        # try to download the slice\n",
    "        newly_retrieved = harvest_slice(date_from, date_until, category, file)\n",
    "        retrieved = retrieved + newly_retrieved\n",
    "\n",
    "        # move on to the next slice\n",
    "        date_from = date_until + datetime.timedelta(days=1)\n",
    "        \n",
    "        # time-out\n",
    "        time.sleep(10)\n",
    "\n",
    "    time_1 = time.time()\n",
    "    \n",
    "    print(\"\".join([category,\n",
    "                   \" from \", str(date_0), \" until \", str(date_1),\n",
    "                   \" retrieved \", str(retrieved), \" records\"\n",
    "                   ,\" in \", f\"{(time_1 - time_0)/60:.0f}\", \" min\\n\"])\n",
    "         )\n",
    "    \n",
    "    return retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old file backed up as test_bak.csv\n",
      "Overwriting test.csv\n",
      "\n",
      "math from 2018-08-01 until 2018-10-31 (    1- 1000)/18269 in 115.80s\n",
      "math from 2018-08-01 until 2018-10-31 ( 1001- 2000)/18269 in 117.18s\n",
      "math from 2018-08-01 until 2018-10-31 ( 2001- 3000)/18269 in 128.13s\n",
      "math from 2018-08-01 until 2018-10-31 ( 3001- 4000)/18269 in 79.81s\n",
      "math from 2018-08-01 until 2018-10-31 ( 4001- 5000)/18269 in 30.15s\n",
      "math from 2018-08-01 until 2018-10-31 ( 5001- 6000)/18269 in 31.29s\n",
      "math from 2018-08-01 until 2018-10-31 ( 6001- 7000)/18269 in 28.71s\n",
      "math from 2018-08-01 until 2018-10-31 ( 7001- 8000)/18269 in 24.71s\n",
      "math from 2018-08-01 until 2018-10-31 ( 8001- 9000)/18269 in 23.62s\n",
      "math from 2018-08-01 until 2018-10-31 ( 9001-10000)/18269 in 31.68s\n",
      "math from 2018-08-01 until 2018-10-31 (10001-11000)/18269 in 23.71s\n",
      "math from 2018-08-01 until 2018-10-31 (11001-12000)/18269 in 115.31s\n",
      "math from 2018-08-01 until 2018-10-31 (12001-13000)/18269 in 68.22s\n",
      "math from 2018-08-01 until 2018-10-31 (13001-14000)/18269 in 38.45s\n",
      "math from 2018-08-01 until 2018-10-31 (14001-15000)/18269 in 108.78s\n",
      "math from 2018-08-01 until 2018-10-31 (15001-16000)/18269 in 295.12s\n",
      "math from 2018-08-01 until 2018-10-31 (16001-17000)/18269 in 44.38s\n",
      "math from 2018-08-01 until 2018-10-31 (17001-18000)/18269 in 24.13s\n",
      "math from 2018-08-01 until 2018-10-31 (18001-18269)/18269 in 5.59s\n",
      "math from 2018-11-01 until 2018-11-01 (    1-  263)/  263 in 5.35s\n",
      "math from 2018-08-01 until 2018-11-01 retrieved 18532 records in 23 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18532"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_data(\"2018-08-01\", \"2018-11-01\", category=\"math\", file_name = \"test.csv\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_data(f\"2011-01-01\", f\"2011-06-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file arXivMeta_all_from_2012-01-01_to_2012-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2013-01-01_to_2013-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2014-01-01_to_2014-12-31.csv already exists\n",
      "\n",
      "The file arXivMeta_all_from_2015-01-01_to_2015-12-31.csv already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for year in ['2011','2012','2013','2014','2015','2016','2017']:\n",
    "    harvest_data(f\"{year}-01-01\", f\"{year}-07-01\")\n",
    "    harvest_data(f\"{year}-07-02\", f\"{year}-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
