{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join('glove.6B', 'glove.6B.100d.txt'), encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the texts with masked LaTeX (see arXiv_shallow.ipynb)\n",
    "\n",
    "with open(os.path.join(\"data\", \"notex_all.csv\")) as file:\n",
    "    data = pd.read_csv(file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 500_000\n",
    "data_train = data[:n_train]\n",
    "text_train = data_train.text\n",
    "label_train = data_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293075"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89582"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(embedding_matrix, axis=1) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embedding_matrix, open(\"GloVe_my_weights.p\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293075, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = int(1.05 * np.max(np.vectorize(len)(sequences_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dense\n",
    "from keras import Model\n",
    "\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_e = LabelEncoder()\n",
    "num_label = label_e.fit_transform(label_train)\n",
    "y_train = to_categorical(num_label)\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   13,     1,  2082,    38,   148,   201,     3,   999,     5,\n",
       "         266,   201,   208,     7,   247,     1,   148,   201,   999,\n",
       "        1306,     5,   524,   266,   201,   208,  2136,    13,  4052,\n",
       "           2,  2087,   291,  5057,  3217,   161,     7,    83,    11,\n",
       "           5,    57,     6,  1814,   578,    81,     2,     1,  5057,\n",
       "        4052,    45,    17,   779,  3764,   320,   148,   201,  3997,\n",
       "           3,     1,   665,     2,   712,   618,    27,   968,     2,\n",
       "         148,   201,     3,   268,   999,  1019,    33,     4,   948,\n",
       "           2,   127,   812,   356,   999,   265,   502,   640,     3,\n",
       "         268,  2961,     3,  2496,   409,  1991,   999,     3,    42,\n",
       "        5312,     2,   999,    11,    29,     5,   284,  5138,     2,\n",
       "           1,   418,   148,   201,   176,    13, 12584,   602,     7,\n",
       "          30,    11,    45,   999,   582,   284,  5138,     8,  1017,\n",
       "        3025,     9,     4,   127,   148,   201,   621,   943,   216,\n",
       "        3312,    36,     2,    10,   789,    16,   252,     8,   508,\n",
       "          11,     1,    10,   252,  1591,  2331,    19,     1, 14303,\n",
       "        4984,  3864,   382,     7,    30,    11,    26,     8,   299,\n",
       "          12,   462,  1222,   583,     2,    10,     5,   441,  1481,\n",
       "         529,     7,    30,    11,   208,    12,   127,   621,     2,\n",
       "          10,  1814,     1,   418,   115,  3864,   382,    45,   208,\n",
       "          32,  1814,     1,   546,   148,   201,  1726,     2,     1,\n",
       "        2087,   291,  3217,   161,     3,  1662,     4,    72,     2,\n",
       "         105,  1222,   355,  5630,     7,    83,    11,   208,    11,\n",
       "           4,  3168,  1752,    18,  5986,   435,     2,   148,   201,\n",
       "        1239,     2,  2835,    26,   265,   268,   999,  5138,  3461,\n",
       "           6,  1018,  1504, 13532,  2749,     3,   479,    52,  1814,\n",
       "         190,    16,  5840,     1,  1273,     2,   706,     1,   229,\n",
       "        2082,    38,   148,   201,     3,   999,     3,     1,   843,\n",
       "         294,    11,  2397,     6,     1,   999,  6660,   201,     2,\n",
       "         161,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      "450000/450000 [==============================] - 2051s 5ms/step - loss: 0.3268 - acc: 0.8882 - val_loss: 0.2563 - val_acc: 0.9125\n",
      "Epoch 2/2\n",
      "267904/450000 [================>.............] - ETA: 13:06 - loss: 0.2535 - acc: 0.9135"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1,\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
