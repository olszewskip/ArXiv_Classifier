{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "### Create word embedding using gensim\n",
    "* The embedding is trained using the subset of first 500k records (regarded as train-data throughout).\n",
    "* The weights are saved to a file as a raw matrix\n",
    "* The whole of data (including the part unseen by embedding) is processed and split into X, Y files\n",
    "    * X contains what was originally texts (lists of words) now zero-padded sequences of integers that correspond to row indeces of the embedding.\n",
    "    * Y contains a sequence of integer-encoded labels (0:6).\n",
    "* The zeroth row of the embedding is filled with zeros, and unknown words are encoded with 0.\n",
    "* The length of the zero-padded sequences is set, based on the longest text seen in train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "\n",
    "n_train = 500_000\n",
    "DATA_FILE = os.path.join(\"data\", \"notex_all.csv\")\n",
    "WORD_PATTERN = re.compile('\\w[\\w\\'`]+')\n",
    "EMBEDD_FILE = os.path.join(\"gensim\", \"embedd_weights.npy\")\n",
    "EMBEDD_DIM = 300\n",
    "X_FILE = os.path.join(\"gensim\", \"embedded_X.txt\")\n",
    "X_FILE_BIN =  os.path.join(\"gensim\", \"embedded_X.npy\")\n",
    "Y_FILE = os.path.join(\"gensim\", \"encoded_Y.txt\")\n",
    "Y_FILE_BIN =  os.path.join(\"gensim\", \"encoded_Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A python iterator for gensim's Word2Vec class\n",
    "# It spits out elements that are lists of words\n",
    "# Each list corresponds to one arXiv article\n",
    "# e.g. next(texts_iter) = ['hello', 'world']\n",
    "\n",
    "class texts_iter():\n",
    "    \n",
    "    def __init__(self, filename, nrows=float('inf'), skiprows=1,\\\n",
    "                 prepend_label=True):\n",
    "        \n",
    "        self.idx = 0\n",
    "        self.nrows = nrows\n",
    "        self.records = open(filename, \"r\")\n",
    "        self.maxlen = 0\n",
    "        self.prepend_label = prepend_label\n",
    "        \n",
    "        #skip rows, default = 1-line header\n",
    "        for _ in range(skiprows):\n",
    "            next(self.records)\n",
    "    \n",
    "    def finish(self):\n",
    "        self.records.close()\n",
    "\n",
    "\n",
    "    # the defining method, returns a list\n",
    "    def __next__(self):\n",
    "        \n",
    "        # don't read beyond 'nrows'\n",
    "        if self.idx < self.nrows:  \n",
    "            self.idx += 1\n",
    "            \n",
    "            try:\n",
    "                record, label = next(self.records).split('\\t')\n",
    "            except:\n",
    "                self.finish()\n",
    "                raise StopIteration()\n",
    "            \n",
    "                \n",
    "            text = record.lower()\n",
    "            words = WORD_PATTERN.findall(text)\n",
    "            self.maxlen = max(len(words), self.maxlen)\n",
    "            \n",
    "            if self.prepend_label:\n",
    "                return [label.strip()] + words\n",
    "            else:\n",
    "                return words\n",
    "        \n",
    "        else:\n",
    "            self.finish()\n",
    "            raise StopIteration()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest wordcount in the train data-set: 647.\n"
     ]
    }
   ],
   "source": [
    "# The main step: apply Word2Vec\n",
    "\n",
    "texts_train = texts_iter(DATA_FILE, nrows=n_train, prepend_label=False)\n",
    "word2vec = Word2Vec(texts_train,\\\n",
    "                    size=EMBEDD_DIM,\\\n",
    "                    min_count=1, sorted_vocab=1).wv\n",
    "\n",
    "print(f\"Highest wordcount in the train data-set: {texts_train.maxlen}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the dimension of texts\n",
    "\n",
    "PADDED_LEN = int(1.05 * 647)\n",
    "PADDED_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The gensim's object may be saved to file\n",
    "#\n",
    "# word2vec.save(GENSIM_WORD_VEC_FILE)\n",
    "# word2vec = KeyedVectors.load(WORD_VEC_TRAIN_FILE, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299305, 300)\n"
     ]
    }
   ],
   "source": [
    "# The resulting embedding and the mapping from words to indeces\n",
    "# are contained in this word2vec object\n",
    "# Note that one needs the map along with the embedding to properly\n",
    "# tokenize the texts\n",
    "\n",
    "n, m = word2vec.vectors.shape\n",
    "embedding = np.zeros((n+1, m))\n",
    "embedding[1:] = np.asarray(word2vec.vectors)\n",
    "print(embedding.shape)\n",
    "\n",
    "def word_to_idx(word):\n",
    "    try:\n",
    "        return word2vec.vocab[word].index + 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx('the'), word_to_idx('Å›wierszcz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding weights to file\n",
    "np.save(EMBEDD_FILE, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phys': 0, 'math': 1, 'q-bio': 2, 'stat': 3, 'q-fin': 4, 'cs': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new files:\n",
    "# one with the texts converted to sequences of integer tokens (0-padded)\n",
    "# and the second with integer-encoded labels,\n",
    "# both using the texts_iter class\n",
    "# Note that the tokenization is applied to the whole of data\n",
    "# including test-data (so don't look inside)\n",
    "\n",
    "label_encoding = {}\n",
    "\n",
    "with open(X_FILE, 'w') as x_file:\n",
    "    with open(Y_FILE, 'w') as y_file:\n",
    "\n",
    "        # generate word-lists using the same iterator as for\n",
    "        # creating the embedding\n",
    "        records = texts_iter(DATA_FILE, prepend_label=True)\n",
    "\n",
    "        for list_ in records:\n",
    "            \n",
    "            label, *text = list_\n",
    "            \n",
    "            if label in label_encoding:\n",
    "                y = label_encoding[label.strip()]\n",
    "            else:\n",
    "                y = len(label_encoding)\n",
    "                label_encoding[label] = y\n",
    "                \n",
    "            y_file.write(str(y) + '\\n')\n",
    "            \n",
    "            sequence = [str(word_to_idx(word)) for word in text]\n",
    "            \n",
    "            # add post-padding\n",
    "            sequence += [str(0)] * (PADDED_LEN - len(sequence))\n",
    "            sequence = sequence[:PADDED_LEN]\n",
    "            \n",
    "            x_file.write(\" \".join(sequence) + '\\n')\n",
    "            \n",
    "label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(X_FILE)\n",
    "y = np.loadtxt(Y_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(X_FILE_BIN, X)\n",
    "np.save(Y_FILE_BIN, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one day it may be fun to randomly sprinkle zeros\n",
    "# (as if missing words) to our train data\n",
    "\n",
    "# # suboptimal function to randomly insert zeros into a list\n",
    "# # the number of zeros is proportional to the length of the list\n",
    "\n",
    "# def sprinkle_zeros(list_, frac):\n",
    "#     k = int(frac * len(list_))\n",
    "#     for _ in range(k):\n",
    "#         list_.insert(randint(0,len(list_)), 0)\n",
    "\n",
    "# li = list(range(1,20))\n",
    "# sprinkle_zeros(li, 0.1)\n",
    "# li"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
