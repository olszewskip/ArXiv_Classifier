{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from itertools import cycle, product\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(123)\n",
    "\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import color_palette \n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I'm fitting the keras' embedding layer for a few epochs within the possibly simplest imaginable setup, namely with global averaging and softmax on top. This examinations in meant to be fairly inexpensive but broad, and serve to produce reasonable starting embeddings for more complicated net architectures and to compare various loss-functions/batch-sizes/optimizers. The choice of loss function is a somewhat non-trivial matter due to the class imbalance of our dataset and the stated objective of maximazing the macroF1 score.\n",
    "\n",
    "* Results land in the directory defined in the *working_dir* variable below.\n",
    "* Weights are stores as *blabla_weights.p* and they may be directly depickled into the *weights* argument of the Embedding layer (see the *Embedding* within *init* of the *BlackBox* class). \n",
    "* There are some nice plots below.\n",
    "\n",
    "We manage to establish an interesting benchmark for our dataset: the simple setup described above reproduces the performance of shallow classifiers, *macroF1*=80% on test-data. Thus, at least neural nets are no-worse than SVMs and LogisticRegression. For other net architectures to carry their weight, they'll have to go beyond that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the directory where to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘keras_GlobalAvg_GridSearch_results’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir keras_GlobalAvg_GridSearch_results\n",
    "working_dir = 'keras_GlobalAvg_GridSearch_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the test- and validation-data\n",
    "The test data is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample=30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(\"data\", \"Kdata\", \"X_train.npy\")) #[-n_sample:]\n",
    "y_train = np.load(os.path.join(\"data\", \"Kdata\", \"y_train.npy\")) #[-n_sample:]\n",
    "X_val = np.load(os.path.join(\"data\", \"Kdata\", \"X_val.npy\")) #[-n_sample:]\n",
    "y_val = np.load(os.path.join(\"data\", \"Kdata\", \"y_val.npy\")) #[-n_sample:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in global parameters describing the data prepared in *keras_preprocessing.ipynb*\n",
    "* dimensions needed for the word embedding\n",
    "* number of classes\n",
    "* class-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unique_words': 277303,\n",
       " 'num_words': 277304,\n",
       " 'padded_length': 679,\n",
       " 'n_classes': 6,\n",
       " 'class_weights': array([ 1.26825655,  0.72736371,  0.27602776, 13.23801959, 30.29201502,\n",
       "         9.49559404])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_params = pickle.load(open(\"global_params.p\", \"rb\"))\n",
    "unique_words = global_params['unique_words']\n",
    "num_words = global_params['num_words']\n",
    "padded_length = global_params['padded_length']\n",
    "n_classes = global_params['n_classes']\n",
    "class_weights = global_params['class_weights']\n",
    "\n",
    "global_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in custom loss functions and metrics\n",
    "* cat.-accuracy\n",
    "* macro-precision, macro-f1, macro-recall\n",
    "* cat.-crossentropy\n",
    "* a custom loss function, my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.cat_acc(y_true, y_pred)>,\n",
       " <function __main__.macroPrec(y_true, y_pred)>,\n",
       " <function __main__.macroF1(y_true, y_pred)>,\n",
       " <function __main__.macroRecall(y_true, y_pred)>,\n",
       " <function __main__.cat_cross(y_true, y_pred)>,\n",
       " <function __main__.fuzzy_macroF1_flip(y_true, y_pred)>,\n",
       " <function __main__.my_cross(y_true, y_pred)>,\n",
       " <function __main__.my_loss(y_true, y_pred)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run keras_custom_functions.ipynb\n",
    "\n",
    "my_metrics = list(CUSTOM_OBJECTS.values())\n",
    "my_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in wrappers for keras' sequential-model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run keras_plot_history.ipynb\n",
    "%run keras_blackbox_wrapper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-grid-search\n",
    "* Define the sequential setup by specifiying the layers, and parameter scopes to search through.\n",
    "* Train for a fixed number of epochs.\n",
    "* Examine how different parameter combinations influence quality of the classification (measured by the resulting model's macro-F1 on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Dropout(0.5),\n",
    "          GlobalAveragePooling1D()\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 options in the cross-search, e.g. ('categorical_crossentropy', 100, 'adam')\n"
     ]
    }
   ],
   "source": [
    "# the options\n",
    "\n",
    "losses = ['categorical_crossentropy', my_loss]\n",
    "batch_sizes = [100, 200, 500]\n",
    "optimizers = ['adam', 'nadam']\n",
    "\n",
    "options = list(product(losses, batch_sizes, optimizers))\n",
    "n_options = len(options)\n",
    "print(f\"{n_options} options in the cross-search, e.g. {options[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/12\n",
      "loss=categorical_crossentropy, batch_size=100, optimizer=adam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1931s 4ms/step - loss: 0.3370\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 2102s 4ms/step - loss: 0.1864\n",
      "\n",
      "2/12\n",
      "loss=categorical_crossentropy, batch_size=100, optimizer=nadam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 2234s 4ms/step - loss: 0.2700\n",
      "Epoch 2/2\n",
      "262900/500000 [==============>...............] - ETA: 16:23 - loss: 0.1691"
     ]
    }
   ],
   "source": [
    "# fit and evaluate on the validation data, loop through the options\n",
    "\n",
    "epochs = 2\n",
    "results = []\n",
    "\n",
    "def run_test(k):\n",
    "\n",
    "    loss, batch_size, optimizer = options[k-1]\n",
    "    print(f\"\\n{k}/{n_options}\")    \n",
    "\n",
    "    model = BlackBox(tag=f\"GS_{k}\",\\\n",
    "                     layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                     epochs=epochs, metrics=None)\n",
    "    model.fit(verbose=1, validate=False)\n",
    "    model.evaluate(X_val, y_val)\n",
    "    \n",
    "    result = (model.eval_df, model.loss_name, model.batch_size, model.optimizer)\n",
    "    results.append(result)\n",
    "    \n",
    "    #model.discard()\n",
    "    del model\n",
    "    #%reset_selective -f \"^model$\"\n",
    "    \n",
    "for k in range(1, n_options+1):\n",
    "    run_test(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take macroF1 from the *results* list\n",
    "F1s = [[result[0].loc['macroF1'].iloc[0], *result[1:]] for result in results]\n",
    "F1s_df = pd.DataFrame(F1s, columns = ['macF1 on val', 'loss', 'batch', 'optimizer'])\n",
    "F1s_df.sort_values(by='macF1 on val', ascending=False, inplace=True)\n",
    "ranking = F1s_df.index\n",
    "\n",
    "F1s_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "Remember that I have let the model fit for only two epochs. The differences are subject to statistical fluctuations due to shuffling of the data done by keras plus a hard-to-gauge bias introduced by initial embedding weights. Seeing as our simple neural network is not necessarilly very representative of more complicated nets it is not guaranteed that the winner of our search will always be best. Nonetheless the above results tell a fairly consistent story in the context of our data. **my_loss** is better than **cat-cross**. **nadam** is better than **adam**. And, in the range of the order of few hundreds, the smaller the batch_size the better but the gain from that is the least significant. The **batch_size=500** already seems smalls seeing as it will often not contain the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the other metrics as well\n",
    "eval_results = pd.concat([result[0] for result in results], axis=1)\n",
    "ordered_columns = eval_results.columns.values[ranking]\n",
    "eval_results[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "As we see, in order to boost the F1 score, it pays to have the precision and recall scores as close as possible and our custom loss function was better and achieving that. The best scoring model above does not have the highest precision. It also does not have the lowest cross-entropy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(Kernel restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer fit of the two loss functions\n",
    "* settle on *optimizer*=nadam and *batch_size*=800\n",
    "* compare the results obtained on a stretch of a few more epochs with different loss functions\n",
    "* save embedding layers to files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "loss, batch_size, optimizer = 'categorical_crossentropy', 800, 'nadam'\n",
    "\n",
    "model1 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(verbose=1)\n",
    "model1.save_embedd()\n",
    "model1.Ksave()\n",
    "model1.save_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.Kload()\n",
    "model1.load_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(X_val, y_val)\n",
    "model1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "loss, batch_size, optimizer = my_loss, 800, 'nadam'\n",
    "\n",
    "model2 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model2.fit(verbose=1, validate=True)\n",
    "model2.save_embedd()\n",
    "model2.Ksave()\n",
    "model2.save_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.Kload()\n",
    "model2.load_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_val, y_val)\n",
    "model2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+2\n",
    "\n",
    "eval_together = pd.concat([model1.eval_df, model2.eval_df], axis=1)\n",
    "eval_together.columns = ['loss: cat. cross.', 'loss: my_loss']\n",
    "eval_together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "* Looking at the validation scores: The *my_loss* function is better than *categorical crossentropy* at keeping the precision and recall equal. It actually emphasizes the recall more, so that - after 3rd epoch - it scores slightly better than precision. It also faster in terms of increasing the F1 at starting epochs, which seems like an advantegous property of a loss function to be used on more complicated architectures. The *cat-cross* on the other hand is able to quickly launch the precision beyond 80%, but the recall is not able to keep up and it then pulls the precision down at later epochs.\n",
    "\n",
    "* Starting from between 2nd and 4th epoch we see overfitting: the scores obtained directly on train-data are higher than on validation-data. Interestingly, the cross-entropy seems easier to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test-data\n",
    "Recall that our final macro-F1 on test-data reached by the SVM was 80%. Here we have not yet looked at the test-data, but, judging by the score on the validation-data, it seems that we have reproduced the result of a shallow classifier with a 50-dimensional embedding, global averaging, and a single softmax-layer. We will yet use the knowledge gained by our grid-search and the embeddings produced by the neural networks trained above in further modelling. But, in order to have a score characterizing the simple architecture with global-averaging, we will now make the final fit for both loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(os.path.join(\"data\", \"Kdata\", \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(\"data\", \"Kdata\", \"y_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "loss, batch_size, optimizer = 'categorical_crossentropy', 800, 'nadam'\n",
    "\n",
    "model1 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model1.Kload()\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "loss, batch_size, optimizer = my_loss, 800, 'nadam'\n",
    "\n",
    "model2 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model2.Kload()\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+2\n",
    "\n",
    "eval_together = pd.concat([model1.eval_df, model2.eval_df], axis=1)\n",
    "eval_together.columns = ['loss: cat. cross.', 'loss: my_loss']\n",
    "eval_together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
