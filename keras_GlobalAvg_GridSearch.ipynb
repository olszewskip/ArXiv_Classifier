{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from itertools import cycle, product\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(123)\n",
    "\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import color_palette \n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I'm fitting the keras' embedding layer for a few epochs within the possibly simplest imaginable setup, namely with global averaging and softmax on top. This examinations in meant to be fairly inexpensive but broad, and serve to produce reasonable starting embeddings for more complicated net architectures and to compare various loss-functions/batch-sizes/optimizers. The choice of loss function is a somewhat non-trivial matter due to the class imbalance of our dataset and the stated objective of maximazing the macroF1 score.\n",
    "\n",
    "* Results land in the directory defined in the *working_dir* variable below.\n",
    "* Weights are stores as *blabla_weights.p* and they may be directly depickled into the *weights* argument of the Embedding layer (see the *Embedding* within *init* of the *BlackBox* class). \n",
    "* There are some nice plots below.\n",
    "\n",
    "We manage to establish an interesting benchmark for our dataset: the simple setup described above reproduces the performance of shallow classifiers, *macroF1*=80% on test-data. Thus, at least neural nets are no-worse than SVMs and LogisticRegression. For other net architectures to carry their weight, they'll have to go beyond that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the directory where to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘keras_GlobalAvg_GridSearch_results’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir keras_GlobalAvg_GridSearch_results\n",
    "working_dir = 'keras_GlobalAvg_GridSearch_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the test- and validation-data\n",
    "The test data is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample=30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(\"data\", \"Kdata\", \"X_train.npy\")) #[-n_sample:]\n",
    "y_train = np.load(os.path.join(\"data\", \"Kdata\", \"y_train.npy\")) #[-n_sample:]\n",
    "X_val = np.load(os.path.join(\"data\", \"Kdata\", \"X_val.npy\")) #[-n_sample:]\n",
    "y_val = np.load(os.path.join(\"data\", \"Kdata\", \"y_val.npy\")) #[-n_sample:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in global parameters describing the data prepared in *keras_preprocessing.ipynb*\n",
    "* dimensions needed for the word embedding\n",
    "* number of classes\n",
    "* class-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unique_words': 277303,\n",
       " 'num_words': 277304,\n",
       " 'padded_length': 679,\n",
       " 'n_classes': 6,\n",
       " 'class_weights': array([ 1.26825655,  0.72736371,  0.27602776, 13.23801959, 30.29201502,\n",
       "         9.49559404])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_params = pickle.load(open(\"global_params.p\", \"rb\"))\n",
    "unique_words = global_params['unique_words']\n",
    "num_words = global_params['num_words']\n",
    "padded_length = global_params['padded_length']\n",
    "n_classes = global_params['n_classes']\n",
    "class_weights = global_params['class_weights']\n",
    "\n",
    "global_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in custom loss functions and metrics\n",
    "* cat.-accuracy\n",
    "* macro-precision, macro-f1, macro-recall\n",
    "* cat.-crossentropy\n",
    "* a custom loss function, my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.cat_acc(y_true, y_pred)>,\n",
       " <function __main__.macroPrec(y_true, y_pred)>,\n",
       " <function __main__.macroF1(y_true, y_pred)>,\n",
       " <function __main__.macroRecall(y_true, y_pred)>,\n",
       " <function __main__.cat_cross(y_true, y_pred)>,\n",
       " <function __main__.fuzzy_macroF1_flip(y_true, y_pred)>,\n",
       " <function __main__.my_cross(y_true, y_pred)>,\n",
       " <function __main__.my_loss(y_true, y_pred)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run keras_custom_functions.ipynb\n",
    "\n",
    "my_metrics = list(CUSTOM_OBJECTS.values())\n",
    "my_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in wrappers for keras' sequential-model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run keras_plot_history.ipynb\n",
    "%run keras_blackbox_wrapper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-grid-search\n",
    "* Define the sequential setup by specifiying the layers, and parameter scopes to search through.\n",
    "* Train for a fixed number of epochs.\n",
    "* Examine how different parameter combinations influence quality of the classification (measured by the resulting model's macro-F1 on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Dropout(0.5),\n",
    "          GlobalAveragePooling1D()\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 options in the cross-search, e.g. ('categorical_crossentropy', 100, 'adam')\n"
     ]
    }
   ],
   "source": [
    "# the options\n",
    "\n",
    "losses = ['categorical_crossentropy', my_loss]\n",
    "batch_sizes = [100, 200, 500]\n",
    "optimizers = ['adam', 'nadam']\n",
    "\n",
    "options = list(product(losses, batch_sizes, optimizers))\n",
    "n_options = len(options)\n",
    "print(f\"{n_options} options in the cross-search, e.g. {options[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/12\n",
      "loss=categorical_crossentropy, batch_size=100, optimizer=adam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1931s 4ms/step - loss: 0.3370\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 2102s 4ms/step - loss: 0.1864\n",
      "\n",
      "2/12\n",
      "loss=categorical_crossentropy, batch_size=100, optimizer=nadam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 2234s 4ms/step - loss: 0.2700\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 2097s 4ms/step - loss: 0.1685\n",
      "\n",
      "3/12\n",
      "loss=categorical_crossentropy, batch_size=200, optimizer=adam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1197s 2ms/step - loss: 0.4137\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 1210s 2ms/step - loss: 0.2069\n",
      "\n",
      "4/12\n",
      "loss=categorical_crossentropy, batch_size=200, optimizer=nadam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1440s 3ms/step - loss: 0.3174\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 1363s 3ms/step - loss: 0.1779\n",
      "\n",
      "5/12\n",
      "loss=categorical_crossentropy, batch_size=500, optimizer=adam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 753s 2ms/step - loss: 0.5828\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 780s 2ms/step - loss: 0.2556\n",
      "\n",
      "6/12\n",
      "loss=categorical_crossentropy, batch_size=500, optimizer=nadam, explicit-class-weights: True, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 824s 2ms/step - loss: 0.4255\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 833s 2ms/step - loss: 0.2048\n",
      "\n",
      "7/12\n",
      "loss=my_loss, batch_size=100, optimizer=adam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1630s 3ms/step - loss: 0.4337\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 1589s 3ms/step - loss: 0.3131\n",
      "\n",
      "8/12\n",
      "loss=my_loss, batch_size=100, optimizer=nadam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1918s 4ms/step - loss: 0.3820\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 1917s 4ms/step - loss: 0.2948\n",
      "\n",
      "9/12\n",
      "loss=my_loss, batch_size=200, optimizer=adam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 997s 2ms/step - loss: 0.4624\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 996s 2ms/step - loss: 0.2848\n",
      "\n",
      "10/12\n",
      "loss=my_loss, batch_size=200, optimizer=nadam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 1165s 2ms/step - loss: 0.3750\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 1165s 2ms/step - loss: 0.2499\n",
      "\n",
      "11/12\n",
      "loss=my_loss, batch_size=500, optimizer=adam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 648s 1ms/step - loss: 0.5623\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 647s 1ms/step - loss: 0.3025\n",
      "\n",
      "12/12\n",
      "loss=my_loss, batch_size=500, optimizer=nadam, explicit-class-weights: False, embedd-trainable: True\n",
      "Epoch 1/2\n",
      "500000/500000 [==============================] - 711s 1ms/step - loss: 0.4314\n",
      "Epoch 2/2\n",
      "500000/500000 [==============================] - 710s 1ms/step - loss: 0.2336\n"
     ]
    }
   ],
   "source": [
    "# fit and evaluate on the validation data, loop through the options\n",
    "\n",
    "epochs = 2\n",
    "results = []\n",
    "\n",
    "def run_test(k):\n",
    "\n",
    "    loss, batch_size, optimizer = options[k-1]\n",
    "    print(f\"\\n{k}/{n_options}\")    \n",
    "\n",
    "    model = BlackBox(tag=f\"GS_{k}\",\\\n",
    "                     layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                     epochs=epochs, metrics=None)\n",
    "    model.fit(verbose=1, validate=False)\n",
    "    model.evaluate(X_val, y_val)\n",
    "    \n",
    "    result = (model.eval_df, model.loss_name, model.batch_size, model.optimizer)\n",
    "    results.append(result)\n",
    "    \n",
    "    #model.discard()\n",
    "    del model\n",
    "    #%reset_selective -f \"^model$\"\n",
    "    \n",
    "for k in range(1, n_options+1):\n",
    "    run_test(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macF1 on val</th>\n",
       "      <th>loss</th>\n",
       "      <th>batch</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GS_</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.800219</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>200</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.799125</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>100</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.794683</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>500</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.794543</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.789097</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.780720</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>100</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.778044</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>200</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.771202</td>\n",
       "      <td>my_loss</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770701</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.723731</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.715217</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>500</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.501920</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     macF1 on val                      loss  batch optimizer\n",
       "GS_                                                         \n",
       "10       0.800219                   my_loss    200     nadam\n",
       "8        0.799125                   my_loss    100     nadam\n",
       "12       0.794683                   my_loss    500     nadam\n",
       "7        0.794543                   my_loss    100      adam\n",
       "9        0.789097                   my_loss    200      adam\n",
       "2        0.780720  categorical_crossentropy    100     nadam\n",
       "4        0.778044  categorical_crossentropy    200     nadam\n",
       "11       0.771202                   my_loss    500      adam\n",
       "1        0.770701  categorical_crossentropy    100      adam\n",
       "3        0.723731  categorical_crossentropy    200      adam\n",
       "6        0.715217  categorical_crossentropy    500     nadam\n",
       "5        0.501920  categorical_crossentropy    500      adam"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take macroF1 from the *results* list\n",
    "F1s = [[result[0].loc['macroF1'].iloc[0], *result[1:]] for result in results]\n",
    "F1s_df = pd.DataFrame(F1s, columns = ['macF1 on val', 'loss', 'batch', 'optimizer'])\n",
    "F1s_df.sort_values(by='macF1 on val', ascending=False, inplace=True)\n",
    "ranking = F1s_df.index\n",
    "\n",
    "F1s_df.index = F1s_df.index + 1\n",
    "F1s_df.index.name = \"GS_\"\n",
    "F1s_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "Remember that I have let the model fit for only two epochs. The differences are subject to statistical fluctuations due to shuffling of the data done by keras plus a hard-to-gauge bias introduced by initial embedding weights. Seeing as our simple neural network is not necessarilly very representative of more complicated nets it is not guaranteed that the winner of our search will always be best. Nonetheless the above results tell a fairly consistent story in the context of our data. **my_loss** is better than **cat-cross**. **nadam** is better than **adam**. And, in the range of the order of few hundreds, the smaller the batch_size the better but the gain from that is the least significant. The **batch_size=100 or =200** already seems small seeing as it will often not contain the least frequent classes (see the *arXiv_cleanup.ipynb*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GS_10</th>\n",
       "      <th>GS_8</th>\n",
       "      <th>GS_12</th>\n",
       "      <th>GS_7</th>\n",
       "      <th>GS_9</th>\n",
       "      <th>GS_2</th>\n",
       "      <th>GS_4</th>\n",
       "      <th>GS_11</th>\n",
       "      <th>GS_1</th>\n",
       "      <th>GS_3</th>\n",
       "      <th>GS_6</th>\n",
       "      <th>GS_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat_cross</th>\n",
       "      <td>0.302168</td>\n",
       "      <td>0.324697</td>\n",
       "      <td>0.284958</td>\n",
       "      <td>0.301253</td>\n",
       "      <td>0.290869</td>\n",
       "      <td>0.181275</td>\n",
       "      <td>0.183533</td>\n",
       "      <td>0.271755</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>0.197483</td>\n",
       "      <td>0.196022</td>\n",
       "      <td>0.230411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_loss</th>\n",
       "      <td>0.206441</td>\n",
       "      <td>0.208660</td>\n",
       "      <td>0.216839</td>\n",
       "      <td>0.216640</td>\n",
       "      <td>0.226614</td>\n",
       "      <td>0.255171</td>\n",
       "      <td>0.266789</td>\n",
       "      <td>0.252830</td>\n",
       "      <td>0.277570</td>\n",
       "      <td>0.327670</td>\n",
       "      <td>0.331436</td>\n",
       "      <td>0.444602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_acc</th>\n",
       "      <td>0.928670</td>\n",
       "      <td>0.929535</td>\n",
       "      <td>0.924200</td>\n",
       "      <td>0.926605</td>\n",
       "      <td>0.922745</td>\n",
       "      <td>0.935455</td>\n",
       "      <td>0.934510</td>\n",
       "      <td>0.918105</td>\n",
       "      <td>0.933550</td>\n",
       "      <td>0.930650</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>0.920860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macroPrec</th>\n",
       "      <td>0.783735</td>\n",
       "      <td>0.765821</td>\n",
       "      <td>0.790043</td>\n",
       "      <td>0.770455</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.849545</td>\n",
       "      <td>0.844752</td>\n",
       "      <td>0.793514</td>\n",
       "      <td>0.843908</td>\n",
       "      <td>0.850876</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>0.813380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macroF1</th>\n",
       "      <td>0.800219</td>\n",
       "      <td>0.799125</td>\n",
       "      <td>0.794683</td>\n",
       "      <td>0.794543</td>\n",
       "      <td>0.789097</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>0.778044</td>\n",
       "      <td>0.771202</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.723731</td>\n",
       "      <td>0.715217</td>\n",
       "      <td>0.501920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macroRecall</th>\n",
       "      <td>0.819289</td>\n",
       "      <td>0.843283</td>\n",
       "      <td>0.799855</td>\n",
       "      <td>0.823802</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.731953</td>\n",
       "      <td>0.729782</td>\n",
       "      <td>0.752909</td>\n",
       "      <td>0.720559</td>\n",
       "      <td>0.656970</td>\n",
       "      <td>0.646718</td>\n",
       "      <td>0.479012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GS_10      GS_8     GS_12      GS_7      GS_9      GS_2  \\\n",
       "cat_cross    0.302168  0.324697  0.284958  0.301253  0.290869  0.181275   \n",
       "my_loss      0.206441  0.208660  0.216839  0.216640  0.226614  0.255171   \n",
       "cat_acc      0.928670  0.929535  0.924200  0.926605  0.922745  0.935455   \n",
       "macroPrec    0.783735  0.765821  0.790043  0.770455  0.776557  0.849545   \n",
       "macroF1      0.800219  0.799125  0.794683  0.794543  0.789097  0.780720   \n",
       "macroRecall  0.819289  0.843283  0.799855  0.823802  0.803427  0.731953   \n",
       "\n",
       "                 GS_4     GS_11      GS_1      GS_3      GS_6      GS_5  \n",
       "cat_cross    0.183533  0.271755  0.186971  0.197483  0.196022  0.230411  \n",
       "my_loss      0.266789  0.252830  0.277570  0.327670  0.331436  0.444602  \n",
       "cat_acc      0.934510  0.918105  0.933550  0.930650  0.931250  0.920860  \n",
       "macroPrec    0.844752  0.793514  0.843908  0.850876  0.858197  0.813380  \n",
       "macroF1      0.778044  0.771202  0.770701  0.723731  0.715217  0.501920  \n",
       "macroRecall  0.729782  0.752909  0.720559  0.656970  0.646718  0.479012  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the other metrics as well\n",
    "eval_results = pd.concat([result[0] for result in results], axis=1)\n",
    "ordered_columns = eval_results.columns.values[ranking]\n",
    "eval_results[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "As we see, in order to boost the F1 score, it pays to have the precision and recall scores close. The best scoring model above does not have the highest precision or recall, it also does not have the lowest cross-entropy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(Kernel restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer fit of the two loss functions\n",
    "* Settle on *optimizer*=nadam and *batch_size*=200 (in this range of values the smaller the batch the longer it takes to fit the whole set, and the difference in perfmormance between 200 and 100 seems already small enough).\n",
    "* Compare the results obtained on a stretch of a few more epochs with different loss functions\n",
    "* Save embedding layers to files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "loss, batch_size, optimizer = 'categorical_crossentropy', 800, 'nadam'\n",
    "\n",
    "model1 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(verbose=1)\n",
    "model1.save_embedd()\n",
    "model1.Ksave()\n",
    "model1.save_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.Kload()\n",
    "model1.load_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(X_val, y_val)\n",
    "model1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "loss, batch_size, optimizer = my_loss, 800, 'nadam'\n",
    "\n",
    "model2 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model2.fit(verbose=1, validate=True)\n",
    "model2.save_embedd()\n",
    "model2.Ksave()\n",
    "model2.save_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.Kload()\n",
    "model2.load_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_val, y_val)\n",
    "model2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+2\n",
    "\n",
    "eval_together = pd.concat([model1.eval_df, model2.eval_df], axis=1)\n",
    "eval_together.columns = ['loss: cat. cross.', 'loss: my_loss']\n",
    "eval_together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "* Looking at the validation scores: The *my_loss* function is better than *categorical crossentropy* at keeping the precision and recall equal. It actually emphasizes the recall more, so that - after 3rd epoch - it scores slightly better than precision. It also faster in terms of increasing the F1 at starting epochs, which seems like an advantegous property of a loss function to be used on more complicated architectures. The *cat-cross* on the other hand is able to quickly launch the precision beyond 80%, but the recall is not able to keep up and it then pulls the precision down at later epochs.\n",
    "\n",
    "* Starting from between 2nd and 4th epoch we see overfitting: the scores obtained directly on train-data are higher than on validation-data. Interestingly, the cross-entropy seems easier to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test-data\n",
    "Recall that our final macro-F1 on test-data reached by the SVM was 80%. Here we have not yet looked at the test-data, but, judging by the score on the validation-data, it seems that we have reproduced the result of a shallow classifier with a 50-dimensional embedding, global averaging, and a single softmax-layer. We will yet use the knowledge gained by our grid-search and the embeddings produced by the neural networks trained above in further modelling. But, in order to have a score characterizing the simple architecture with global-averaging, we will now make the final fit for both loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(os.path.join(\"data\", \"Kdata\", \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(\"data\", \"Kdata\", \"y_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "loss, batch_size, optimizer = 'categorical_crossentropy', 800, 'nadam'\n",
    "\n",
    "model1 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model1.Kload()\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "loss, batch_size, optimizer = my_loss, 800, 'nadam'\n",
    "\n",
    "model2 = BlackBox(tag=\"GlobalAvg\",\\\n",
    "                  layers=layers, loss=loss, batch_size=batch_size, optimizer=optimizer,\\\n",
    "                  epochs=epochs, metrics=my_metrics)\n",
    "model2.Kload()\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+2\n",
    "\n",
    "eval_together = pd.concat([model1.eval_df, model2.eval_df], axis=1)\n",
    "eval_together.columns = ['loss: cat. cross.', 'loss: my_loss']\n",
    "eval_together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
